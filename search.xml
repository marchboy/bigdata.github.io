<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>沟通能力进阶法则</title>
      <link href="2021/07/23/L4-%E6%B2%9F%E9%80%9A%E8%83%BD%E5%8A%9B%E8%BF%9B%E9%98%B6%E6%B3%95%E5%88%99/"/>
      <url>2021/07/23/L4-%E6%B2%9F%E9%80%9A%E8%83%BD%E5%8A%9B%E8%BF%9B%E9%98%B6%E6%B3%95%E5%88%99/</url>
      
        <content type="html"><![CDATA[<h3 id="不懂沟通，人生处处不轻松"><a href="#不懂沟通，人生处处不轻松" class="headerlink" title="不懂沟通，人生处处不轻松"></a>不懂沟通，人生处处不轻松</h3><p>1、最能打动人心的，是差异化的东西。</p><p>2、把你的事情变成大家共同的事情，让他成为你的工作伙伴（让对方知道，他为什么要帮你，他帮你能获得什么）。</p><p>考虑下，让你的事情，是不是能给对方带来什么信息增量，或者是为对方带去什么价值，是最能打动他们的。</p><p>3、怎样建立人脉网</p><p>关系的本质是拿来解决问题的</p><p>有共同的问题–&gt; 再到私人话题–&gt; 关系升温</p><p>求人办事的过程–&gt; 建立利益的关系–&gt; 学会麻烦别人</p><p>4、不带任何框架（目的）地去看待这个问题的本身</p><p>让交谈的内容从公域到私域</p><p>沟通的一场相互探索的过程</p><p>5、真正的聊天是倾听，且不带任何目的</p><p>让沟通进入你的意识，并形成过一个习惯</p><p>6、对事情能做到有掌控感</p><p>做任何事情，都要匹配资历，匹配能力。</p>]]></content>
      
      
      <categories>
          
          <category> 沟通能力 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 沟通能力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据质量监控</title>
      <link href="2021/07/08/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%9B%91%E6%8E%A7/"/>
      <url>2021/07/08/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>随着大数据时代的带来，数据的应用也日趋繁茂，越来越多的应用和服务都基于数据而建立，数据的重要性不言而喻。<strong>而且，数据质量是数据分析和数据挖掘结论有效性和准确性的基础，也是这一切的数据驱动决策的前提！</strong>如何保障数据质量，确保数据可用性是每一位数据人都不可忽略的重要环节。</p><p>数据质量，主要从四个方面进行评估，即完整性、准确性、一致性和及时性，本文将会结合业务流程和数据处理流程，对这个四个方面进行详细的分析和讲解。</p><p>数据，最终是要服务于业务价值的，因此，本文不会单纯讲解理论，而是会从数据质量监控这一数据的应用为出发点，为大家分享居士对数据质量的思考。通过本文，你将获得如下几方面的知识点：</p><ol><li>数据质量核心关注的要点</li><li>从数据计算链条理解，每一个环节会出现哪些数据质量问题</li><li>从业务逻辑理解，数据质量监控能带来的帮助</li><li>实现数据质量监控系统时要关注的点</li><li>数据质量监控面临的一些难点和解决思路</li></ol><h3 id="四大关注点"><a href="#四大关注点" class="headerlink" title="四大关注点"></a>四大关注点</h3><p>本节，先简单地聊一下数据质量需要关注的四个点：即完整性、准确性、一致性和及时性。这四个关注点，会在我们的数据处理流程的各个环节有所体现。</p><p><img src="/2021/07/08/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%9B%91%E6%8E%A7/1.jpg"></p><h4 id="完整性"><a href="#完整性" class="headerlink" title="完整性"></a>完整性</h4><p>完整性是指数据的记录和信息是否完整，是否存在缺失的情况。数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成统计结果不准确，所以说完整性是数据质量最基础的保障。</p><p>简单来讲，如果要做监控，需要考虑两个方面：一是，数据条数是否少了，二是，某些字段的取值是否缺失。完整性的监控，多出现在日志级别的监控上，一般会在数据接入的时候来做数据完整性校验。</p><h4 id="准确性"><a href="#准确性" class="headerlink" title="准确性"></a>准确性</h4><p>准确性是指数据中记录的信息和数据是否准确，是否存在异常或者错误的信息。</p><p>直观来讲就是看数据是否上准确的。一般准确性的监控多集中在对业务结果数据的监控，比如每日的活跃、收入等数据是否正常。</p><h4 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h4><p>一致性是指同一指标在不同地方的结果是否一致。</p><p>数据不一致的情况，多出现在数据系统达到一定的复杂度后，同一指标会在多处进行计算，由于计算口径或者开发人员的不同，容易造成同一指标出现的不同的结果。</p><h4 id="及时性"><a href="#及时性" class="headerlink" title="及时性"></a>及时性</h4><p>在确保数据的完整性、准确性和一致性后，接下来就要保障数据能够及时产出，这样才能体现数据的价值。</p><p>及时性很容易理解，主要就是数据计算出来的速度是否够快，这点在数据质量监控中可以体现在监控结果数据数据是否在指定时间点前计算完成。</p><h3 id="数据处理各环节的数据质量"><a href="#数据处理各环节的数据质量" class="headerlink" title="数据处理各环节的数据质量"></a>数据处理各环节的数据质量</h3><p>数据质量监控之所以难做，是因为在数据的各个环节都会出现数据质量的问题。因此，本节将以一个典型的数据处理链条为例，为大家分享在每个阶段容易出现哪些数据质量问题。</p><p>如下图，为了举例说明，我画了一个简单的数据处理流程（<em>在实际中的情况会比该情况复杂很多</em>），我将数据处理分为 3 个阶段：数据接入、中间数据清洗、结果数据计算。</p><p><img src="/2021/07/08/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%9B%91%E6%8E%A7/2.jpg"></p><h4 id="数据接入"><a href="#数据接入" class="headerlink" title="数据接入"></a>数据接入</h4><p>如上图所示，数据接入环节最容易出现的是数据完整性的问题，这里要特别注意的是数据量是否陡增和陡降。</p><p>陡增意味着可能会出现大量数据重复上报或者异常数据侵入等情况，陡降意味着可能出现数据丢失的情况。</p><p>另一方面，也要检查不同字段的的取值是否有丢失，比如地址和设备字段是否出现大量空值等异常。</p><h4 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h4><p>在这里，我将数据清洗的范围局限在数据仓库的中间表清洗上，这一部分一般也是我们的数据仓库要建设的核心部分，业务到了一定程度，数据中间层的建设必不可少！</p><p>在这一环节，最容易出现的是数据一致性和数据准确性的问题。数据中间层保障来数据是从统一出口而出，让数据一起对或者一起错。但是很难保证数据准确性的问题，因此在数据清洗阶段需要尽量保障数据的准确性。</p><h4 id="数据结果"><a href="#数据结果" class="headerlink" title="数据结果"></a>数据结果</h4><p>结果数据，主要是强调对外提供数据的过程，一般是从中间表中计算或直接取得的可展示数据。这里是业务方和老板最容易感知的到的地方，因此在这环节，主要关注的是数据准确性和数据及时性。</p><p>整体来讲，数据的完整性、准确性、一致性和及时性在数据处理的各个阶段都需要关注，但是可以先抓住的核心的问题来解决。</p><h3 id="业务流程各环节的数据质量"><a href="#业务流程各环节的数据质量" class="headerlink" title="业务流程各环节的数据质量"></a>业务流程各环节的数据质量</h3><p>聊完数据处理，我们继续聊一下业务流程。数据最终的价值是要服务于业务的，因此数据质量最好也是能从解决业务问题出发，因此，本节从典型的业务场景来讲解数据质量该怎么做。</p><p>首先，居士认为，既然做监控肯定是要考虑使用方的，而我们的数据质量监控平台一个很重要的作用是希望让老板、产品和运营这些使用方对我们的数据放心，那么他们的关注点是什么？居士认为，是业务指标！</p><p>那么，这个业务指标可以从两个角度来考虑：</p><ol><li>单个指标的数值异常，比如说数据是否达到来某个临界值？是否有陡增和陡降？</li><li>整个业务链条的数据是否有异常，比如从曝光到注册的转化是否有异常？</li></ol><p>如下图，是一个 App 的用户行为漏斗分析，其实也就是从获取用户到转化的简单链路。</p><p>那么针对该链路，我们数据质量监控要做的事，除了告诉使用方某一个节点的值有问题，也需要告诉他们整个链条哪里出了问题，哪里的转化低了。</p><p><img src="/2021/07/08/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%9B%91%E6%8E%A7/3.jpg"></p><h3 id="如何实现数据质量监控"><a href="#如何实现数据质量监控" class="headerlink" title="如何实现数据质量监控"></a>如何实现数据质量监控</h3><p>前面分享了数据质量关注的点，以及从技术和业务角度会如何关注数据质量，本节将简单地分享一下如何实现数据质量监控。这里将分两个角度：宏观的设计思路和技术实现思路。</p><h4 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h4><p>数据质量监控的设计要分为四个模块：数据、规则、告警和反馈。</p><ul><li>数据：主要是需要被数据质量监控到的数据，数据可能存放在不同的存储引擎中，比如Hive、PG、ES等。</li><li>规则：是指如何设计发现异常的规则，一般而言主要是数值的异常和环比等异常监控方式。也会有一些通过算法来发掘异常数据的方法。</li><li>告警：告警是指发出告警的动作，这里可以通过微信消息、电话、短信或者是微信小程序的方式来触发告警内容。</li><li>反馈：这里需要特别注意，反馈是指对告警内容的反馈，比如说收到的告警的内容，那么负责人要来回应这个告警消息是否是真的异常，是否需要忽略该异常，是否已经处理了该异常。有了反馈的机制，整个数据质量监控才容易形成闭环。更能体现业务价值。</li></ul><p><img src="/2021/07/08/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%9B%91%E6%8E%A7/4.jpg"></p><h4 id="技术方案"><a href="#技术方案" class="headerlink" title="技术方案"></a>技术方案</h4><p>关于技术方案，这里不多描述细节，因为不同的公司和团队情况对实现方案的考虑是不同的，简单做的话，可以写一些定时脚本即可，复杂的话可以做成一个分布式的系统。这里也可以参考居士17年写的一部分内容<a href="http://mp.weixin.qq.com/s?__biz=MzUyMjI4MzE0MQ==&mid=2247483847&idx=1&sn=d7c7001a72a9fc4ce578ee256afe609a&chksm=f9cf702dceb8f93bd66d49c681a79f1a586c504ee906b4275c4acb2fdb606411fafd18a6fb2d&scene=21#wechat_redirect">No.22 漫谈数据质量监控</a>。</p><p>本篇只简单说明几个技术实现中需要关注的点：</p><ol><li>最开始可以先关注核心要监控的内容，比如说准确性，那么就对核心的一些指标做监控即可，不用开始就做很大的系统。</li><li>监控平台尽量不要做太复杂的规则逻辑，尽量只对结果数据进行监控。比如要监控日志量是否波动过大，那么把该计算流程前置，先计算好结果表，最后监控平台只监控结果表是否异常即可。</li><li>多数据源，多数据源的监控有两种方式可以处理：针对每个数据源定制实现一部分计算逻辑，也可以通过额外的任务将多数据源中的数据结果通过任务写入一个数据源中，再该数据源进行监控，这样可以减少数据监控平台的开发逻辑。具体的优缺点可以自行衡量。</li><li>实时数据的监控，实时和离线数据监控的主要区别在于扫描周期的不同，因此在设计的时候可以先以离线数据为主，但是尽量预留好实时监控的设计。</li><li>在设计之初，尽量预留好算法监控的设计，这是一个很大的加分项，具体的结合方式也可以和第二点建议接近，比如算法异常数据放到一张结果表中，再在上面配置简单的告警规则即可。</li></ol><h3 id="一些困难"><a href="#一些困难" class="headerlink" title="一些困难"></a>一些困难</h3><p>在做数据质量监控的时候难免会遇到一些困难点，亦或是被老板挑战的地方，下面列举几个问题和解决的思路，供大家参考：</p><p>问题一：假设你的结果表要经过多层的中间表计算，你怎么保证每个环节都是正确的，且最终结果是正确的？</p><p>思路：从两个点考虑：</p><ol><li>每一层代码有 Code Review，保证代码逻辑正常。</li><li>单独一条计算流，对关键指标从原始数据直接计算结果，和日常的结果表做对比，发现不同则告警。这种方式也可以理解为是结果数据和源数据的对账。</li></ol><p>问题二：告警信息太多了，太容易被忽略怎么办？</p><p>思路：主要是思路是提高告警的准确率，避免无用的告警，有三个思路：</p><ol><li>多使用机器学习算法的方式来发现异常点，比如：异常森林。</li><li>加入反馈机制，如果业务负责人认为该告警是正常的，就打上正常的tag，后续告警规则根据反馈进行优化。</li><li>加入屏蔽功能，屏蔽不感兴趣的告警。</li></ol><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>草稿发出来后，收到了一些反馈，但是要将这些反馈都融入到文章中需要很多的时间，因此先将内容在展现出来，供大家参考。</p><h4 id="水大人"><a href="#水大人" class="headerlink" title="水大人"></a>水大人</h4><p>思路很清晰，展示在这里给大家做参考</p><p><img src="/2021/07/08/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%9B%91%E6%8E%A7/5.jpg"></p><h4 id="邢胖"><a href="#邢胖" class="headerlink" title="邢胖"></a>邢胖</h4><p>数据准确性 是建立在合理的业务口径下，从口径角度去统一才会获得准确的结果。</p><p>而不是仅仅认为从某个面去看这个数据是准确的就要做统一，不应从数据去逆推口径。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>和本系列其它文章相似，本文更侧重的是做数据质量过程的思考，这个思考主要体现的地方是，怎么去定义问题和解决问题，而不是直接给出解决的方案。</p><p>比如说从数据流程的各个环节来梳理需要做数据质量的点，以及业务方核心会关注的点，这些才是能决定你的数据质量监控平台能否获得认可的关键因素。当这些东西都理清之后，技术实现只是把你的想法具像化的工具，这并非是不重视技术，而是更看重如何让技术的价值最大化。</p><p>最后，欢迎大家多多交流。</p><p>参考：<a href="https://mp.weixin.qq.com/s/HzKI8fNdkeIv2y0oDScDkA">数据质量监控</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据质量 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据质量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据建模理论、设计和实践</title>
      <link href="2021/07/03/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E7%90%86%E8%AE%BA%E3%80%81%E8%AE%BE%E8%AE%A1%E5%92%8C%E5%AE%9E%E8%B7%B5/"/>
      <url>2021/07/03/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E7%90%86%E8%AE%BA%E3%80%81%E8%AE%BE%E8%AE%A1%E5%92%8C%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么要数据建模"><a href="#为什么要数据建模" class="headerlink" title="为什么要数据建模"></a>为什么要数据建模</h3><p>为什么要数据建模？</p><p>这是一个互联网的时代，也是大数据的时代，数据的价值不言而喻。虽然大家都知道数据很重要，但如果数据不能得到很好应用，那么数据就没有价值，<font color="red">数据建模就是为了能够将数据的价值更好的挖掘出来 </font>，所进行的一系列工作。</p><p>数据建模本身是一种<font color="red">组织、分析、存储、应用数据的方法论</font>，尽然是一种方法论，那么就有衡量好坏的标准：性能、成本、效率、质量。因此，数据建模的工作，就是围绕这四个指标做出最优解而进行的努力。</p><p>数据建模是数据研发岗位的核心内容，是<font color="red">数据采集 - 数据建模 - 数据开发 - BI应用</font>这个研发链条上的关键步骤。</p><p><font color="red">数据建模的日常工作，就是通过宏观模型 + 分层模型，做好标准化的模型建设，并依赖报表、接口等产品服务，输出数据价值。</font></p><p>因此，掌握核心的数据建模理论，就是数据研发工程师的核心竞争力。</p><p>好在，数据建模的理论并不难理解，很容易学会。但困难的是，想要把这套理论应用的的新应用，需要非常多的训练才能掌握。尤其对于互联网行业来讲，数据量足够大、业务足够复杂，才能练出足够好的数据建模师来。</p><h3 id="数据建模理论"><a href="#数据建模理论" class="headerlink" title="数据建模理论"></a>数据建模理论</h3><p>让我们先来看看基础的数据建模理论。</p><p>由于在变化快速的商业世界里，业务形态多种多样，为了能够更有针对性的进行数据建模，经过长时间的摸索，业界逐步形成了数据建模的四部曲：<font color="red">业务建模-&gt;领域建模-&gt;逻辑建模-&gt;物理建模</font>。简单讲，就是<strong>明确具体业务，抽象实体和关系，结合具体的建模方法，确定所有关键成分和属性，最后建数据表进行数据的存储和计算。</strong></p><p>目前数据建模的方法论有两大阵营，一个是<font color="red">基于关系型数据库理论</font>设计出来的，比如<code>基于3NF的范式建模</code>。虽然目前也有不少非关系型数据库以及不少半结构化和非结构化数据。但将半结构化/非结构化数据转化为结构化数据，然后再利用关系型数据库处理仍然是一种通用的主流数据处理方案。另一个是<code>基于数据仓库之父Bill Inmon提出的维度建模理论</code>，是从全企业的高度利用实体关系来对企业业务进行描述。</p><p>先讲讲范式建模，包括<code>E-R模型、DATA Vault模型和Anchor模型</code>。</p><p><code>E-R模型</code>是典型的范式数仓建模，满足3NF，通过实体关系来描述企业业务之间的关系，通常是面向整体的，而不是面向单个业务过程来进行建立。作为一种标准的数据建模方案，E-R模型的数据一致性和扩展性都比较好，且经得起时间的考验，但在大数据多分析型场景诉求下，查询性能和便捷度都存在一定的不足，因此在实际业务中，往往只应用在DWD层。</p><p><code>DATA Vault模型</code>是ER模型的衍生版本，将实体的描述部分拆分开来，当作单独的一个部分存在。DATA Vault模型不能直接面向业务进行数据分析决策，需要关联处理之后才能进行相应数据指标的统计，它的扩展性更好，但同时性能和便捷度更差。</p><p><code>Anchor模型</code>是对DATA Vault更进一步的范式化处理，扩展性更好，但可分析性能更差。</p><p>再讲讲维度建模。</p><p><code>维度模型</code>主要围绕一个确定的业务过程展开，先确定好粒度和维度属性，再计算具体的指标。维度模型的典型结构是星型模型，也可能演变为雪花模型。经过互联网行业多年的探索，证实设计出好的维度模型比E-R模型更简单快速，但是缺点也比较明显，由于冗余太多维度属性，数据一致性和扩展型都相对较差，对过快业务变化的场景并不友好。由于维度建模更适合大数据面向OLAP场景下的数据建模方案，因此在实际业务中，通常应用在中间层DWS或ADS上层的数据集市。</p><p>以上内容，网上有大量的书籍和文章详细介绍，这里推荐阅读两本就足够了，一本是Kimball的原书《数据仓库生命周期工具箱》、一本是《阿里巴巴大数据实践之路》。</p><h3 id="数据建模中的宏观模型"><a href="#数据建模中的宏观模型" class="headerlink" title="数据建模中的宏观模型"></a>数据建模中的宏观模型</h3><p>讲完了基础的数据建模理论，再来看看日常工作，先讲一下宏观模型。</p><p><font color="red">数据仓库的定义是什么？是一个面向“主题”的、集成的、相对稳定的、反映历史变化的“数据集合”，用于支持管理决策。数据仓库的指导思想是：以“维度建模”为基础构建总线矩阵，按照“数据域”和“业务过程”进行“主题模型”设计，构建一致性的维度和事实。</font></p><p>主题模型是指数仓使用用户所重点关心大脑内容，是一个商业领域所涉及的所有分析对象，如“销售分析”、“用户分析”，通常横跨多个业务系统或者数据系统。主题域下面可以有多个主题，主题还可以划分成更多的子主题。</p><p>把一个数据仓库做了横向的切分，或者说把企业的多个业务横向切分成不同的主题模型，又可继续细分为不同的子域，因此形成了一个具有公共前缀的树形结构，这个公共前缀通过命名规范体现出来，这也是为什么我们要强调表的命名规范的原因。</p><p>如下图所示，基础DB或者是Log日志，通过不同的主题进行聚集，最后输出到同一个主题域中。</p><p><img src="/2021/07/03/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E7%90%86%E8%AE%BA%E3%80%81%E8%AE%BE%E8%AE%A1%E5%92%8C%E5%AE%9E%E8%B7%B5/1.png"></p><p>从宏观的顶层设计来看，我们的数据模型应该是面向主题设计的，由于业务体系庞大，为了能够更加清晰的区分数据表的职责，我们将主题模型设计将细分为数据域和业务过程。</p><p><strong>数据域：将业务过程或维度进行抽象，即“业务过程”或“维度”的集合，数据域还可划分多级子域。数据域是指面向业务分析，将业务过程进行抽象的集合。数据域需要抽象提炼，并且长期维护和更新，但不轻易变动。</strong>数据域的划分对于维度建模理论在层次数仓建模落地上有重要意义，一般根据业务过程相似性将同类业务数据划分到一个数据域中，底层数据的加工都会聚焦在同一个数据域中进行，在上层应用时才会加工跨数据域的数据模型。</p><p><strong>业务过程：指企业活动中一个不可拆分的的行为事件，如下单、支付、退款等行为事件</strong>。</p><p>因此，我们可以简单的认为，<font color="red">主题模型 = 数据域 + 业务过程。</font></p><p>我们通常所做的模型设计，宏观层面上，就是合理的划分数据域和业务过程，而这种划分的结果是以表名的形式体现出来的，因此合理的数仓结构中，通过<strong>表名便应该能够清晰的分辨出这张表所属的数据域和业务过程，这与工程上所强调的“代码整洁之道”有异曲同工之妙</strong>。</p><h3 id="数据建模中的分层设计"><a href="#数据建模中的分层设计" class="headerlink" title="数据建模中的分层设计"></a>数据建模中的分层设计</h3><p>划分清楚了大的概念，接下来讲实际开发，最重要的是<font color="red">分层设计</font>，这也是维度建模的灵魂，也是互联网公司经过过年的探索，得出的最佳实践。</p><p>在介绍分层之前，讲几个基本的概念。</p><p><img src="/2021/07/03/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E7%90%86%E8%AE%BA%E3%80%81%E8%AE%BE%E8%AE%A1%E5%92%8C%E5%AE%9E%E8%B7%B5/2.png"></p><p>接下来讲分层的内容。</p><p><font color="red">数据分层 = ODS + CDM（DIM + DWD + DWS）+ ADS</font>，当业务不复杂时，也可以忽略某些层，比如DB设计的足够优秀且业务比较简单，ODS就可以不用建设。</p><p>ODS面向ETL过程设计，通常不改变数据的内容，重点考虑数据如何及时可靠的同步过来。</p><p><strong>ODS作为原始数据存储层</strong>，建议保留全部的数据原始记录，留底而不做任何清洗操作。ODS层对接的系统侧数据一般为明细数据，如果不是实时数据，调度周期一般为日调度或小时调度。对于数据的同步方式，ODS只记录会发生更新变更的数据，例如累计快照事实表、多事务事实表，建议采用时间上的全量同步；不会发生更改删除的数据，例如事务事实表，建议采用时间增量同步。ODS表的保留周期，一般为全量7天、增量永久。</p><p><strong>CDM = Common Data Model</strong>，该层不面向产品需求开发，而是针对业务过程的抽象和定义，是数据模型的核心层，一般需要非常严格的规范管理，每次修改理论上都需要进行CodeReview。</p><p><strong>DIM作为维表的概念</strong>，依据维度建模理论，数据只来自于ODS层，可以做一定加工处理操作。DIM表的调度周期一般为日调度，同步DB全量的快照数据，通常不建议采用增量进行同步。由于是快照表，因此建议永久保留，如果是拉链表则建议保留7天。DIM表有一些额外的考虑因素，即是否缓慢变化维度（选择快照还是拉链需要根据变化频次和业务场景来确定），如果维度存在层次关系，可以通过_ext或者是_tree或者是_graph的后缀进行标注。</p><p><strong>DWD作为数据仓库明细层</strong>，建设思路主要围绕事务事实表、多事务事实表、累计快照事实表等的明细类表进行建设，对于多个事务事实表可以进行合并为多事务事实表，在各个业务域中结合业务过程选择一个应用较为通用的粒度建立一张明细宽表，同时还需要进行原始数据的清洗工作。DWD表的调度周期一般为日调度，时间上建议全量快照保留，少部分采用增量（主要考虑数据是否发生修改或删除，若存在则需要使用全量）。保留周期同样为全量7天、增量永久。如果有清洗数据的需求，通常做在DWD层，而退化部分维度属性，杂项维度、行为维度、微型维度等直接退化在事实表中，对于明细宽表会考虑冗余部分常用属性提升查询效率。</p><p><strong>DWS作为数据仓库服务层</strong>，考虑建设完善的数据服务体系与数据集市，统一指标口径，主要存放周期快照事实表、聚集型事实表等汇总类型指标数据表，并且根据是否跨数据域分为两个部分进行建设。统计周期上，DWS与DWD和ODS不同，统计周期通常为：1d（最近一天）、nd（最近n天）、cm（自然月）等方式，因此需要设定通用的规范，与ODS/DWD进行区分。DWS层不存在时间上的全量，只能根据汇总的维度进行周期统计。DWS分为跨数据域和不跨数据域两类汇总表，在同一个业务过程中进行指标汇总计算可以确保数据维度的统一且高效产出，但由于DWS沉淀不足，对上游应用使用不友好，如果都放在ADS进行跨业务过程融合会过于臃肿，因此考虑在DWS进行同一业务过程内汇总的同时，也沉淀一部分通用的跨业务过程汇总表。</p><p><strong>ADS是面向应用、服务、需求而开发</strong>，命名规范不会限定的很严格，因为存在很多手工数据这种无法标准化的情况，但ADS需要严格的限定依赖深度，防止设计的太复杂导致维护成本很高。</p><p><strong>ADS作为应用数据存储层</strong>，建设思路是根据业务具体诉求，提供数据服务，公共通用指标尽量从DWS层获取，明细数据透出或其他业务可以从DWD层获取，不建议直接从ODS层获取数据。ADS尽量展示完整的原子指标（例如：有今年花费和去年花费两个指标，应用性能满足的情况下，同比指标可不提供）。</p><h3 id="数据建模的标准化"><a href="#数据建模的标准化" class="headerlink" title="数据建模的标准化"></a>数据建模的标准化</h3><p>标准化是数据建模非常重要的一步，有道是：“如果数据都是错误的，那么业务如何做出正确的决策呢？”由于数据离不开人的开发，而人通常都是会犯错误的，因此如果标准化做的不好，不光做出来的数据容易出错，而且很容易背锅。</p><p>那么数据建模如何标准化？如果有五点：</p><p><strong>第一点，数据表的命名是标准的。</strong>比如：分层_数据域_二级数据域_业务过程_业务描述_主键_统计周期_刷新周期，通常没有绝对的标准，只有相对的标准。如果数据表的命名规范做的好，不用看注释，其他人都能直接用。</p><p><strong>第二点，表指标定义是标准的，无二义性的。</strong>表的字段通常包含三个部分：实体&amp;属性、派生指标、时间定义，实体代表DWS和ADS的统计粒度，其属性是从我们的维表中关联出来的；派生指标，是基于原子指标、业务限定等组合出来的；时间定义，通常是天、小时、分钟等。数仓标准化的程度，很大一部分就是看派生指标是否规范、是否统一，很多公司做的“指标库”，本质就是做表指标定义的标准化。</p><p><strong>第三点，模型中语料是不断完善的。</strong>前面两点中，表和字段都会拆解成最小粒度了，数仓模型针对每个最小粒度都进行了定义，后续的工作中，表命名、字段命名、字段口径都应该从这些模型中能够取出来。</p><p>第四点，数据质量是有保障的。</p><p>第五点，通过数据来保障模型的规范性。还是那句话，人都是会犯错误的，那么用数据来治理数据，并做成工具天天提醒你，是目前大公司比较统一的思路。参考笔者之前的文章：<a href="http://mp.weixin.qq.com/s?__biz=MzU4MDg5NjE4OA==&mid=2247485090&idx=1&sn=8d114a9967bcff54caaa4174c61ace84&chksm=fd4e9271ca391b67f3b803f165b530f1fe96303a31c0f6e94091a7a965656e8ef0e081c52dfe&scene=21#wechat_redirect">《数据资产治理概要：用数据来治理数据》</a>。</p><h3 id="数据建模中的应用价值"><a href="#数据建模中的应用价值" class="headerlink" title="数据建模中的应用价值"></a>数据建模中的应用价值</h3><p>我们构建的数据模型实际上是一种信息流，从<font color="red">明细 -&gt; 轻度汇总 -&gt; 高度汇总的链路，细节逐渐消失，携带的信息量逐渐减少</font>。在明细我们可以看到业务细节，在汇总我们只能看到一些结论性的数据，那么底层的模型设计对外其实是不可见的，那么我们通过高度汇总的数据怎么透传数据价值呢？</p><p>其中一种方式，是做<strong>数据中台，提供数据产品、数据服务和其他一些数据基础设施，即丰富数据的应用场景可以带来较大的增量价值。</strong></p><p>另外一种方式，是做<strong>基于数据的业务中台，提供一些对业务场景的洞察</strong>。单独的数据模型其实很难产生价值，通常需要配合其他的一些业务场景，来产生增量的价值。</p><p>参考：<a href="https://mp.weixin.qq.com/s/WBuv9jPuJJx0SNmQSihY_A">数据建模理论、设计和实践</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
            <tag> 数据建模 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生成式模型和判别式模型</title>
      <link href="2021/05/23/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>2021/05/23/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="生成式模型和判别式模型"><a href="#生成式模型和判别式模型" class="headerlink" title="生成式模型和判别式模型"></a>生成式模型和判别式模型</h2><h3 id="决策函数-Y-f-X-与条件概率分布-P-Y-X"><a href="#决策函数-Y-f-X-与条件概率分布-P-Y-X" class="headerlink" title="决策函数$Y=f(X)$与条件概率分布$P(Y|X)$"></a>决策函数$Y=f(X)$与条件概率分布$P(Y|X)$</h3><p>决策函数$Y=f(x)$：输入一个$x$，它就输出一个$y$值，这个$y$与一个阈值比较，根据比较结果判定$x$属于哪个类别。</p><p>条件概率分布$P(y|x)$：输入一个$x$，它通过比较它属于所有类的概率，然后预测时应用最大后验概率法（MAP）即比较条件概率最大的类为x对应的类别。</p><p>举个例子，对于一个二分类问题：对于$Y=f(x)$形式的分类模型，如果输出$Y$大于某个阈值$V$就属于类$w_1$，否则属于类$w2$；而对于$P(y|x)$形式的分类模型，分别计算条件概率，如果$P(w_1|x)$大于$P(w_2|x)$，$x$就属于类$w_1$，否则属于类$w_2$。</p><h3 id="生成式模型与判别式模型"><a href="#生成式模型与判别式模型" class="headerlink" title="生成式模型与判别式模型"></a>生成式模型与判别式模型</h3><p>生成式模型（Generative Model）：由数据学习联合概率密度分布P(x,y)，然后生成条件概率分布P(y|x)，或者直接学得一个决策函数 Y=f(x)，用作模型预测。</p><p>判别式模型（Discriminative Model）：由数据直接学习决策函数f(x)或者条件概率分布P(y|x)作为预测。</p><h3 id="异同点"><a href="#异同点" class="headerlink" title="异同点"></a>异同点</h3><p>1.生成模型和判别模型都属于监督学习的模型。</p><p>2.生成式模型可以根据贝叶斯公式得到条件概率分布P(y|x)，但反过来不行，即判别方法不能还原出联合概率分布P(x,y)。</p><p>3.生成方法学习联合概率密度分布P(x,y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，但它不关心到底划分各类的那个分类边界在哪；判别方法不能反映训练数据本身的特性，但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。</p><h3 id="常见模型分类"><a href="#常见模型分类" class="headerlink" title="常见模型分类"></a>常见模型分类</h3><h4 id="生成式模型："><a href="#生成式模型：" class="headerlink" title="生成式模型："></a>生成式模型：</h4><p>判别式分析<br>朴素贝叶斯<br>混合高斯模型<br>隐马尔科夫模型（HMM）<br>贝叶斯网络<br>Sigmoid Belief Networks<br>马尔可夫随机场（Markov Random Fields）<br>深度信念网络（DBN）</p><h4 id="判别式模型："><a href="#判别式模型：" class="headerlink" title="判别式模型："></a>判别式模型：</h4><p>线性回归（Linear Regression）<br>逻辑斯特回归（Logistic Regression）<br>K近邻（KNN）<br>感知机<br>神经网络（NN）<br>支持向量机（SVM）<br>决策树<br>最大熵模型（maximum entropy model, MaxEnt）<br>高斯过程（Gaussian Process）<br>条件随机场（CRF）<br>区分度训练<br>boosting方法</p><h3 id="借用一个网友举的通俗易懂的例子"><a href="#借用一个网友举的通俗易懂的例子" class="headerlink" title="借用一个网友举的通俗易懂的例子"></a>借用一个网友举的通俗易懂的例子</h3><p>假如你的任务是识别一个语音属于哪种语言，对面一个人走过来，和你说了一句话，你需要识别出她说的到底是汉语、英语还是法语等。那么你可以有两种方法达到这个目的：</p><p>（1）学习每一种语言，你花了大量精力把汉语、英语和法语等都学会了，我指的学会是你知道什么样的语音对应什么样的语言。然后再有人过来对你说，你就可以知道他说的是什么语音.</p><p>（2）不去学习每一种语言，你只学习这些语言之间的差别，然后再判断（分类）。意思是指我学会了汉语和英语等语言的发音是有差别的，我学会这种差别就好了。</p><p>第一种方法就是生成方法，第二种方法是判别方法。</p><p><strong>参考</strong>：<a href="https://blog.csdn.net/qq_14997473/article/details/85219353">机器学习：生成式模型和判别式模型</a></p><p><strong>判别式模型（Discriminative Model）</strong></p><p>即判别(数据输出量)的模型，直接对条件概率$p(x|y;\theta)$建模。常见的判别式模型有线性回归模型、线性判别分析、支持向量机SVM、神经网络等。</p><p>判别式模型举例：</p><p>要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。（logistic回归，&gt;0.5为正例，否则，为反例） </p><p><strong>生成式模型（Generative Model）</strong></p><p>即生成(数据的分布)的模型，会对x和y的联合分布$p(x,y)$建模，然后通过贝叶斯公式来求得$p(x,y)$，然后选取使得$p(y_i|x)$最大的$y_i$，即：<br>$$<br>arg\ max\ {p(y|x)} = arg max \frac{p(x|y)p(y)}{P(x)}<br>= arg\ max\ p(x|y)p(y)<br>$$</p><p>常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等。</p><p>生成式模型举例：</p><p>利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。（朴素贝叶斯分类就是这样） </p><p> 常见的模型大多是判别模型，生成模型有：朴素贝叶斯（需要求联合概率分布），隐马尔科夫HMM，高斯混合模型GMM，LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层<a href="https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF">贝叶斯</a>概率模型。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week5_OOV和Word-repetition问题的改进</title>
      <link href="2021/05/22/week5_OOV%E5%92%8CWord-repetition%E9%97%AE%E9%A2%98%E7%9A%84%E6%94%B9%E8%BF%9B/"/>
      <url>2021/05/22/week5_OOV%E5%92%8CWord-repetition%E9%97%AE%E9%A2%98%E7%9A%84%E6%94%B9%E8%BF%9B/</url>
      
        <content type="html"><![CDATA[<h3 id="Out-of-Vocabulary"><a href="#Out-of-Vocabulary" class="headerlink" title="Out of Vocabulary"></a>Out of Vocabulary</h3><h4 id="why-amp-what-is-OOV"><a href="#why-amp-what-is-OOV" class="headerlink" title="why&amp;what is OOV"></a>why&amp;what is OOV</h4><p>未登录词就是训练时未出现，测试时出现了的单词。在自然语言处理或者文本处理的时候，我们通常会有一个字词库（vocabulary）。这个vocabulary要么是提前加载的，或者是自己定义的，或者是从当前数据集提取的。假设之后你有了另一个的数据集，这个数据集中有一些词并不在你现有的vocabulary里，我们就说这些词汇是Out-of-vocabulary，简称OOV。</p><p>For example, words such as “tensor” and “tensor” are present in the vocabulary of Word2Vec. But if you try to get embedding for the compound word “tensorflow”, you will get an out of vocabulary error.</p><ul><li>Typo（排版错误，文本里面存在错别字造成的oov）</li><li>Different Vocabularies（不同的词汇表，词不在你来做训练的语料库中、新词的产生并没有添加到你训练的语料库中）</li><li>new words are being created every second（每时每刻都在产生新的词汇）</li><li>you cannot afford to annotate too many data（不能注解太多的词汇）</li></ul><h4 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h4><ul><li><p>Spell check（过滤掉错误的单词）</p></li><li><p>Ignore（忽略，这种方法不常用）</p></li><li><p>UNK work（变成默认词（UNK work，可以在一定程度上缓解oov的问题，但是很多重要的词这样做后都会变成默认词，不太合适 ）</p><p>This solution is to reserve a dimension in feature space, it may eliminate some impact of OOV word, but very limited. Especially the OOV word plays an important role in the NLP task, e.g. some positive and negative words are OOV words in sentiment analysis.</p><p>这种解决方案是在特征空间中保留一个维度，它可能会消除OOV词的一些影响，但非常有限。特别是面向对象词在自然语言处理中起着重要的作用，如一些积极词和消极词在情感分析中都属于面向对象词。</p></li><li><p>Enlarge Vocabulary（速度下降，会造成数据分布不均）</p></li><li><p>Individual Characters（把单词切开，将词转换成字符（将每个token变换成字符进行切割，再根据字符拼接，这样会丢失句法和语义信息）</p></li><li><p>subword（介于token和character方法，对词频低的词不太友好）</p></li><li><p>Byte Pair Encoding：BPE（根据词频确定留哪些subword）</p></li><li><p>word piece </p></li><li><p>unigram language model(看下ppt中的步骤)。 </p></li><li><p>PGN网络（重点难点，参考一下 <a href="https://www.cnblogs.com/monkeyT/p/12337556.html%EF%BC%89">https://www.cnblogs.com/monkeyT/p/12337556.html）</a></p></li></ul><h3 id="Subword"><a href="#Subword" class="headerlink" title="Subword"></a>Subword</h3><h4 id="sub-word-generation"><a href="#sub-word-generation" class="headerlink" title="sub-word generation"></a>sub-word generation</h4><p>For a word, we generate character n-grams of length FROM n to m(n&lt;m)  present in it.</p><p>因为可能有大量唯一的n-gram，所以我们应用哈希来限制内存需求。每个字符n-gram被哈希为1到B之间的整数。</p><h4 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte Pair Encoding(BPE)"></a>Byte Pair Encoding(BPE)</h4><p>1、准备⾜够大的训练语料<br>2、确定期望的subword词表大小<br>3、将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5<br>4、统计每⼀个连续字节对的出现频率，选择最⾼频者合并成新的subword<br>5、重复第4步直到达到第2步设定的subword词表大小或下⼀个最⾼频的字节对出现频率为1</p><h4 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h4><p>1、准备⾜够大的训练语料<br>2、确定期望的subword词表大小<br>3、将单词拆分成字符序列<br>4、基于第3步数据训练语⾔模型<br>5、从所有可能的subword单元中选择加⼊语⾔模型后能最⼤程度地增加训练数据概率的单元作为新的单元<br>6、重复第5步直到达到第2步设定的subword词表⼤⼩或概率增量低于某⼀阈值</p><p><a href="https://huggingface.co/transformers/_modules/transformers/tokenization_bert.html#BertTokenizer">https://huggingface.co/transformers/_modules/transformers/tokenization_bert.html#BertTokenizer</a></p><h4 id="Unigram-Language-Model"><a href="#Unigram-Language-Model" class="headerlink" title="Unigram Language Model"></a>Unigram Language Model</h4><p>1、准备⾜够⼤的训练语料<br>2、确定期望的subword词表⼤⼩<br>3、给定词序列优化下⼀个词出现的概率<br>4、计算每个subword的损失<br>5、基于损失对subword排序并保留前X%。为了避免OOV<br>6、重复第3⾄第5步直到达到第2步设定的subword词表⼤⼩或第5步的结果不再变化</p><h3 id="Pointer-Generator-Network"><a href="#Pointer-Generator-Network" class="headerlink" title="Pointer-Generator Network"></a>Pointer-Generator Network</h3><p>文本摘要旨在将文本或文本集合转换为包含关键信息的简短摘要。按照输出类型可分为抽取式摘要和生成式摘要。<strong>抽取式摘要从源文档中抽取关键句和关键词组成摘要，摘要全部来源于原文</strong>。<strong>生成式摘要根据原文，允许生成新的词语、原文本中没有的短语来组成摘要。</strong></p><p>指针生成网络属于生成式模型。</p><p>仅用Neural sequence-to-sequence模型可以实现生成式摘要，但存在两个问题：<br><strong>1、 可能不准确地再现细节,</strong> <strong>无法处理词汇不足（OOV）单词;</strong><br><strong>2、</strong> <strong>倾向于重复自己</strong>。</p><p>原文是（they are liable to reproducefactual details inaccurately, and they tendto repeat themselves.）<br>指针生成网络（Pointer-Generator-Network）从两个方面<strong>进行了改进</strong>：</p><p><strong>1. 该网络通过指向（pointer）从源文本中复制单词，有助于准确地复制信息，同时保留通过生成器产生新单词的能力；</strong><br><strong>2. 使用coverage机制来跟踪已总结的内容，防止重复。</strong> </p><p>接下来从下面几个部分介绍Pointer-Generator-Network原理：<br>1、Baseline sequence-to-sequence;<br>2、Pointer-Generator-Network;<br>3、Coverage Mechanism。</p><h4 id="Baseline-sequence-to-sequence"><a href="#Baseline-sequence-to-sequence" class="headerlink" title="Baseline sequence-to-sequence"></a>Baseline sequence-to-sequence</h4><p>Pointer-Generator Networks是在Baseline sequence-to-sequence模型的基础上构建的，我们首先Baseline seq2seq+attention。其架构图如下：</p><p><img src="/2021/05/22/week5_OOV%E5%92%8CWord-repetition%E9%97%AE%E9%A2%98%E7%9A%84%E6%94%B9%E8%BF%9B/Baseline_Seq2Seq.png"></p><p>该模型可以<strong>关注原文本中的相关单词以生成新单词</strong>进行概括。比如：模型可能<strong>注意到原文</strong>中的**”victorious”<strong>和</strong>“ win”<strong>这个两个单词，在摘要”Germany beat Argentina 2-0”中</strong>生成了新的单词beat** 。</p><p>Seq2Seq的模型结构是<strong>经典的Encoder-Decoder模型</strong>，即先用Encoder将原文本编码成一个中间层的隐藏状态，然后用Decoder来将该隐藏状态解码成为另一个文本。Baseline Seq2Seq在<strong>Encoder端</strong>是一个双向的LSTM，这个<strong>双向的LSTM可以捕捉原文本的长距离依赖关系以及位置信息，</strong>编码时词嵌入经过双向LSTM后得到编码状态$h_i$。在<strong>Decoder端</strong>，<strong>解码器是一个单向的LSTM</strong>，训练阶段时参考摘要词依次输入(测试阶段时是上一步的生成词)，在时间步$t$得到解码状态$s_t$。使用$h_i$和$s_t$得到该时间步原文第$i$个词注意力权重。</p><h4 id="Pointer-Generator-Network-1"><a href="#Pointer-Generator-Network-1" class="headerlink" title="Pointer-Generator-Network"></a>Pointer-Generator-Network</h4><p>原文中的Pointer-Generator Networks是一个混合了 Baseline seq2seq和PointerNetwork的网络，它具有Baseline seq2seq的生成能力和PointerNetwork的Copy能力。该网络的结构如下：</p><p><img src="/2021/05/22/week5_OOV%E5%92%8CWord-repetition%E9%97%AE%E9%A2%98%E7%9A%84%E6%94%B9%E8%BF%9B/pointer-generationer-structure.png"></p><p><strong>如何权衡一个词应该是生成的还是复制的？</strong></p><p>　　<strong>原文中引入了一个权重$p_{gen}$ 。</strong></p><p>　　从Baseline seq2seq的模型结构中得到了$s_t$和$h_t^*$，和解码器输入$x_t$一起来计算$p_{gen}$：<br>$$<br>p_{gen} = \sigma(w_{h^*}^T h_t^* + w_s^Ts_t + w_x^Tx_t + b_{ptr})<br>$$<br>　　这时，会扩充单词表形成一个更大的单词表–扩充单词表(将原文当中的单词也加入到其中)，该时间步的预测词概率为：</p><p>$$<br>P(w) = p_{gen}P_{vocab}(w) + (1 - p_{gen}) \sum_{i:w_i=w} a_i^t<br>$$</p><p>　　其中 atiait 表示的是原文档中的词。我们可以看到解码器一个词的输出概率有其是否拷贝是否生成的概率和决定。<strong>当一个词不出现在常规的单词表上时$P_{vocab}(w)$ 为0，当该词不出现在文档中$\sum_{i:w_i=w} a_i^t$为0。</strong></p><h4 id="Coverage-mechanism"><a href="#Coverage-mechanism" class="headerlink" title="Coverage mechanism"></a>Coverage mechanism</h4><p>原文的特色是运用了Coverage Mechanism来解决重复生成文本的问题，下图反映了前两个模型与添加了Coverage Mechanism生成摘要的结果：</p><p><img src="/2021/05/22/week5_OOV%E5%92%8CWord-repetition%E9%97%AE%E9%A2%98%E7%9A%84%E6%94%B9%E8%BF%9B/CoverageMechanism.png"></p><p>蓝色的字体表示的是参考摘要，三个模型的生成摘要的结果差别挺大<br>红色字体表明了不准确的摘要细节生成(UNK未登录词，无法解决OOV问题);<br>绿色的字体表明了模型生成了重复文本。<br>为了解决此问题–Repitition，原文使用了在机器翻译中解决“过翻译”和“漏翻译”的机制–Coverage Mechanism。</p><p>具体实现上，就是将先前时间步的注意力权重加到一起得到所谓的覆盖向量 ct(coveragevector)，用先前的注意力权重决策来影响当前注意力权重的决策，这样就避免在同一位置重复，从而避免重复生成文本。</p><p><a href="https://www.cnblogs.com/think90/p/11590469.html">指针生成网络(Pointer-Generator-Network)原理与实战</a></p><p><a href="https://www.cnblogs.com/zingp/p/11571593.html">指针生成网络(Pointer-Generator-Network)原理与实战</a></p><h3 id="Word-Repetition"><a href="#Word-Repetition" class="headerlink" title="Word Repetition"></a>Word Repetition</h3><p>生成式任务存在文本重复问题解决方法–Coverage(每次预测都会的到概率分布，根据概率去强迫模型关注之前没有关注的内容)。</p><p><img src="/2021/05/22/week5_OOV%E5%92%8CWord-repetition%E9%97%AE%E9%A2%98%E7%9A%84%E6%94%B9%E8%BF%9B/word_repetion.png"></p><h3 id="Text-Summarization-在阿⾥"><a href="#Text-Summarization-在阿⾥" class="headerlink" title="Text Summarization 在阿⾥"></a>Text Summarization 在阿⾥</h3><p>[论文：Multi-Source Pointer Network for Product Title Summarization](<a href="https://arxiv.org/pdf/1808.06885.pdf">Multi-Source Pointer Network for Product Title Summarization (arxiv.org)</a>)</p><p>商品标题摘要这个特殊问题天然存在对模型的两个限制：<br>1）摘要中不能引⼊不相关信息<br>2）摘要中必须保留源⽂本的关键信息（如品牌名和商品名）</p><h3 id="Hyper-Parameter-Tuning"><a href="#Hyper-Parameter-Tuning" class="headerlink" title="Hyper-Parameter Tuning"></a>Hyper-Parameter Tuning</h3><h4 id="Hand-Turning"><a href="#Hand-Turning" class="headerlink" title="Hand Turning"></a>Hand Turning</h4><h5 id="Good-Coding-Style"><a href="#Good-Coding-Style" class="headerlink" title="Good Coding Style"></a>Good Coding Style</h5><ul><li>将各个参数的设置部分集中在⼀起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会⾮常痛苦。</li><li>可以输出模型的损失函数值以及训练集和验证集上的准确率。</li><li>可以考虑设计⼀个⼦程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。再由⼀个主程序，分配参数以及并⾏启动⼀系列子程序。</li></ul><h5 id="General-to-Specific"><a href="#General-to-Specific" class="headerlink" title="General to Specific"></a>General to Specific</h5><ul><li>建议先参考相关论⽂，以论⽂中给出的参数作为初始参数。⾄少论⽂中的参数，是个不差的结果。</li><li>如果找不到参考，那么只能⾃⼰尝试了。可以先从⽐较重要，对实验结果影响⽐较⼤的参数开始，同时固定其他参数，得到⼀个差不多的结果以后，在这个结果的基础上，再调其他参数。<br>例如学习率⼀般就⽐正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚⾄会⽆法收敛。</li><li>如果实在找不到⼀组参数让模型收敛。那么就需要检查，是不是其他地⽅出了问题，例如模型实现，数据等等。</li></ul><h5 id="Speed-up-Experiment"><a href="#Speed-up-Experiment" class="headerlink" title="Speed up Experiment"></a>Speed up Experiment</h5><ul><li>对训练数据进⾏采样。例如原来100W条数据，先采样成1W，进⾏实验看看。</li><li>减少训练类别。例如⼿写数字识别任务，原来是10个类别， 那么我们可以先在2个类别上训练，看看结果如何。</li></ul><h5 id="Experiment-Number"><a href="#Experiment-Number" class="headerlink" title="Experiment Number"></a>Experiment Number</h5><ul><li>learning rate: 1 0.1 0.01 0.001, ⼀般从1开始尝试。很少见learning rate⼤于10的。<br>学习率⼀般要随着训练进⾏衰减。<br>衰减系数⼀般是0.5。<br>衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。<br>不过更建议使⽤⾃适应梯度的办法，例如adam,adadelta,rmsprop等，这些⼀般使⽤相关论⽂提供的默认值即可，可以避免再费劲调节学习率。<br>对RNN来说，有个经验，如果RNN要处理的序列⽐较长，或者RNN层数⽐较多，那么learning rate⼀般⼩⼀些⽐较好，否则有可能出现结果不收敛，甚⾄Nan等问题。</li><li>⽹络层数： 先从1层开始。</li><li>每层结点数： 16 32 128，超过1000的情况⽐较少见。超过1W的从来没有见过。</li><li>batch size: 128上下开始。batch size值增加，的确能提⾼训练速度。但是有可能收敛结果变差。如果显存⼤⼩允许，可以考虑从⼀个⽐较⼤的值开始尝试。因为batch size太⼤，⼀般不会对结果有太⼤的影响，⽽batch size太⼩的话，结果有可能很差。</li><li>clip c(梯度裁剪): 限制最⼤梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算⼀个衰减系系数,让value的值等于阈值: 5,10,15</li><li>dropout： 0.5</li><li> L2正则：1.0，超过10的很少见。</li><li>词向量embedding⼤⼩：128，256</li><li>正负样本⽐例： 这个是⾮常忽视，但是在很多分类问题上，又⾮常重要的参数。很多⼈往往习惯使⽤训练数据中默认的正负类别⽐例，当训练数据⾮常不平衡的时候，模型很有可能会偏向数⽬较⼤的类别，从⽽影响最终训练结果。除了尝试训练数据默认的正负类别⽐例之外，建议对数⽬较⼩的样本做过采样，例如进⾏复制。提⾼他们的⽐例，看看效果如何，这个对多分类问题同样适⽤。在使⽤mini-batch⽅法进⾏训练的时候，尽量让⼀个batch内，各类别的⽐例平衡，这个在图像识别等多分类任务上⾮常重要。</li></ul><h5 id="HP-Range"><a href="#HP-Range" class="headerlink" title="HP Range"></a>HP Range</h5><p>建议优先在对数尺度上进⾏超参数搜索。⽐较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进⾏尝试。<br>因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进⾏搜索，例如dropout值: 0.3 0.5 0.7)。</p><h4 id="Auto-Turning"><a href="#Auto-Turning" class="headerlink" title="Auto Turning"></a>Auto Turning</h4><p>Tensorboard（Keras Turner）</p><p><a href="https://github.com/tensorflow/adanet">Adanet</a></p><p><a href="https://github.com/h2oai/h2o-3">H2O.ai</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 摘要自动生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 摘要自动生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week4_NLG过程的优化与项目Inference</title>
      <link href="2021/05/16/week4_NLG%E8%BF%87%E7%A8%8B%E7%9A%84%E4%BC%98%E5%8C%96%E5%92%8C%E9%A1%B9%E7%9B%AEInference/"/>
      <url>2021/05/16/week4_NLG%E8%BF%87%E7%A8%8B%E7%9A%84%E4%BC%98%E5%8C%96%E5%92%8C%E9%A1%B9%E7%9B%AEInference/</url>
      
        <content type="html"><![CDATA[<h2 id="Text-Summarization-NLG"><a href="#Text-Summarization-NLG" class="headerlink" title="Text Summarization - NLG"></a>Text Summarization - NLG</h2><p>Natural Language Generation</p><h3 id="NLG-Introduction"><a href="#NLG-Introduction" class="headerlink" title="NLG Introduction"></a>NLG Introduction</h3><h4 id="1、Application"><a href="#1、Application" class="headerlink" title="1、Application"></a>1、Application</h4><p>Translation</p><p>Document Summarization</p><p>E-mail Summarization</p><p>Meeting Summarization</p><h4 id="2、What’s-Natural-Language-Generation"><a href="#2、What’s-Natural-Language-Generation" class="headerlink" title="2、What’s Natural Language Generation?"></a>2、What’s Natural Language Generation?</h4><p>Any task involving text production for human consumption requires natural language generation</p><p>NLG 是为了跨越人类和机器之间的沟通鸿沟，将非语言格式的数据转换成人类可以理解的语言格式，如文章、报告等。</p><h4 id="3、自然语言生成的2种方式："><a href="#3、自然语言生成的2种方式：" class="headerlink" title="3、自然语言生成的2种方式："></a>3、自然语言生成的2种方式：</h4><p>text – to – text：文本到语言的生成</p><p>data – to – text ：数据到语言的生成</p><h4 id="4、NLG的3个LEVEL"><a href="#4、NLG的3个LEVEL" class="headerlink" title="4、NLG的3个LEVEL"></a>4、NLG的3个LEVEL</h4><p>1、<strong>简单的数据合并：</strong>自然语言处理的简化形式，这将允许将数据转换为文本（通过类似Excel的函数）。为了关联，以<a href="https://baike.baidu.com/item/%E9%82%AE%E4%BB%B6%E5%90%88%E5%B9%B6/7804213?fr=aladdin">邮件合并</a>（MS Word mailmerge）为例，其中间隙填充了一些数据，这些数据是从另一个源（例如MS Excel中的表格）中检索的。</p><p>2、<strong>模板化的 NLG</strong> ：这种形式的NLG使用模板驱动模式来显示输出。以足球比赛得分板为例。数据动态地保持更改，并由预定义的业务规则集（如if / else循环语句）生成。</p><p>3、<strong>高级 NLG</strong> ：这种形式的自然语言生成就像人类一样。它理解意图，添加智能，考虑上下文，并将结果呈现在用户可以轻松阅读和理解的富有洞察力的叙述中。</p><h4 id="5、NLG-的6个步骤"><a href="#5、NLG-的6个步骤" class="headerlink" title="5、NLG 的6个步骤"></a>5、NLG 的6个步骤</h4><p><strong>第一步：内容确定 – Content Determination</strong></p><p>作为第一步，NLG 系统需要决定哪些信息应该包含在正在构建的文本中，哪些不应该包含。通常数据中包含的信息比最终传达的信息要多。</p><p><strong>第二步：文本结构 – Text Structuring</strong></p><p>确定需要传达哪些信息后，NLG 系统需要合理的组织文本的顺序。例如在报道一场篮球比赛时，会优先表达“什么时间”“什么地点”“哪2支球队”，然后再表达“比赛的概况”，最后表达“比赛的结局”。</p><p><strong>第三步：句子聚合 – Sentence Aggregation</strong></p><p>不是每一条信息都需要一个独立的句子来表达，将多个信息合并到一个句子里表达可能会更加流畅，也更易于阅读。</p><p><strong>第四步：语法化 – Lexicalisation</strong></p><p>当每一句的内容确定下来后，就可以将这些信息组织成自然语言了。这个步骤会在各种信息之间加一些连接词，看起来更像是一个完整的句子。</p><p><strong>第五步：参考表达式生成 – Referring Expression Generation|REG</strong></p><p>这个步骤跟语法化很相似，都是选择一些单词和短语来构成一个完整的句子。不过他跟语法化的本质区别在于“REG需要识别出内容的领域，然后使用该领域（而不是其他领域）的词汇”。</p><p><strong>第六步：语言实现 – Linguistic Realisation</strong></p><p>最后，当所有相关的单词和短语都已经确定时，需要将它们组合起来形成一个结构良好的完整句子。</p><h4 id="6、NLG-的3种典型应用"><a href="#6、NLG-的3种典型应用" class="headerlink" title="6、NLG 的3种典型应用"></a>6、NLG 的3种典型应用</h4><p>NLG 的不管如何应用，大部分都是下面的3种目的：</p><ol><li>能够大规模的产生个性化内容</li><li>帮助人类洞察数据，让数据更容易理解</li><li>加速内容生产</li></ol><h3 id="NLG-Decoding-Strategy"><a href="#NLG-Decoding-Strategy" class="headerlink" title="NLG Decoding Strategy"></a>NLG Decoding Strategy</h3><h4 id="1、Greedy-Decoding"><a href="#1、Greedy-Decoding" class="headerlink" title="1、Greedy Decoding"></a>1、Greedy Decoding</h4><p>Greedy Decoding（贪心算法）：挑选P中概率最高的一个作为预测结果，停止条件是遇到&lt;end&gt;。贪心算法存在一些问题：比如重复（重复的语句会降低模型的loss，也就是说模型可以从重复的语句中学到东西）、局部最优并不是全局最优，结果存在一定偏差。需要结合老师给的代码理解一下原理。</p><h5 id="1-1-Greedy-Search"><a href="#1-1-Greedy-Search" class="headerlink" title="1.1 Greedy Search"></a>1.1 Greedy Search</h5><p>核心思想：每一步取当前最可能的结果，作为最终结果。</p><p>On each step of decoder, keep track of the k most probable partial  translations (which we call hypotheses)。</p><p>在解码器的每一步，跟踪k个最可能的部分翻译(我们称之为假设)。</p><p>具体方法：获得新生成的词是vocab中各个词的概率，取argmax作为需要生成的词向量索引，继而生成后一个词。</p><h5 id="1-2-Beam-Search"><a href="#1-2-Beam-Search" class="headerlink" title="1.2 Beam Search"></a>1.2 Beam Search</h5><p>对比贪心算法，beam search取到了概率中最大的K个值进行下一步的运算，每一步都取到概率中的Top-k。Beam Search也有一些限制条件：</p><blockquote><p>1、timestep T（预定义值需要自己根据应用场景来设定）</p><p>2、生成假设(hypothese)的数量是需要考虑的。K的数值代表beam size，beam size设置的大小也有很多的影响，具体表现为：</p><p>beam size过小时会出现生成的语句没有语法性、不自然、不顺畅、不正确等问题；</p><p>beam size变大时上面的问题会缓解，但是也存在一些缺点：</p><blockquote><p>1、k变大，相应的计算代价也会变大，</p><p>2、Blue score指标会下降的非常快，</p><p>3、生成的句子变短了，</p><p>4、topic相关度会下降。</p></blockquote></blockquote><p>核心思想：beam search尝试在广度优先基础上进行进行搜索空间的优化（类似于剪枝）达到减少内存消耗的目的。</p><p>具体方法：在decoding的每个步骤，我们都保留着 top K 个可能的候选单词，然后到了下一个步骤的时候，我们对这 K 个单词都做下一步 decoding，分别选出 top K，然后对这 $K^2$ 个候选句子再挑选出 top K 个句子。以此类推一直到 decoding 结束为止。当然 Beam Search 本质上也是一个 greedy decoding 的方法，所以我们无法保证自己一定可以得到最好的 decoding 结果。</p><h5 id="1-3-Greedy-Decoding与Beam-Search存在问题"><a href="#1-3-Greedy-Decoding与Beam-Search存在问题" class="headerlink" title="1.3 Greedy Decoding与Beam Search存在问题"></a>1.3 Greedy Decoding与Beam Search存在问题</h5><p>①容易出现很无聊的回答：I don’t know.</p><p>②容易重复自己：I don’t know. I don’t know. I don’t know. I don’t know. I don’t know. I don’t know.</p><h5 id="1-4-NLG中重复性问题"><a href="#1-4-NLG中重复性问题" class="headerlink" title="1.4 NLG中重复性问题"></a>1.4 NLG中重复性问题</h5><p>出现重复性原因是重复的语句会降低模型的loss，也就是说模型可以从重复的语句中学到东西。</p><p>解决方法：</p><p>1、将前面生成的文本写入到n-gram中，</p><p>2、在loss中加入多项式惩罚相似文本，</p><p>3、对loss进行改变，</p><p>4、F2 softmax,将频率分类，再和原loss相乘。</p><h4 id="2、Sampling引入随机性"><a href="#2、Sampling引入随机性" class="headerlink" title="2、Sampling引入随机性"></a>2、Sampling引入随机性</h4><p>核心思想： 根据单词的概率分布随机采样</p><h5 id="2-1-随机Sampling-vocab-y-i"><a href="#2-1-随机Sampling-vocab-y-i" class="headerlink" title="2.1 随机Sampling(vocab($y_i$))"></a>2.1 随机Sampling(vocab($y_i$))</h5><p>我们可以在生成文本的时候引入一些随机性。例如现在语言模型告诉我们下一个单词在整个单词表上的概率分布是$$p = (p_1, p_2, … p_|V|)$$,那么我们就可以按照这个概率分布进行随机采样，然后决定下一个单词生成什么。采样相对于greedy方法的好处是，我们生成的文字开始有了一些随机性，不会总是生成很机械的回复了。</p><h5 id="2-2-随机Sampling存在问题"><a href="#2-2-随机Sampling存在问题" class="headerlink" title="2.2 随机Sampling存在问题"></a>2.2 随机Sampling存在问题</h5><p>①生成的话容易不连贯，上下文比较矛盾。</p><p>②容易生成奇怪的话，出现罕见词。</p><h5 id="2-3-top-k-sampling"><a href="#2-3-top-k-sampling" class="headerlink" title="2.3 top-k sampling"></a>2.3 top-k sampling</h5><p>可以缓解生成罕见单词的问题。比如说，我们可以每次只在概率最高的50个单词中按照概率分布做采样。</p><p>我只保留top-k个probability的单词，然后在这些单词中根据概率做sampling。</p><h5 id="2-4-Neucleus-Sampling"><a href="#2-4-Neucleus-Sampling" class="headerlink" title="2.4 Neucleus Sampling"></a>2.4 Neucleus Sampling</h5><p>Neucleus Sampling的基本思想是，top p sampling，例如设置一个threshold，p=0.95。</p><h4 id="3、Sampling-Strategy"><a href="#3、Sampling-Strategy" class="headerlink" title="3、Sampling Strategy"></a>3、Sampling Strategy</h4><p>常用的随机采样会随机选取到文本出现概率很低的词语，这样对结果很不利，所以会有一些采样的策略来解决这些问题。</p><p>方法：</p><h5 id="1、Temperature-Sampling"><a href="#1、Temperature-Sampling" class="headerlink" title="1、Temperature Sampling"></a>1、Temperature Sampling</h5><p>通过选取temperature大小，来将重要词的概率变大，将不重要词的概率变小，突出重要部分。</p><h5 id="2、Top-K-Sampling"><a href="#2、Top-K-Sampling" class="headerlink" title="2、Top-K Sampling"></a>2、Top-K Sampling</h5><p>根据不同场景选取不同top-k个样本，存在问题：会把距离top近的其他样本舍弃掉。</p><h5 id="3、Top-p-p代表percentage"><a href="#3、Top-p-p代表percentage" class="headerlink" title="3、Top-p(p代表percentage)"></a>3、Top-p(p代表percentage)</h5><p>本质上Top-p sampling和Top-k sampling都是从truncated vocabulary distribution中sample token,区别在于置信区间的选择不同。</p><h3 id="NLG的评估指标"><a href="#NLG的评估指标" class="headerlink" title="NLG的评估指标"></a>NLG的评估指标</h3><p>1、Rouge-N(N代表n-gram)</p><p>2、Rouge-L(L代表the longest common subseqence 最长子序列)</p><p>3、Rouge-s(s代表skip-gram)</p><p>这几种评价指标前两种用的较多，需要重点掌握。</p><h3 id="现在自然语言生成Structure"><a href="#现在自然语言生成Structure" class="headerlink" title="现在自然语言生成Structure"></a>现在自然语言生成Structure</h3><p><img src="/2021/05/16/week4_NLG%E8%BF%87%E7%A8%8B%E7%9A%84%E4%BC%98%E5%8C%96%E5%92%8C%E9%A1%B9%E7%9B%AEInference/structure.png"></p><p>1、<a href="https://blog.csdn.net/weixin_42615068/article/details/93767781">CTC loss的几种解码方法：贪心搜索 （greedy search）、束搜索（Beam Search）、前缀束搜索（Prefix Beam Search）</a></p><p>2、<a href="https://blog.csdn.net/sollasido/article/details/10798653">NLP（一）文本生成 –Sampling问题</a>)</p><p>3、<a href="https://blog.csdn.net/jingfudu1538/article/details/88863247">Pretraining-Based Natural Language Generation for Text Summarization</a></p><p>4、<a href="https://www.csie.ntu.edu.tw/~yvchen/s105-icb/doc/170502_NLG.pdf">自然语言简报</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 摘要自动生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 摘要自动生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week3_基于Seq2Seq架构的模型搭建</title>
      <link href="2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/"/>
      <url>2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="Text-Summarization"><a href="#Text-Summarization" class="headerlink" title="Text Summarization"></a>Text Summarization</h3><h4 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h4><p>Automatic text summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning.</p><p>自动文本摘要是生成简洁流畅的摘要，同时保留关键信息内容和整体含义的任务</p><h4 id="categories"><a href="#categories" class="headerlink" title="categories"></a>categories</h4><h5 id="Extractive-Summarization"><a href="#Extractive-Summarization" class="headerlink" title="Extractive Summarization"></a>Extractive Summarization</h5><p>抽取式自动文摘方法，通过提取文档中已存在的关键词，句子形成摘要</p><h5 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h5><p>生成式自动文摘方法，通过建立抽象的语意表示，使用自然语言生成技术，形成摘要。由于生成式自动摘要方法需要复杂的自然语言理解和生成技术支持，应用领域受限。</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/summary-approaches.png" style="zoom: 60%;"><p><strong><a href="https://www.jiqizhixin.com/articles/2019-03-25-7">文本摘要简述</a></strong></p><p><strong><a href="http://xcfeng.net/res/presentation/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81%E7%AE%80%E8%BF%B0.pdf">文本摘要简述 - Xiachong Feng</a></strong></p><p>目前主要方法有：</p><p>基于统计：统计词频，位置等信息，计算句子权值，再简选取权值高的句子作为文摘，特点：简单易用，但对词句的使用大多仅停留在表面信息。</p><p>基于图模型：构建拓扑结构图，对词句进行排序。例如，TextRank/LexRank</p><p>基于潜在语义：使用主题模型，挖掘词句隐藏信息。例如，采用LDA，HMM</p><p>基于整数规划：将文摘问题转为整数线性规划，求全局最优解。</p><h3 id="Text-Rank"><a href="#Text-Rank" class="headerlink" title="Text Rank"></a>Text Rank</h3><p>TextRank 算法是一种用于文本的基于图的排序算法。其基本思想来源于谷歌的 PageRank算法,通过把文本分割成若干组成单元（单词、句子）并建立图模型，利用投票机制对文本中的重要成分进行排序，仅利用单篇文档本身的信息即可实现关键词提取、文摘。和 LDA、M 等模型不同，TextRank不需要事先对多篇文档进行学习训练，因其简洁有效而得到广泛应用。</p><p><a href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">最早提出TextRank的论文</a></p><p><a href="https://www.cnblogs.com/coshaho/p/9740937.html">马尔科夫链及其平稳状态</a></p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/TextRank.png"></p><p><strong><a href="https://www.jiqizhixin.com/articles/2018-12-28-18">TextRank算法的流程</a></strong></p><p><strong>1.</strong> 第一步是把所有文章整合成文本数据</p><p><strong>2.</strong> 接下来把文本分割成单个句子</p><p><strong>3.</strong> 然后，我们将为每个句子找到向量表示（词向量）。</p><p><strong>4.</strong> 计算句子向量间的相似性并存放在矩阵中</p><p><strong>5.</strong> 然后将相似矩阵转换为以句子为节点、相似性得分为边的图结构，用于句子TextRank计算。</p><p><strong>6.</strong> 最后，一定数量的排名最高的句子构成最后的摘要。 </p><p><strong>TFIDF&amp;TextRank对比总结</strong></p><blockquote><p>TextRank与TFIDF均严重依赖于分词结果——如果某词在分词时被切分成了两个词，那么在做关键词提取时无法将两个词黏合在一起（TextRank有部分黏合效果，但需要这两个词均为关键词）。因此是否添加标注关键词进自定义词典，将会造成准确率、召回率大相径庭。</p><p>TextRank的效果并不优于TFIDF。</p><p>TextRank虽然考虑到了词之间的关系，但是仍然倾向于将频繁词作为关键词。</p></blockquote><p>此外，由于TextRank涉及到构建词图及迭代计算，所以提取速度较慢。</p><p>发现以上两种方法本质上还是基于词频，这也导致了我们在进行自然语言处理的时候造成的弊端，因为我们阅读一篇文章的时候，并不是意味着主题词会一直出现，特别对于中文来说，蕴含的中心思想也往往不是一两个词能够说明的，这也是未来自然语言方面要解决的基于语义的分析，路还很长。</p><h3 id="Recurrent-Neural-Network-RNN"><a href="#Recurrent-Neural-Network-RNN" class="headerlink" title="Recurrent Neural Network (RNN)"></a>Recurrent Neural Network (RNN)</h3><p><strong><a href="https://zybuluo.com/hanbingtao/note/541458">循环神经网络</a></strong></p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/classical_RNN.png" style="zoom: 50%;"><p>RNN 跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。如下图所示：</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/RNN_Architecture.gif"><p>假如需要判断用户的说话意图（问天气、问时间、设置闹钟…），用户说了一句“what time is it？”我们需要先对这句话进行分词：</p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/RNN_Layers.gif"></p><p>当我们判断意图的时候，只需要最后一层的输出「05」，如下图所示：</p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/RNN-output.gif"></p><p><strong>Solve Vanishing Gradient</strong></p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/solve_vanishing_gradient.png" style="zoom:50%;"><p><a href="https://arxiv.org/pdf/1705.08209.pdf"><strong>Solve Exploding Gradient</strong></a></p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/solve_exploding_gradient.png" style="zoom:50%;"><p><strong>RNN优点</strong></p><p>1、The main advantage of RNN over ANN is that RNN <strong>can model sequence of data</strong> (i.e. time series) so that each sample can be assumed to be dependent on previous ones<br>2、Share Parameters</p><p><strong>RNN 的缺点也比较明显</strong></p><p>通过上面的例子，我们已经发现，短期的记忆影响较大（如橙色区域），但是长期的记忆影响就很小（如黑色和绿色区域），这就是 RNN 存在的短期记忆问题。</p><p>1、**<font color="red"> Gradient vanishing and exploding problems </font>**</p><p>2、Training an RNN is a very difficult task</p><p>3、<strong>It cannot process very long sequences if using tanh or relu as an activation function</strong></p><p>4、RNN 有短期记忆问题，无法处理很长的输入序列</p><p>5、训练 RNN 需要投入极大的成本</p><h3 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short Term Memory (LSTM)"></a>Long Short Term Memory (LSTM)</h3><p><strong><a href="https://zhuanlan.zhihu.com/p/32085405">人人都能看懂的LSTM</a></strong></p><p><strong><a href="https://zybuluo.com/hanbingtao/note/581764">零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a></strong></p><p>RNN 是一种死板的逻辑，越晚的输入影响越大，越早的输入影响越小，且无法改变这个逻辑。</p><p><a href="https://easyai.tech/ai-definition/lstm/">LSTM</a> 做的最大的改变就是打破了这个死板的逻辑，而改用了一套灵活了逻辑——只保留重要的信息。</p><p><strong>简单说就是：抓重点！</strong></p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/long_short_term_memory.png"></p><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><strong><a href="https://zhuanlan.zhihu.com/p/32481747">人人都能看懂的GRU</a></strong></p><p>GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。</p><p>GRU和LSTM在很多情况下实际表现上相差无几，那么为什么我们要使用新人GRU（2014年提出）而不是相对经受了更多考验的LSTM（1997提出）呢。</p><p>下图1-1引用论文中的一段话来说明GRU的优势所在。</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/R_NET.png" style="zoom: 50%;"><blockquote><p>简单译文：我们在我们的实验中选择GRU是因为它的实验效果与LSTM相似，但是更易于计算。</p></blockquote><p><strong>简单来说就是贫穷限制了我们的计算能力…</strong></p><p>相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。</p><p>GRU输入输出的结构与普通的RNN相似，其中的内部思想与LSTM相似。</p><p>与LSTM相比，GRU内部少了一个”门控“，参数比LSTM少，但是却也能够达到与LSTM相当的功能。考虑到硬件的<strong>计算能力</strong>和<strong>时间成本</strong>，因而很多时候我们也就会选择更加”实用“的GRU啦。</p><p><strong>Differences &amp; Trade-off</strong></p><p>1、对于 LSTM 与 GRU ⽽⾔， 由于 GRU 参数更少，收敛速度更快，因此其实际花费时间要少很多，这可以⼤⼤加速了我们的迭代过程。<br>2、⽽从表现上讲，⼆者之间孰优孰劣并没有定论，这要依据具体的任务和数据集⽽定，⽽实际上，⼆者之间的 performance 差距往往并不⼤，远没有调参所带来的效果明显，与其争论 LSTM 与 GRU 孰优孰劣， 不如在 LSTM 或 GRU的激活函数（如将tanh改为tanh变体）和权重初始化上功夫。<br>3、⼀般来说，我会选择GRU作为基本的单元，因为它收敛速度快，可以加速试验进程，快速迭代，⽽我认为快速迭代这⼀特点很重要。如果实现没其余优化技，才会尝试将 GRU 换为 LSTM，看看有没有什么惊喜发⽣。</p><h3 id="Seq2seq-Architecture"><a href="#Seq2seq-Architecture" class="headerlink" title="Seq2seq Architecture"></a>Seq2seq Architecture</h3><h4 id="什么是-Encoder-Decoder"><a href="#什么是-Encoder-Decoder" class="headerlink" title="什么是 Encoder-Decoder"></a><a href="https://easyai.tech/ai-definition/encoder-decoder-seq2seq/">什么是 Encoder-Decoder</a></h4><p>Encoder-Decoder 模型主要是 NLP 领域里的概念。它并不特值某种具体的算法，而是一类算法的统称。Encoder-Decoder 算是一个通用的框架，在这个框架下可以使用不同的算法来解决不同的任务。</p><p>Encoder-Decoder 这个框架很好的诠释了机器学习的核心思路：将现实问题转化为数学问题，通过求解数学问题，从而解决现实问题。</p><p>Encoder 又称作编码器。它的作用就是「将现实问题转化为数学问题」</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/encoder.png" style="zoom:50%;"><p>Decoder 又称作解码器，他的作用是「求解数学问题，并转化为现实世界的解决方案」</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/decoder.png" style="zoom:50%;"><p>把 2 个环节连接起来，用通用的图来表达则是下面的样子:</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/Encoder-Decoder.png" style="zoom:50%;"><p>关于 Encoder-Decoder，有2 点需要说明：</p><ol><li>不论输入和输出的长度是什么，中间的「向量 c」 长度都是固定的（这也是它的缺陷，下文会详细说明）</li><li>根据不同的任务可以选择不同的编码器和解码器（可以是一个 <em><a href="https://easyai.tech/ai-definition/rnn/">RNN</a></em> ，但通常是其变种 <em><a href="https://easyai.tech/ai-definition/lstm/">LSTM</a></em> 或者 <em>GRU</em> ）</li></ol><p>只要是符合上面的框架，都可以统称为 Encoder-Decoder 模型。</p><h4 id="什么是-Seq2Seq"><a href="#什么是-Seq2Seq" class="headerlink" title="什么是 Seq2Seq"></a>什么是 Seq2Seq</h4><p>Seq2Seq（是 Sequence-to-sequence 的缩写），就如字面意思，输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。例如下图：</p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/seq2seq.gif"></p><p>如上图：输入了 6 个汉字，输出了 3 个英文单词。输入和输出的长度不同。</p><h4 id="Seq2Seq-的由来"><a href="#Seq2Seq-的由来" class="headerlink" title="Seq2Seq 的由来"></a><strong>Seq2Seq 的由来</strong></h4><p>在 Seq2Seq 框架提出之前，深度神经网络在图像分类等问题上取得了非常好的效果。在其擅长解决的问题中，输入和输出通常都可以表示为固定长度的向量，如果长度稍有变化，会使用补零等操作。</p><p>然而许多重要的问题，例如机器翻译、语音识别、自动对话等，表示成序列后，其长度事先并不知道。因此如何突破先前深度神经网络的局限，使其可以适应这些场景，成为了13年以来的研究热点，Seq2Seq框架应运而生。</p><h4 id="「Seq2Seq」和「Encoder-Decoder」的关系"><a href="#「Seq2Seq」和「Encoder-Decoder」的关系" class="headerlink" title="「Seq2Seq」和「Encoder-Decoder」的关系"></a><strong>「Seq2Seq」和「Encoder-Decoder」的关系</strong></h4><p>Seq2Seq（强调目的）不特指具体方法，满足「输入序列、输出序列」的目的，都可以统称为 Seq2Seq 模型。</p><p>而 Seq2Seq 使用的具体方法基本都属于Encoder-Decoder 模型（强调方法）的范畴。</p><p>总结一下的话：</p><ul><li>Seq2Seq 属于 Encoder-Decoder 的大范畴</li><li>Seq2Seq 更强调目的，Encoder-Decoder 更强调方法</li></ul><h4 id="Encoder-Decoder-的缺陷"><a href="#Encoder-Decoder-的缺陷" class="headerlink" title="Encoder-Decoder 的缺陷"></a>Encoder-Decoder 的缺陷</h4><p>上文提到：Encoder（编码器）和 Decoder（解码器）之间只有一个「向量 c」来传递信息，且 c 的长度固定。</p><p>为了便于理解，我们类比为「压缩-解压」的过程：</p><p>将一张 800X800 像素的图片压缩成 100KB，看上去还比较清晰。再将一张 3000X3000 像素的图片也压缩到 100KB，看上去就模糊了。</p><p>Encoder-Decoder 就是类似的问题：当输入信息太长时，会丢失掉一些信息。</p><h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a><a href="https://blog.csdn.net/sinat_34072381/article/details/106728056">Attention Mechanism</a></h3><p>Attention 机制就是为了解决「信息过长，信息丢失」的问题。</p><p><em>Ａttention</em> 模型的特点是 Eecoder 不再将整个输入序列编码为固定长度的「中间向量 Ｃ」 ，而是编码成一个向量的序列。引入了 <em>Ａttention</em> 的 <em>Encoder-Decoder</em> 模型如下图：</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/attention.png" style="zoom:50%;"><p>这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。而且这种方法在翻译任务中取得了非常不错的成果。</p><p>Attention 是一个很重要的知识点，想要详细了解 Attention，请查看《<a href="https://easyai.tech/ai-definition/attention/">一文看懂 Attention（本质原理+3大优点+5大类型）</a>》</p><h4 id="Additive-Attention-Bahdanau-Attention"><a href="#Additive-Attention-Bahdanau-Attention" class="headerlink" title="Additive Attention (Bahdanau Attention)"></a>Additive Attention (Bahdanau Attention)</h4><p>传统seq2seq模型中encoder将输入序列编码成一个context向量，decoder将context向量作为初始隐状态，生成目标序列。随着输入序列长度的增加，<strong>编码器难以将所有输入信息编码为单一context向量</strong>，编码信息缺失，难以完成高质量的解码。</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/seq2seq.png" style="zoom: 67%;"><p>注意力机制是在每个时刻解码时，基于当前时刻解码器的隐状态、输入或输出等信息，<strong>计算其对输入序列各位置隐状态的注意力（分数）并加权生成context向量</strong>用于当前时刻解码。引入注意力机制，使得不同时刻的解码能够关注不同位置的输入信息，提高预测准确性。</p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/seq2seq2.png" style="zoom:67%;"><h4 id="Bahdanau-Attention-Mechanism"><a href="#Bahdanau-Attention-Mechanism" class="headerlink" title="Bahdanau Attention Mechanism"></a>Bahdanau Attention Mechanism</h4><p>Bahdanau本质是一种 <strong>加性attention机制</strong>，将decoder的隐状态和encoder所有位置输出通过线性组合对齐，得到context向量，用于改善序列到序列的翻译模型。</p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/bahdanau.png"></p><p>本质：两层全连接网络，隐藏层激活函数tanh，输出层维度为1。</p><p><strong>Bahdanau的特点为：</strong></p><p>1、编码器隐状态 ：编码器对于每一个输入向量产生一个隐状态向量；</p><p>2、计算对齐分数：使用上一时刻的隐状态$s_{t-1}$ 和编码器每个位置输出$s_t$计算对齐分数（使用前馈神经网络计算），编码器最终时刻隐状态可作为解码器初始时刻隐状态；</p><p>3、概率化对齐分数：解码器上一时刻隐状态$s_{t-1}$在编码器每个位置输出的对齐分数，通过softmax转化为概率分布向量；</p><p>4、计算上下文向量：根据概率分布化的对齐分数，加权编码器各位置输出，得上下文向量$c_t$;</p><p>5、解码器输出：将上下文向量$c_t$和上一时刻编码器输出$\hat y_{t-1} $对应的embedding拼接，作为当前时刻编码器输入，经RNN网络产生新的输出和隐状态，训练过程中有真实目标序列$y=(y_1···y_m)$，多使用$y_{t-1}$取代$\hat y_{t-1} $作为解码器t时刻输入；</p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/time_t.png"></p><p>使用Bahdanau注意力机制的解码过程：</p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/bahdanau_mechanism.png"></p><h4 id="Multiplicative-Attention-Luong-Attention"><a href="#Multiplicative-Attention-Luong-Attention" class="headerlink" title="Multiplicative Attention (Luong Attention)"></a>Multiplicative Attention (Luong Attention)</h4><p>Luong本质是一种 乘性attention机制，将解码器隐状态和编码器输出进行矩阵乘法，得到上下文向量。</p><p>Luong注意力机制是对Bahdanau注意力机制的改进，根据是否全部使用所有编码器输出分为两种：全局注意力和局部注意力，全局注意力适合用于短输入序列，局部注意力适合用于长输入序列（计算全局注意力代价高），以下内容仅介绍全局注意力。</p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/Luong_Attention.png"></p><p><strong>Luong注意力机制与Bahdanau注意力机制的主要不同点：</strong></p><p>1、对齐分数计算：使用当前时刻的隐状态$s_t$计算在编码器位置i输出的对齐分数；</p><p>2、解码器输出：解码器在t时刻，拼接上下文向量$c_t$ 和当前时刻隐状态$s_t$，经全连接层，产生当前时刻输出$\hat y_{t+1}$ ，并作为下一时刻输入；</p><p>3、流程图当前时刻计算的context向量，会被应用到下一时刻的输入；</p><p><strong>Luong注意力的三种计算方法：</strong></p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/Luong_Attention2.png"></p><p><img src="/2021/04/24/week3_%E5%9F%BA%E4%BA%8ESeq2Seq%E6%9E%B6%E6%9E%84%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/difference.png"></p><h4 id="Teacher-Forcing"><a href="#Teacher-Forcing" class="headerlink" title="Teacher Forcing"></a>Teacher Forcing</h4><p>Teacher Forcing是一种用来训练循环神经网络模型的方法，这种方法以上一时刻的输出作为下一时刻的输入。</p><p><a href="https://kexue.fm/archives/7818"><a href="https://kexue.fm/archives/7818">让Teacher Forcing更有“远见”一些</a></a></p><p><a href="https://wmathor.com/index.php/archives/1449/">Teacher Forcing</a></p><h3 id="Layer-amp-Model-in-Action"><a href="#Layer-amp-Model-in-Action" class="headerlink" title="Layer &amp; Model in Action"></a>Layer &amp; Model in Action</h3><p>Tensorflow 2 API (<a href="https://github.com/lyhue1991/eat_tensorflow2_in_30_days">https://github.com/lyhue1991/eat_tensorflow2_in_30_days</a>)</p><h3 id="Tensorflow的一些配置"><a href="#Tensorflow的一些配置" class="headerlink" title="Tensorflow的一些配置"></a>Tensorflow的一些配置</h3><h4 id="设置Tensorflow使用的显存大小"><a href="#设置Tensorflow使用的显存大小" class="headerlink" title="设置Tensorflow使用的显存大小"></a>设置Tensorflow使用的显存大小</h4><h5 id="获得当前主机上特定运算设备的列表"><a href="#获得当前主机上特定运算设备的列表" class="headerlink" title="获得当前主机上特定运算设备的列表"></a>获得当前主机上特定运算设备的列表</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpus = tf.config.experimental.list_physical_devices(device_type=<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line">cpus = tf.config.experimental.list_physical_devices(device_type=<span class="string">&#x27;CPU&#x27;</span>)</span><br><span class="line">print(gpus, cpus)</span><br></pre></td></tr></table></figure><h5 id="设置当前程序可见的设备范围"><a href="#设置当前程序可见的设备范围" class="headerlink" title="设置当前程序可见的设备范围"></a>设置当前程序可见的设备范围</h5><p>默认情况下 TensorFlow 会使用其所能够使用的所有 GPU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.config.experimental.set_visible_devices(devices=gpus[<span class="number">2</span>:<span class="number">4</span>], device_type=<span class="string">&#x27;GPU&#x27;</span>)</span><br></pre></td></tr></table></figure><p>设置之后，当前程序只会使用自己可见的设备，不可见的设备不会被当前程序使用。</p><p>另一种方式是使用环境变量 CUDA_VISIBLE_DEVICES 也可以控制程序所使用的 GPU。</p><p>在终端输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=2,3</span><br></pre></td></tr></table></figure><p>或者在代码里加入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &quot;2,3&quot;</span><br></pre></td></tr></table></figure><p>都可以达到同样的效果。</p><h5 id="显存的使用"><a href="#显存的使用" class="headerlink" title="显存的使用"></a>显存的使用</h5><p>默认情况下，TensorFlow 将使用几乎所有可用的显存，以避免内存碎片化所带来的性能损失。</p><p>但是TensorFlow 提供两种显存使用策略，让我们能够更灵活地控制程序的显存使用方式：</p><ol><li><p>仅在需要时申请显存空间【按需设置显存】（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）；</p></li><li><p>限制消耗固定大小的显存【定量设置显存】（程序不会超出限定的显存大小，若超出的报错）。</p></li></ol><p>设置仅在需要时申请显存空间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> gpu <span class="keyword">in</span> gpus:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpu, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>下面的方式是设置Tensorflow固定消耗GPU:0的2GB显存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">    gpus[<span class="number">0</span>],</span><br><span class="line">    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">2048</span>)]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h5 id="单GPU模拟多GPU环境"><a href="#单GPU模拟多GPU环境" class="headerlink" title="单GPU模拟多GPU环境"></a>单GPU模拟多GPU环境</h5><p>上面的方式不仅可以设置显存的使用，还可以在只有单GPU的环境模拟多GPU进行调试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.config.experimental.set_virtual_device_configuration(</span><br><span class="line">    gpus[<span class="number">0</span>],</span><br><span class="line">    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">2048</span>),</span><br><span class="line">     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">2048</span>)])</span><br></pre></td></tr></table></figure><p>上面的代码就在GPU:0上建立了两个显存均为 2GB 的虚拟 GPU。</p><h4 id="Tensorflow-错误"><a href="#Tensorflow-错误" class="headerlink" title="Tensorflow 错误"></a>Tensorflow 错误</h4><p>Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED</p><p>解决方案：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">gpus &#x3D; tf.config.experimental.list_physical_devices(device_type&#x3D;&#39;GPU&#39;)</span><br><span class="line">for gpu in gpus:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpu, True)</span><br><span class="line">os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;]&#x3D;&#39;2&#39;</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>1、关于seq2seq的整个流程，可以参考<a href="https://www.toutiao.com/i6648878247188627972/%E3%80%82">https://www.toutiao.com/i6648878247188627972/。</a></p><p>2、关于loss函数中的trick，对于Seq2Seq模型来说，输入和输出序列的class便是词汇表的大小，而对于训练集来说，输入和输出的词汇表的大小是比较大的。为了减少计算每个词的softmax的时候的资源压力，通常会减少词汇表的大小，但是便会带来另外一个问题，由于词汇表的词量的减少，语句的Embeding的id表示时容易大频率的出现未登录词‘UNK’。所以计算词汇表的softmax的时候，并不采用全部的词汇表中的词，而是进行一定手段的sampled的采样，从而近似的表示词汇表的loss输出，sampled采样需要定义好候选分布Q。即按照什么分布去采样。因此loss函数可以用sampled_softmax_loss。</p><p>3、在训练过程中通常采用的是teacher-forcing进行训练纠正，但是在预测阶段，是不知道真实标签的，所以会引起Exposure Bias， 使用Beam Search的Encoder的方式也能一定程度上降低Exposure Bias问题。</p><p>4、关于oov问题和低频词。OOV表示的是词汇表外的未登录词，低频词则是词汇表中的出现次数较低的词。在Decoder阶段时预测的词来自于词汇表，这就造成了未登录词难以生成，低频词也比较小的概率被预测生成。PGN网络可以解决（之后的课程会有）。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/heiheiya/article/details/102776353">Tensorflow 2.0 GPU的使用与分配</a></p><p><a href="https://blog.csdn.net/HHTNAN/article/details/78032712">TextRank算法原理与提取关键词、自动提取摘要PYTHON</a></p><p><a href="https://github.com/STHSF/TextRank">关于自动文摘</a></p><p><a href="https://www.hankcs.com/nlp/textrank-algorithm-java-implementation-of-automatic-abstract.html">TextRank算法自动摘要的Java实现</a></p><p><a href="https://www.jiqizhixin.com/articles/2018-12-28-18">基于TextRank算法的文本摘要（附Python代码）</a></p><p><a href="https://www.cnblogs.com/en-heng/p/6626210.html">关键词提取算法TextRank</a></p><p><a href="https://zhuanlan.zhihu.com/p/41091116">通俗易懂理解——TF-IDF与TextRank</a></p><p><a href="https://blog.csdn.net/sinat_34072381/article/details/106728056">Attention机制（Bahdanau attention &amp; Luong Attention）</a></p><p><a href="https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a">Attention: Sequence 2 Sequence model with Attention Mechanism</a></p><p><a href="https://arxiv.org/pdf/1409.0473.pdf">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a></p><p><a href="https://arxiv.org/pdf/1508.04025.pdf">Effective Approaches to Attention-based Neural Machine Translation</a></p><p><a href="https://blog.floydhub.com/attention-mechanism/">Attention Mechanism</a></p><p><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">Attention? Attention!</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 摘要自动生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 摘要自动生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week2_项目导论中与中文词向量实践</title>
      <link href="2021/04/15/week2_%E9%A1%B9%E7%9B%AE%E5%AF%BC%E8%AE%BA%E4%B8%AD%E4%B8%8E%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%E5%AE%9E%E8%B7%B5/"/>
      <url>2021/04/15/week2_%E9%A1%B9%E7%9B%AE%E5%AF%BC%E8%AE%BA%E4%B8%AD%E4%B8%8E%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Preview（Text-Summarization）"><a href="#一、Preview（Text-Summarization）" class="headerlink" title="一、Preview（Text Summarization）"></a>一、Preview（Text Summarization）</h2><h3 id="1、What’s-a-good-algorithm-engineer"><a href="#1、What’s-a-good-algorithm-engineer" class="headerlink" title="1、What’s a good algorithm engineer?"></a>1、What’s a good algorithm engineer?</h3><p>算法基础</p><p>代码功底</p><p>业务理解和应用能力</p><p>良好的产品sense和业务owner意识</p><h3 id="2、Level-of-Machine-Learning-Engineer"><a href="#2、Level-of-Machine-Learning-Engineer" class="headerlink" title="2、Level of Machine Learning Engineer"></a>2、Level of Machine Learning Engineer</h3><h4 id="第一阶段：高效执行机器"><a href="#第一阶段：高效执行机器" class="headerlink" title="第一阶段：高效执行机器"></a>第一阶段：高效执行机器</h4><p>这一阶段我认为算法工程师的核心竞争力是对模型的理解，对于模型不仅知其然，还得知其所以然。</p><h4 id="第二阶段：算法选型和改造能力"><a href="#第二阶段：算法选型和改造能力" class="headerlink" title="第二阶段：算法选型和改造能力"></a>第二阶段：算法选型和改造能力</h4><p>这一阶段我认为算法工程师的核心竞争力在于代码功底好，一则知道各个模型的实现细节，二则能即快又好地实现idea。</p><h4 id="第三阶段：业务抽象能力"><a href="#第三阶段：业务抽象能力" class="headerlink" title="第三阶段：业务抽象能力"></a>第三阶段：业务抽象能力</h4><p>他们像大夫，望闻问切，跟客户一起梳理出业务流程中的痛点，找到优化方式。不只是对行业整体的判断，还要对赛道<br>中的选手体检，有开药的能力。可以把对方的难言之隐梳理出来，定量、优先级排序，然后从整体到细节，一层层结构<br>化分解，最后进入具体执行。你要在传统行业创造新价值，就要搞清楚：什么东西制约了你的产能，制约了你的效率，<br>制约了你的利润率。</p><h2 id="二、Language-to-word-embedding"><a href="#二、Language-to-word-embedding" class="headerlink" title="二、Language to word embedding"></a>二、Language to word embedding</h2><h3 id="1、what-is-Out-of-vocabulary-OOV"><a href="#1、what-is-Out-of-vocabulary-OOV" class="headerlink" title="1、what is Out-of-vocabulary(OOV)"></a>1、what is Out-of-vocabulary(OOV)</h3><p>未登录词就是训练时未出现，测试时出现了的单词。在自然语言处理或者文本处理的时候，我们通常会有一个字词库（vocabulary）。这个vocabulary要么是提前加载的，或者是自己定义的，或者是从当前数据集提取的。假设之后你有了另一个的数据集，这个数据集中有一些词并不在你现有的vocabulary里，我们就说这些词汇是Out-of-vocabulary，简称OOV。</p><h3 id="2、Word-Embedding"><a href="#2、Word-Embedding" class="headerlink" title="2、Word Embedding"></a>2、Word Embedding</h3><p>简单的讲vector就是一个一维的数组，每一个词都变成一个vector。比如说先把一个词变到一个多维空间中，然后把所有的词都放在这个多维空间中。最大的好处是，这些词对计算机来说是categorical feature，像one-hot一样，两个词放在不同位置完全没有关系。如果用vector来表示，词与词之间的关系就可以用距离来表现。也就是说这些词对计算机来说本来是没有关系的，但通过vector转换之后，它们的距离代表了它们的关系，这也是比较好的帮助计算机去理解词之间关系的方法。</p><p>Word Embedding 实际上就是把词从词本身或从one-hot本身变成一个vector 的过程，Embedding就是你怎么去变换这个向量。</p><h3 id="3、How-do-we-represent-the-meaning-of-a-word"><a href="#3、How-do-we-represent-the-meaning-of-a-word" class="headerlink" title="3、How do we represent the meaning of a word?"></a>3、How do we represent the meaning of a word?</h3><p>Meaning本身在字典的定义是：词背后的想法，或是某个人、文章、艺术品想要表达的想法。Meaning本身是个idea，它在大脑里面怎么存储的我们不知道，这个idea怎么让计算机系统去理解它，比较好的办法是把它变成一个vector。</p><p>词本身如果不做向量的变化，那计算机看起来是什么？如果两个词不一样，那就是一个分类的feature, 那我们就直接做one-hot，就是在出现的位置记为1，其他位置记为0，这样做显然是可以的，但是维度是十分大的，尤其是英文。比如说，你搜索电视大小其实和电视容量是一个意思，那计算机怎么知道电视大小和电视容量是同一个意思？包括你要查hotel和motel，其实是一个意思，如果用one-hot它们将在两个维度上，完全没有关系。如果把它变到一个比one-hot低维的，但每个位置上都是有浮点数的vector，而且这些浮点数的数值是有意义的，比如说两个词的浮点数值大小非常靠近，那这两个词就比较靠近，那这样学出来的vector 也是非常有意义的。</p><h3 id="4、Problems-with-WordNet"><a href="#4、Problems-with-WordNet" class="headerlink" title="4、Problems with WordNet"></a>4、Problems with WordNet</h3><p>Great but missing nuance(细微差别)<br>    • e.g., “proficient” is listed as a synonym for “good” This is only<br>correct in some contexts<br>    • Missing new meanings of words<br>    • e.g., wicked, badass, nifty, wizard, genius, ninja, bombast<br>    • Impossible to keep up-to-date!<br>Subjective<br>Requires human labor to create and adapt<br>Can’t compute accurate word similarity</p><h3 id="5、Representing-words-as-discrete-symbols"><a href="#5、Representing-words-as-discrete-symbols" class="headerlink" title="5、Representing words as discrete symbols"></a>5、Representing words as discrete symbols</h3><p>In traditional NLP, we regard words as discrete symbols:<br>hotel, conference, motel – a localist representation<br>Such symbols for words can be represented by one-hot vectors:<br>motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]<br>hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]<br>Vector dimension = number of words in vocabulary (e.g., 5000,000)</p><h3 id="6、Problem-with-words-as-discrete-symbols"><a href="#6、Problem-with-words-as-discrete-symbols" class="headerlink" title="6、Problem with words as discrete symbols"></a>6、Problem with words as discrete symbols</h3><p>Example: in web search, if user searches for “Seattle motel”,<br>we would like to match documents containing “Seattle hotel”<br>But:<br>motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]<br>hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]<br>These two vectors are orthogonal<br>There is no natural notion of similarity for one-hot vectors!<br>Solution:<br>Could try to rely on WordNet’s list of synonyms to get similarity?<br>• But it is well-known to fail badly: incompleteness, etc<br>• Instead: learn to encode similarity in the vectors themselves</p><h3 id="7、Question-of-N-gram"><a href="#7、Question-of-N-gram" class="headerlink" title="7、Question of N-gram"></a>7、Question of N-gram</h3><p>数据稀疏，难免会出现OOV的问题<br>• 随着 n 的增⼤，参数空间呈指数增⻓（维度灾难）<br>• 缺少⻓期依赖，只能建模到前 n-1 个词<br>• ⽆法表示⼀词多义（语义鸿沟）</p><h3 id="8、Word-Vector"><a href="#8、Word-Vector" class="headerlink" title="8、Word Vector"></a>8、Word Vector</h3><p>分布表示（distributional representation）与分布式表示（distributed representation）</p><h4 id="分布表示（distributional-representation）"><a href="#分布表示（distributional-representation）" class="headerlink" title="分布表示（distributional representation）"></a>分布表示（distributional representation）</h4><p>分布表示（distributional representation）：是基于分布假设理论，利用共生矩阵来获取词的语义表示，可以看成是一类获取词表示的方法。</p><p>什么是分布假说呢？词是承载语义的最基本的单元，而传统的独热表示（one-hot represen-tation）仅仅将词符号化，不包含任何语义信息。如何将语义融入到词表示中？Harris 在 1954 年提出的分布假说（distributional hypothesis）为这一设想提供了理论基础：上下文相似的词，其语义也相似。</p><p>“这里的分布”与中文“统计分布”一词语义对应，描述的是上下文的概率分布。用上下文描述语义的表示方法（或基于分布假说的方法）都可以称作分布表示，如潜在语义分析模型（Latent Semantic Analysis, LSA）、潜在狄利克雷分配模型（Latent Dirichlet Allocation，LDA）等。</p><h4 id="分布式表示（distributed-representation）"><a href="#分布式表示（distributed-representation）" class="headerlink" title="分布式表示（distributed representation）"></a>分布式表示（distributed representation）</h4><p>分布式表示（distributed representation），描述的是把文本分散嵌入到另一个空间，一般从是从高维空间嵌入到低维空间。</p><p>“嵌入”是几个意思？感觉跟塞入、挤入差不多呀。</p><p>还真是这样。如词的独热表示（one-hot representation），首先是高维的，且在高维向量中只有一个维度描述了词的语义。多高？词典有多大就有多少维，怎么也得万把维度吧。</p><p>如何在低维空间表达一个词呢？目前流行的是通过矩阵降维或神经网络降维将语义分散存储到向量的各个维度中，这两类方法得到的向量空间是低维的一般都可以称作分布式表示，又称为词嵌入（word embedding）或词向量）。</p><p>看吧，这就把词的表示从高维（5000-20000）嵌入到低维（50-300）。what？300维也叫低维？！是的，你没学过相对论? ^_^</p><p>这里的分布式（distributed）是“分散”、“分配”的意思，与中文“分布式计算”一词语义对应，与之相对的是局部表示（local representation）。</p><p>词嵌入vs词向量<br>还是叫词嵌入好点。词向量容易绕人。从广义上讲，传统的词袋子模型也是用向量描述文本，也应当被称作词的向量表示，但是这种向量是高维稀疏的。在目前的NLP语境中，“词向量”特指由神经网络模型得到的低维实数向量表示。</p><h3 id="9、Neural-Probabilistic-Language-Model"><a href="#9、Neural-Probabilistic-Language-Model" class="headerlink" title="9、Neural Probabilistic Language Model"></a>9、Neural Probabilistic Language Model</h3><h3 id="10、Problems-with-Neural-Probabilistic-Language-Model"><a href="#10、Problems-with-Neural-Probabilistic-Language-Model" class="headerlink" title="10、Problems with Neural Probabilistic Language Model"></a>10、Problems with Neural Probabilistic Language Model</h3><p>• 有限的前文信息<br>• 计算量过大<br>• 词向量是副产品</p><h2 id="三、Word2Vec"><a href="#三、Word2Vec" class="headerlink" title="三、Word2Vec"></a>三、Word2Vec</h2><h3 id="1、Word2Vec-2013"><a href="#1、Word2Vec-2013" class="headerlink" title="1、Word2Vec(2013)"></a>1、Word2Vec(2013)</h3><p><img src="/2021/04/15/week2_%E9%A1%B9%E7%9B%AE%E5%AF%BC%E8%AE%BA%E4%B8%AD%E4%B8%8E%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%E5%AE%9E%E8%B7%B5/word2vec.png"></p><h3 id="2、Word2vec-objective-function"><a href="#2、Word2vec-objective-function" class="headerlink" title="2、Word2vec: objective function"></a>2、Word2vec: objective function</h3><p><img src="/2021/04/15/week2_%E9%A1%B9%E7%9B%AE%E5%AF%BC%E8%AE%BA%E4%B8%AD%E4%B8%8E%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%E5%AE%9E%E8%B7%B5/word2vec_objectve_function.png"></p><h3 id="3、Word2vec-prediction-function"><a href="#3、Word2vec-prediction-function" class="headerlink" title="3、Word2vec: prediction function"></a>3、Word2vec: prediction function</h3><p><img src="/2021/04/15/week2_%E9%A1%B9%E7%9B%AE%E5%AF%BC%E8%AE%BA%E4%B8%AD%E4%B8%8E%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%E5%AE%9E%E8%B7%B5/word2vec_predict_function.png"></p><h3 id="4、CBOW-amp-skip-gram"><a href="#4、CBOW-amp-skip-gram" class="headerlink" title="4、CBOW&amp;skip-gram"></a>4、CBOW&amp;skip-gram</h3><p><img src="/2021/04/15/week2_%E9%A1%B9%E7%9B%AE%E5%AF%BC%E8%AE%BA%E4%B8%AD%E4%B8%8E%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%E5%AE%9E%E8%B7%B5/CBOW.png"></p><p><img src="/2021/04/15/week2_%E9%A1%B9%E7%9B%AE%E5%AF%BC%E8%AE%BA%E4%B8%AD%E4%B8%8E%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%E5%AE%9E%E8%B7%B5/skip_gram.png"></p><p>下面举一个Skip-Gram Model的例子，它的主要思想是：如果你能够拿到一些文本，可能是维基百科、百度百科的文章，很自然的有一些词就会出现在另一些词的附近，那我们在做Skip-Gram的过程实际就是在create　一个train data的过程，我们把文本拿来，把中间词作为ｘ，两边的词作为label或是topic words，这两个词如果同时出现在附近，可以记为１，如果没有记为０。</p><p>这样的就可以得到一个train sample，这样的train sample都是一个pairs，这样就可以把文本变成很多个train sample，再返回刚才的模型，能很好的把Hidden Layer学出来。学到Hidden Layer之后这就一个embedding了，通过word Paris建立语言模型，然后每一个词再回来，本身还是一个one-hot encoding，再经过Hidden Layer weight matrix，会变成一些的word vector。</p><p>回溯总结一下，vector就是把词本身变成一个向量，怎么得到这个向量？刚才举到了用神经网络，Skip-Gram 建立train 数据，然后学到这个数据，然后Embedding实际上就是Hidden Layer weight matrix，通过Embedding就得到了向量。这是一个比较直接的，事实证明也是一个比较有效的办法。</p><h3 id="5、Difference-between-Skip-Gram-and-CBOW"><a href="#5、Difference-between-Skip-Gram-and-CBOW" class="headerlink" title="5、Difference between Skip-Gram and CBOW"></a>5、Difference between Skip-Gram and CBOW</h3><ul><li>Intuitively, the first task is much simpler, this implies a much faster convergence for CBOW than for Skip-gram, in the original paper (link below) they wrote that CBOW took hours to train, Skip-gram 3 days.</li><li>For the same logic regarding the task difficulty, CBOW learn better syntactic relationships between words while Skipgram<br>is better in capturing better semantic relationships.</li><li>A final consideration to make deals instead with the sensitivity to rare and frequent words.</li></ul><h2 id="四、Hierarchical-Softmax（H-Softmax）"><a href="#四、Hierarchical-Softmax（H-Softmax）" class="headerlink" title="四、Hierarchical Softmax（H-Softmax）"></a>四、Hierarchical Softmax（H-Softmax）</h2><h3 id="1、Negative-Sampling"><a href="#1、Negative-Sampling" class="headerlink" title="1、Negative Sampling"></a>1、Negative Sampling</h3><p>刚刚有讲到每个词用one-hot encoding，然后用weight matrix与它相乘，假设我们想要得到的vector的size是300，输入字典的维度是10000，可以看到weight matrix 有300*10000个parameter，只有一个FC Layer就非常大了，所以不仅是weight matrix 这么大，每一次迭代都要把matrix更新一遍，这样整个学习过程的效率是十分低的。</p><p>比如说在10000个词中有很多和它是词义相近的，但绝大部分和它是没有关系的，数学的角度是正交的，所以它不需要每次都进行更新。所以Negative Sampling的核心：大量减少更新的内容，而且可以大量的减少训练损失，实际测量下来的结果也是非常好的。当然还要做一个词频，高频的词需要放到训练的过程当中去，低频的就不需要做了。</p><p>总结一下Word2Vec 是怎样一个流程：</p><p>Collect text data</p><p>Process text</p><p>Skip-Gram to generate word pair</p><p>Training embedding</p><p>Word2Vec</p><h3 id="2、Vector-Space-Visualization"><a href="#2、Vector-Space-Visualization" class="headerlink" title="2、Vector Space Visualization"></a>2、Vector Space Visualization</h3><p>把它们全都变成vector之后下一步需要做什么？下一步最简单的做法就是把它们画出来。</p><p>当然之前例子中说到把每一个词变到300维，300维是人的肉眼是看不出来的，大家的物理世界只有３维，需要一些降维的方法，降维之后可以看到本来一些词是没有关系的，最后自动的group到一起。</p><p>比如说左上角12345678910的英语都到一起了，左下角和时间相关的都在一起了，右下角语文数学化学高考科目都到一起了，这是个比较有意思的事情，明显的看到这个学习的过程是比较有意义的，意思相近的词都在一起了。</p><p>还有一个点，word vector不仅仅是把词进行分类，而且词和词之间的距离也是有很强的关系。</p><p>比如说英语里面有基本级、比较级、最高级，如果看成一个向量，一个的比较级减去最高级和另外一个比较级减去另外一个最高级，下面的向量还是一样的。它不仅把词之间的分类学会了，词之间的关系也学会了。还有就是男人对应国王，女人对应什么？是皇后。这个也能学出来。所以Word2Vec的fundamental idea不是很难，但效果也是非常好的。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 摘要自动生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 摘要自动生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全连接神经网络</title>
      <link href="2021/04/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>2021/04/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="全连接神经网络简介"><a href="#全连接神经网络简介" class="headerlink" title="全连接神经网络简介"></a>全连接神经网络简介</h2><p>对于全连接神经网络，相信很多读者一听到“网络”二字，头皮就开始发麻，笔者一开始学的时候也一样，觉得网络密密麻麻地，绝对很难，其实不然，这里的网络比我们现实生活中的网络简化了不止一丁点儿，但是它却能出奇地完成各种各样的任务，逐渐成为我们人类智能生活的璀璨明珠。当然，虽然全连接神经网络并不是最耀眼的一颗，但却是每一个初学的读者必须去了解的一颗，在这里，笔者认为全连接神经网络是每位读者深度学习之旅的开端。</p><h2 id="全连接神经网络原理"><a href="#全连接神经网络原理" class="headerlink" title="全连接神经网络原理"></a>全连接神经网络原理</h2><p>光看名字，可能大家并不了解这个网络是干啥的，那么笔者先给大家附上一张图，如下图所示。它作为神经网络家族中最简单的一种网络，相信大家看完它的结构之后一定会对它有个非常直观的了解。</p><p><img src="/2021/04/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg"></p><p>对，就是这么一个东西，左边输入，中间计算，右边输出。可能这样还不够简单，笔者给大家画一个更简单的运算示意图，如下图所示。</p><p><img src="/2021/04/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%90%E7%AE%97%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg"></p><p>不算输入层，上面的网络结构总共有两层，隐藏层和输出层，它们“圆圈”里的计算都是公式(1)和(2)的计算组合：<br>$$<br>a = wx +b<br>$$</p><p>$$<br>f(z) = \frac {1} {e^{-z}}<br>$$</p><p>每一级都是利用前一级的输出做输入，再经过圆圈内的组合计算，输出到下一级。</p><p>看到这里，可能很多人会疑惑，为什么要加上f(z)这个运算，这个运算的目的是为了将输出的值域压缩到（0,1），也就是所谓的归一化，因为每一级输出的值都将作为下一级的输入，只有将输入归一化了，才会避免某个输入无穷大，导致其他输入无效，变成“一家之言”，最终网络训练效果非常不好。</p><p>此时，有些记忆力比较好的读者可能会想，反向传播网络？反向去哪了？对的，这个图还没画完整，整个网络结果结构应该是这样，如下图所示。</p><p><img src="/2021/04/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg"></p><p>那有些读者又会提出新的问题了，那反向传播的东西到底是什么呢？目的又是什么呢？这里，所有读者都要有这么一点认识，神经网络的训练是有监督的学习，也就是输入X 有着与之对应的真实值Y ，神经网络的输出Y 与真实值Y 之间的损失Loss 就是网络反向传播的东西。整个网络的训练过程就是不断缩小损失Loss 的过程。为此，就像高中一样，我们为了求解某个问题，列出了一个方程，如公式(3)~(5)：<br>$$<br>Loss = \sum_{i=1}^{n} (y_i - (wx_i + b))^2<br>$$</p><p>$$<br>Loss=\sum_{i=1}^n(x_i^2w^2 + b^2 + 2x_iwb+ 2y_ib - 2x_iy_iw + y_i ^2)<br>$$</p><p>$$<br>Loss = \sum_{i=1}^n{Aw^2 + Bb^2 +Cwb - Db-Ew+F}<br>$$</p><p>上述的公式经过化简，我们可以看到A、B、C、D、E、F都是常系数，未知数就是w 和b ，也就是为了让Loss 最小，我们要求解出最佳的w 和b 。这时我们稍微想象一下，如果这是个二维空间，那么我们相当于要找一条曲线，让它与坐标轴上所有样本点距离最小。比如这样，如下图所示。</p><p><img src="/2021/04/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88.jpg"></p><p>同理，我们可以将Loss方程转化为一个三维图像求最优解的过程。三维图像就像一个“碗”，如下图所示，它和二维空间的抛物线一样，存在极值，那我们只要将极值求出，那就保证了我们能求出最优的（w , b）也就是这个“碗底”的坐标，使Loss 最小。</p><p><img src="/2021/04/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%89%E7%BB%B4%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg"></p><p>那说了这么多，我们应该如何求解呢？</p><p>读者们是否还记得上高中的时候，当我们列完函数方程之后，做的第一件事就是对这个函数求导，是的，这里也一样，要求极值，首先求导。不过，我们高中没有接触过二元凸函数的求导，但是相信翻阅此书的读者应该都是大学生，这时候要拿出高等数学这本书来了，偏导数在这里隆重登了场。偏导数简单来讲，也就是对X，Y分别求导，在求导过程中，把其他的未知量当成常数即可。</p><p>好了，理论知识补充完了，这时候我们想象自己在一座山上，要想从山上最快地去到谷底，那就要沿着最陡峭的地方往下走。这个最陡峭的地方，我们叫做梯度，像不像我们对上面那个“碗”做切线，找出最陡的那条切线，事实上我们做的就是这个，求偏导就是这么一个过程。<br>$$<br>梯度 \nabla =\begin{pmatrix} \frac {\partial f(x, y)} {\partial x}, \frac {\partial f(x, y)} {\partial y}\end{pmatrix}<br>$$</p><p>我们每走一步，坐标就会更新：<br>$$<br>x_{n+1} = x_n - \alpha *  \frac {\partial f(x, y)} {\partial {x}}<br>$$</p><p>$$<br>y_{n+1} = y_n - \alpha *  \frac {\partial f(x, y)} {\partial {y}}<br>$$</p><p>当然，这是三维空间中的，假如我们在多维空间漫步呢，其实也是一样的，也就是对各个维度求偏导，更新自己的坐标。<br>$$<br>\nabla = \begin{pmatrix}  \frac {\partial f(x, y)} {\partial w_1}, \frac {\partial f(x, y)} {\partial w_2}, \frac {\partial f(x, y)} {\partial w_3}, ···， \frac {\partial f(x, y)} {\partial w_n}   \end{pmatrix}<br>$$</p><p>$$<br>w_{i+1} = w - \alpha * \frac {\partial f(x, y)} {\partial w^i}<br>$$</p><p>其中，w的上标i表示第几个w，下标n表示第几步，α是学习率，后面会介绍α的作用。所以，我们可以将整个求解过程看做下山（求偏导过程），为此，我们先初始化自己的初始位置。<br>$$<br>(w_0, y_0)<br>$$</p><p>$$<br>w_1 = w_0 - \alpha * \frac {\partial Loss(w,b)} {\partial w}<br>$$</p><p>$$<br>b_1 = b_0 - \alpha * \frac {\partial Loss(w,b)} {\partial b}<br>$$</p><p>这样我们不断地往下走（迭代），当我们逐渐接近山底的时候，每次更新的步伐也就越来越小，损失值也就越来越小，直到达到某个阈值或迭代次数时，停止训练，这样找到 就是我们要求的解。</p><p>我们将整个求解过程称为<strong>梯度下降求解法</strong>。</p><p>这里还需要补充的是为什么要有<strong>学习率α</strong>，以及如何选择学习率α？</p><p>通常来说，学习率是可以随意设置，你可以根据过去的经验或书本资料选择一个最佳值，或凭直觉估计一个合适值，一般在（0，1）之间。这样做可行，但并非永远可行。事实上选择学习率是一件比较困难的事，图 4.7显示了应用不同学习率后出现的各类情况，其中epoch为使用训练集全部样本训练一次的单位，loss表示损失。</p><p><img src="/2021/04/12/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%8F%98%E5%8C%96%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg"></p><p>可以发现，学习率直接影响我们的模型能够以多快的速度收敛到局部最小值（也就是达到最好的精度）。一般来说，学习率越大，神经网络学习速度越快。如果学习率太小，网络很可能会陷入局部最优；但是如果太大，超过了极值，损失就会停止下降，在某一位置反复震荡。</p><p>也就是说，如果我们选择了一个合适的学习率，我们不仅可以在更短的时间内训练好模型，还可以节省各种运算资源的花费。</p><p>如何选择？业界并没有特别硬性的定论，总的来说就是试出来的，看哪个学习率能让Loss收敛得更快，Loss最小，就选哪个。</p><h2 id="全连接神经网络小结"><a href="#全连接神经网络小结" class="headerlink" title="全连接神经网络小结"></a><strong>全连接神经网络小结</strong></h2><p>可能很多读者在看到第4.1节内容的时候会认为，既然深度学习已经将整个梯度下降的求解过程都封装好了，笔者为什么还要花这么大的篇幅来讲解呢？</p><p>因为我们后续接触的CNN，RNN等神经网络的原理和训练过程都是差不多的，无非就是网络结构改变罢了，在这里把最基本的原理掌握了，后面就算碰到再复杂的网络结构也不会慌张。</p><p>另外，当大家专研理论至深处且需要设计一个新的网络结构时，那时我们对原理掌握的熟练程度直接决定着所设计网络结构的优劣。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/104576756">深度学习开端｜全连接神经网络</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week8-Conversational-AI-in-Industry-and-Interview</title>
      <link href="2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
      <url>2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Conversational-AI-in-Industry"><a href="#Conversational-AI-in-Industry" class="headerlink" title="Conversational AI in Industry"></a>Conversational AI in Industry</h2><h3 id="智变中的美团客服"><a href="#智变中的美团客服" class="headerlink" title="智变中的美团客服"></a>智变中的美团客服</h3><h4 id="客服系统的演变历史"><a href="#客服系统的演变历史" class="headerlink" title="客服系统的演变历史"></a>客服系统的演变历史</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%B1%BB%E5%88%AB.png"></p><p>第一阶段是语音呼叫中心，这种客服系统纯靠人工服务，且支持语音电话，效率低成本高。</p><p>第二阶段进化到了网页在线客服，这种客服系统基于网页会话，服务形式支持文本和语音，同时还利于对流量数据进行挖掘。</p><p>第三阶段是SaaS客服系统，这种客服系统支持多渠道接入，有了丰富的辅助功能和知识库管理。</p><p>如今客服系统进化到了智能客服，它最大的特点就是人机协同，许多简单问题都可以由机器自主解决，这个系统可以自主学习不断进化。回顾客服系统的演变历史可以发现，智能化是客服系统的一个演变趋势。</p><h4 id="对话系统的分类"><a href="#对话系统的分类" class="headerlink" title="对话系统的分类"></a>对话系统的分类</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%B1%BB%E5%88%AB.png"></p><p>对话系统主要包括四类：问答型对话、任务型对话、闲聊型对话、图谱型对话。在问答型对话中，我们使用QABot机器人完成简单任务，这种对话通常是与上下文无关的单轮对话。在任务型对话中，TaskBot机器人完成特定场景下的复杂任务。另外还有ChatBot闲聊型机器人，这种对话通常不以解决实际问题为目的，我们的客服系统也有用到这种机器人。最后是图谱型机器人KBQA，这种机器人可能更多地用在金融、医疗等领域。</p><h4 id="智能客服机器人的智能水平"><a href="#智能客服机器人的智能水平" class="headerlink" title="智能客服机器人的智能水平"></a>智能客服机器人的智能水平</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%9C%BA%E5%99%A8%E4%BA%BA.png"></p><p>分为四个档次：简单检索机器人、语义识别机器人、场景导向机器人、智慧机器人。简单检索机器人只支持特定类型的检索，只要说法稍微一变可能就不能正确识别，匹配性较差。语义识别机器人基于知识库，可以更智能地理解所检索的问题。场景导向机器人根据不同场景量身定制机器人，机器人的聪明程度与场景有关。智慧机器人是智能程度最高的机器人，甚至可以达到拟人的程度。现在来看，大多数机器人还只是停留在第二个阶段，能达到第三个阶段的还是少数。</p><h4 id="智变之路"><a href="#智变之路" class="headerlink" title="智变之路"></a>智变之路</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/zhibianzhilu.png"></p><p>原来的客服系统系统中客户将请求传送到客服服务器，然后客服操作平台就会分配相应的人工客服处理相应的客户请求，客服操作平台只具备简单的知识管理功能。这种客服系统最大的问题就是效率低，需要的人力成本高。对于这样的客服系统来说，实际上需要的人工客服数目和订单数目是成正比的。美团现在的业务正处于飞速的发展过程中，现在就有近万名客服人员，如果不对客服系统进行改进，可以想象未来这个队伍还会扩充很多倍。基于原来客服系统的这些缺点，我们对这个架构进行了改进，增添了会话管理服务，后面连接着QABot、TakBot、ChatBot等机器人以及人工服务，有一个专门的知识管理平台来支撑QABot、TakBot、ChatBot，AI训练师对知识管理平台设计离线学习和自动训练的算法。除此之外我们还设计了话术提醒、舆情监控、转接提示等模块来辅助人工客服。</p><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E6%99%BA%E5%8F%98%E5%9F%BA%E7%A1%80.png"></p><p>要做好一个AI系统必须满足这样的条件：要有大量业务专家、要有强大算法团队、要有大规模GPU计算引擎、要有海量场景数据，这些条件在美团都是满足的，这无疑给了我们使智能客服系统落地的信心。</p><h4 id="语义识别流程"><a href="#语义识别流程" class="headerlink" title="语义识别流程"></a>语义识别流程</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E8%AF%AD%E4%B9%89%E8%AF%86%E5%88%AB%E6%B5%81%E7%A8%8B.png"></p><p>语义识别的流程模拟了人解决问题的思路。人类在解决一个问题的时候会首先考虑以前有没有类似的问题，对应于语义识别中我们也首先采用搜索检索的方法。在找寻相似问题的过程中，人类通常会考虑：“我们要找的到底是哪些问题呢？”，对应于语义识别中，这是一个对问题进行候选筛选的过程。当人类发现一些问题明显和意图无关，通常要把他们去掉，对应于语义识别中，这就是要进行意图识别从而筛选问题。面临最后挑选出的几个问题，人类通常要对其进行优先级排序，对应于语义识别中，这就是Rank的过程。最后人类可能还会对结果进行检查：“问的确实是这个问题嘛？”，这就是语义识别中的语义完整性检查。这整个过程中运用了各种各样的模型来达成语义识别的目的。</p><h4 id="基于深度模型的识别–DSSM"><a href="#基于深度模型的识别–DSSM" class="headerlink" title="基于深度模型的识别–DSSM"></a>基于深度模型的识别–DSSM</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/DSSM.png"></p><p>DSSM模型是一个双塔模型，它在句子embedding上效果很好，因此我们也借鉴了这一模型。我们把标准问与拓展问语义相同的句子对作为正例，把标准问与拓展问语义不同的句子对作为反例，训练了DSSM模型。模型训练后，对于任给的一个问题，可以得到它的embedding结果，和其他标准问embedding结果相对比就可以算出相似度。比如拓展问“我的外卖怎么还没到”可以计算出一个Sentence Vector，而标准问“配送超时催单”和“餐品有质量问题”分别有一个Sentence Vector，通过计算可得，“我的外卖怎么还没到”和“配送超时催单”的相似度为0.98，我的外卖怎么还没到”和“餐品有质量问题”的相似度是0.62，所以可以把“我的外卖怎么还没到”的语义识别为“配送超时催单”</p><h4 id="基于深度学习的识别–Seq2Seq"><a href="#基于深度学习的识别–Seq2Seq" class="headerlink" title="基于深度学习的识别–Seq2Seq"></a>基于深度学习的识别–Seq2Seq</h4><p> <img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/Seq2Seq.png"></p><p>Seq2seq模型本身是一个生成模型，但是我们把它用来计算句子之间的相似度，我们把它encoder和attention的结果和不同的候选做loss计算，把loss作为一种度量结果，Loss越小代表输入和候选越接近。以上两个深度模型是我们尝试过的深度模型中效果最好的两个</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>智能化是客服系统发展的趋势，是解决有限的客服资源与不断增长的海量用户服务请求矛盾的唯一选择。实践证明，智能化客服确实可以大量消除人的重复劳动，业务专家也可以从繁杂中解脱出来，可以有更多的时间进行方案优化。最重要的一点，智能化客服系统不是一个纯人的系统，也不是一个纯算法的系统，也不是一个静态的系统，它需要人机协同，自主学习不断进化。</p><h3 id="对话机器人在瓜子的实践"><a href="#对话机器人在瓜子的实践" class="headerlink" title="对话机器人在瓜子的实践"></a>对话机器人在瓜子的实践</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E5%8C%97%E4%BA%AC.png"></p><p>目前对话机器人很火，是有多方面原因的：第一，图灵在定义智能时就将对话机器人作为人工智能的一个标志；第二，深度学习技术越来越成熟，对话机器人在工业界已经达到一定水平；第三，对话机器人由于有智能客服的积累，有很多公司在做这方面的东西。上面是一个智能客服设计图，左边是接入渠道，登录进来，会提供一些客服产品，如机器人客服、人工在线客服、云呼叫中心，以及用户依据产品做一些自助服务。聊天过程中用户会将其数据留下来（反馈数据、对话数据、人工客服数据），利用这些数据就可以做分析，如客服数据可以做质检，用户数据可以做营销工作，与CRM接入打通。</p><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E8%83%8C%E6%99%AF2.png"></p><p>数字化就是将用户和企业交互的数据都记录下来，将数据结构化，做成算法可用的数据叫数据化，有了数据化就可以用建模等一系列智能化手段做一些智能化提升。在线化做后可以做到整个沟通可追踪、提供可优化、差异化的服务以及精细化运营，最终推动企业在线化。</p><h4 id="什么是对话机器人"><a href="#什么是对话机器人" class="headerlink" title="什么是对话机器人"></a>什么是对话机器人</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.png"></p><h4 id="对话机器人的经典流程"><a href="#对话机器人的经典流程" class="headerlink" title="对话机器人的经典流程"></a>对话机器人的经典流程</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E7%BB%8F%E5%85%B8%E6%B5%81%E7%A8%8B.png"></p><h4 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h4><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B.png"></p><p>对话机器人开始是基于关键词，然后就是模板技术，目前很多公司还在使用，优点是质量可控、准确率高，其缺点就是泛化能力比较弱。随着功能不断迭代，模板很大程度依赖于人工，不能自主提升自己的泛化能力。然后有了基于搜索的对话机器人，有很强的业务适应能力，其缺点就是准确率低。最近几年深度学习火起来后，利用深度学习替换原来的模型进行意图识别，意图识别相对传统方法准确率提升很大，但是缺点就是对数据质量要求较高。</p><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%E7%9A%84%E6%A8%A1%E6%9D%BF.png"></p><p>维护问题到模板以及模板到回答的映射关系，人工需要做很多审核以及一些校对的工作。而搜索方案，将query经过预处理打散成terms，进入搜索系统，如果按照原始结果会得到一个排序“泉州到厦门过户问题，泉州到厦门远吗，泉州到厦门怎么坐车，泉州到福州过户问题，泉州到厦门过户问题”，最后得出结果与查询一致，将最相近的query回答返回。而解决排序不正确的方法就是需要海量数据。</p><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B-%E6%90%9C%E7%B4%A2-%E7%AE%97%E6%B3%95.png"></p><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B-%E7%9F%A5%E8%AF%86%E5%BA%93.png"></p><p>多轮是一个更偏工程的过程。里面更多的算法是在做槽位解析，需要做好三件事，第一个就是填槽，如果对话过程中槽位未补全，在下轮对话过程中引导用户补全槽位信息。再者就是场景管理，需要维护海量用户的聊天信息。第三点就是可配置，多轮最后面都是一个业务问题，开发一个可配置的界面，让运营自行配置其需要的对话。多轮的逻辑是在知识库里配置的，DM是和业务无关的，只需要按配置的解析结果执行即可。</p><p>按照上面设计还是会出现风险，常见的五个风险有：任何算法的选择都只是满足当前的需求，数据是历史数据，算法是当前反馈，业务演化过程不可知； 模型互搏，各种模型都要去做A/BTest确定哪种好那种坏，之前更多的判断是从原理上判断；意图爆炸，目前知识库是基于意图回答一对一关系，业务相对收敛，但是未来发展速度可能导致意图不可收敛； 主观标准的反复，很多过程都由人工参与，每个人评判标准不一；模型更新滞后于业务发展，技术发展较快。解决方案就是永远保持主动，提前应对。</p><p>系统架构：前端有一个对话框和消息服务器，类似于IM基本架构，消息服务器会将消息路由到对话管理模块（中控）。用户聊天文本会在中控识别意图和槽位，通过意图在知识库中获取对应的话术。知识库有一个控制台，与外部交互的界面，对话管理也会访问后端云服务，比如通过ip地址获取其属于哪个城市，除此外还有语义理解、CRM服务等。</p><p><img src="/2021/03/27/Week8-%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%B7%A5%E4%B8%9A%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/%E4%B8%8D%E5%8F%AA%E6%98%AF%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F.png"></p><p>可以利用数据做商业智能，覆盖售前、售中、售后所有场景，用户沟通可追踪可优化，精准营销，从客服转化为专家顾问，实现用户服务在线化和企业在线化，最终实现整个企业的智能化。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 任务型对话 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 任务型对话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week7-Model-based-DP-and-Template-based-NLG</title>
      <link href="2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/"/>
      <url>2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="Model-based-Dialogue-Policy"><a href="#Model-based-Dialogue-Policy" class="headerlink" title="Model-based Dialogue Policy"></a>Model-based Dialogue Policy</h2><h3 id="Markov-Chain-马尔科夫链"><a href="#Markov-Chain-马尔科夫链" class="headerlink" title="Markov Chain - 马尔科夫链"></a>Markov Chain - 马尔科夫链</h3><p>马尔科夫链，它是假设某一时刻状态转移的概率只依赖于它的前一个状态。</p><p>举个形象的比喻，假如每天的天气是一个状态的话，那个今天是不是晴天只依赖于昨天的天气，而和前天的天气没有任何关系。当然这么说可能有些武断，但是这样做可以大大简化模型的复杂度，因此马尔科夫链在很多时间序列模型中得到广泛的应用，比如循环神经网络RNN，隐式马尔科夫模型HMM等，当然MC也需要它。</p><p>如果用精确的数学定义来描述，则假设我们的序列状态是$…X_{t-2}, X_{t-1}, X_{t}, X_{t+1},…$，那么我们的在时刻$X_{t+1}$的状态的条件概率仅仅依赖于时刻$X_{t}$，即：$$P(X_{t+1} |…X_{t-2}, X_{t-1}, X_{t} ) = P(X_{t+1} | X_{t})$$</p><p>既然某一时刻状态转移的概率只依赖于它的前一个状态，那么我们只要能求出系统中任意两个状态之间的转换概率，这个马尔科夫链的模型就定了。我们来看看下图这个马尔科夫链模型的具体的例子(来源于维基百科)。</p><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/markov-chain.png" alt="img"></p><p>这个马尔科夫链是表示股市模型的，共有三种状态：牛市（Bull market）, 熊市（Bear market）和横盘（Stagnant market）。每一个状态都以一定的概率转化到下一个状态。比如，牛市以0.025的概率转化到横盘的状态。这个状态概率转化图可以以矩阵的形式表示。如果我们定义矩阵阵$P$某一位置$P(i,j)$的值为$P(j|i)$,即从状态i转化到状态j的概率，并定义牛市为状态0， 熊市为状态1, 横盘为状态2. 这样我们得到了马尔科夫链模型的状态转移矩阵为：</p><p>$ P=\left( \begin{matrix} 0.9&amp;0.075&amp;0.025 \\ 0.15&amp;0.8&amp; 0.05 \\ 0.25&amp;0.25&amp;0.5 \end{matrix} \right) $</p><h3 id="HMM-隐马尔可夫模型"><a href="#HMM-隐马尔可夫模型" class="headerlink" title="HMM-隐马尔可夫模型"></a>HMM-隐马尔可夫模型</h3><h4 id="HMM-形象的例子描述"><a href="#HMM-形象的例子描述" class="headerlink" title="HMM 形象的例子描述"></a>HMM 形象的例子描述</h4><p>在引入HMM的公式化描述之前，为了更好的理解 HMM 模型，我们先用一个掷骰子的例子来形象的描述HMM模型。</p><p>模型描述 假设我手里有三个不同的骰子。</p><ul><li>第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。</li><li>第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。</li><li>第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。</li></ul><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/2018_04_10-4.png"></p><p>假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。 然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。 不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。 例如我们可能得到这么一串数字（掷骰子10次）：1635273524</p><p>这串数字叫做<strong>可见状态链</strong>。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串<strong>隐含状态链</strong>。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6-D8-D8-D6-D4-D8-D6-D6-D4-D8</p><p>一般来说，HMMHMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。</p><p>同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做<strong>输出概率</strong>（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。</p><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/2018_04_10-1.png" alt="img"></p><p><strong>隐马尔可夫示意图</strong></p><p>其实对于 HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。</p><p>但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。</p><h3 id="Markov-Chain"><a href="#Markov-Chain" class="headerlink" title="Markov-Chain"></a>Markov-Chain</h3><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/markov-chain-turns.png"></p><h3 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h3><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/Random_process.png"></p><h3 id="强化学习在Dialogue-Policy的应用"><a href="#强化学习在Dialogue-Policy的应用" class="headerlink" title="强化学习在Dialogue Policy的应用"></a>强化学习在Dialogue Policy的应用</h3><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/ReinforceLearningonDP.png"></p><h3 id="一些技巧"><a href="#一些技巧" class="headerlink" title="一些技巧"></a>一些技巧</h3><p>关注对话轮次、对话时间、对话情感变化。</p><h2 id="Template-based-Natural-Language-Generation"><a href="#Template-based-Natural-Language-Generation" class="headerlink" title="Template-based Natural Language Generation"></a>Template-based Natural Language Generation</h2><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/Template.png"></p><h2 id="Rasa-Response-Selector"><a href="#Rasa-Response-Selector" class="headerlink" title="Rasa Response Selector"></a>Rasa Response Selector</h2><h3 id="参考老师第九周课件"><a href="#参考老师第九周课件" class="headerlink" title="参考老师第九周课件"></a>参考老师第九周课件</h3><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/pizza-bot.png"></p><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/config.png"></p><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/dialogue-faq.png"></p><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/text_represenation.png"></p><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/diet.png"></p><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/diet-certain-setting.png"></p><h2 id="End2End-Conversational-AI"><a href="#End2End-Conversational-AI" class="headerlink" title="End2End Conversational AI"></a>End2End Conversational AI</h2><h3 id="The-Decoder-Block"><a href="#The-Decoder-Block" class="headerlink" title="The Decoder Block"></a>The Decoder Block</h3><p><img src="/2021/03/20/Week7-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E7%89%88%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E5%92%8C%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA/TheDecoderBlock.png"></p><h3 id="The-Decoder-Only-Block"><a href="#The-Decoder-Only-Block" class="headerlink" title="The Decoder-Only Block"></a>The Decoder-Only Block</h3><p>![](Week7-基于模版的对话生成和有限状态机/The Decoder-Only-Block.png)</p><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>1、<a href="https://www.cnblogs.com/cmt/p/14553189.html">马尔科夫链</a></p><p>2、<a href="http://ulsonhu.cn/HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B.html">HMM-隐马尔可夫模型</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 任务型对话 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 任务型对话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库的基础知识</title>
      <link href="2021/03/16/data-warehouse/"/>
      <url>2021/03/16/data-warehouse/</url>
      
        <content type="html"><![CDATA[<h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>1、<strong>数仓是什么，其实就是存储数据，体现历史变化的一个数据仓库。</strong>因为互联网时代到来，基于数据量的大小，分为了传统数仓和现代数仓。</p><p>2、传统数仓，使用传统的关系型数据库进行数据存储，因为关系型数据库本身可以使用SQL以及函数等做数据分析。所以把数据存储和数据分析功能集合为一体，加上一个可视化界面，就能从数据存储，数据分析，数据展示完整方案。</p><p>3、到了互联网时代，由于上网用户剧增，特别是移动互联网时代，海量的网络设备，导致了海量的数据产生，企业需要也希望从这些海量数据中挖掘有效信息，如行为日志数据，业务数据，爬虫数据等等中提炼出有价值信息。但传统的关系型数据库由于本身技术限制，无法很好承担这么大数据量存储和分析的任务，现代大数据技术应运而生。而数仓就是做这些海量数据存储的地方。</p><h3 id="经典数仓分层架构"><a href="#经典数仓分层架构" class="headerlink" title="经典数仓分层架构"></a>经典数仓分层架构</h3><p>​    <strong>ADS</strong></p><p>​    <strong>DWS           DIM</strong></p><p>​    <strong>DWD</strong></p><p>​    <strong>ODS</strong></p><h4 id="数据运营层（ODS）"><a href="#数据运营层（ODS）" class="headerlink" title="数据运营层（ODS）"></a>数据运营层（ODS）</h4><p>Operation Data Store 数据准备区，也称为贴源层。数据仓库源头系统的数据表通常会原封不动的存储一份，这称为ODS层，是后续数据仓库加工数据的来源。</p><p><strong>ODS层数据的来源方式：</strong></p><ul><li>业务库<ul><li>经常会使用sqoop来抽取，例如每天定时抽取一次。</li><li>实时方面，可以考虑用canal监听mysql的binlog，实时接入即可。</li></ul></li><li>埋点日志<ul><li>日志一般以文件的形式保存，可以选择用flume定时同步</li><li>可以用spark streaming或者Flink来实时接入</li><li>kafka也OK</li></ul></li><li>消息队列：即来自ActiveMQ、Kafka的数据等。</li></ul><h4 id="数据仓库层（DW）"><a href="#数据仓库层（DW）" class="headerlink" title="数据仓库层（DW）"></a>数据仓库层（DW）</h4><p>DW数据分层，由下到上为DWD，DWB，DWS。</p><ul><li>DWD: data warehouse details 细节数据层，是业务层与数据仓库的隔离层。主要对ODS数据层做一些数据清洗和规范化的操作。</li><li>数据清洗：去除空值、脏数据、超过极限范围的</li><li>DWB: data warehouse base 数据基础层，存储的是客观数据，一般用作中间层，可以认为是大量指标的数据层。</li><li><strong>DWS: data warehouse service 数据服务层，基于DWB上的基础数据，整合汇总成分析某一个主题域的服务数据层，一般是宽表。用于提供后续的业务查询，OLAP分析，数据分发等。</strong></li><li>用户行为，轻度聚合<ul><li>主要对ODS/DWD层数据做一些轻度的汇总。</li></ul></li></ul><h4 id="数据服务层-应用层（ADS）"><a href="#数据服务层-应用层（ADS）" class="headerlink" title="数据服务层/应用层（ADS）"></a>数据服务层/应用层（ADS）</h4><ul><li>ADS：applicationData Service应用数据服务，该层主要是提供数据产品和数据分析使用的数据，一般会存储在ES、mysql等系统中供线上系统使用。<ul><li>我们通过说的报表数据，或者说那种大宽表，一般就放在这里</li></ul></li></ul><p>1、其实数仓数据分层，就跟代码分层一样。如果所有数据都放在一层，就跟代码都放在一个文件，肯定是可以运行的，但带来的问题就是阅读性差，复用性和维护性降低。</p><blockquote><p>数仓的分层也是一样，每一层都有自己的职责，同时都是基于下一层或者下面多层做数据处理之后的结果。<br>这样一来，<strong>最上层就是ADS，数据应用层</strong>，当更上层需要数据时，不需要再从最底层进行数据计算，可以复用中间层级的现有结果，可以提升数据处理速度。<br>同样的，因为更上层数据都是从下一层或者下面多层数据处理而来，这样就算下层数据丢失，也不会造成企业所有数据毁灭性灾难，算是一种数据冗余机制，不过更上层数据一般做了数据处理，<strong>提升了维度信息</strong>。</p></blockquote><p>2、需要注意，大数据的特点之一就是海量数据，但是数<strong>据价值密度较低</strong>。<br>使用数据分层机制，相当于提炼黄金的流程，逐步逐步将有价值信息进行汇总聚合，这样就跟分步操作一样，最终提炼出想要的结果。同时就算原始数据丢失了，只要中间结果还在，依然可以保证最上层数据的稳定性。类似加了一层缓冲一样。</p><p>3、大数据的特点，海量数据，这其实带来了较大的存储压力。将数据进行分层之后， 最原始的数据存储周期就可以适当降低，这样可以降低存储压力。而更上层的数据，因为都是加工后的数据，数据量相对较少，存储压力就会小一些，存储周期也就可以长一些。</p><blockquote><p>综上，道理其实都是相通的，数据分层最终遵循的还是很朴素的道理。降低存储压力，降低企业使用成本，复用中间数据结果，提升数据处理速度。逐步处理，这样可以更加灵活应对企业开发需求，不需要所有需求都从数据量最大的ODS层进行数据处理。</p></blockquote><h3 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h3><h4 id="3-1-ODS层"><a href="#3-1-ODS层" class="headerlink" title="3.1 ODS层"></a>3.1 ODS层</h4><p>1、这一层又叫做贴源层，就是接近数据源的一层，需要存储的数据量是最大的，存储的数据也是最原始，最真实未经过太多处理的数据。</p><p>2、按照目前大数据企业开发的数据来源来看，不管是爬虫数据，日志数据还是业务数据，都会有一层ODS层，存放最原始的数据。</p><p>3、注意，ODS层数据还起到一个数据备份作用，如果是比较特殊行业，在ODS层的数据会保留一年甚至多年。不过普通公司一般就保存3–6个月，看数据量和存储压力以及存储预算决定。</p><p>4、日志数据估算，如日活100万用户，每个用户访问1次，每次操作5min，每个用户平均3秒一条日志数据，一条数据1kb。最后体积是100w<em>1</em>5<em>60/3</em>1kb=100w*100kb=97656.25MB=95.36GB;</p><blockquote><p>注意，数据估算最好结合公司实际情况，如果已经运行一段，可以让运维同事帮忙做估算<br>因为数据本身可以做压缩，数仓数据还需要做分层，数据本身存储时还会有备份机制(HDFS\Kafka等框架)<br>数据还会源源不断增长，同时磁盘还需要预留一定缓冲空间，一般是30%缓冲空间。所以除非是新建项目或者遇到超快速增长的公司，一般的大数据容量评估都是按照最高上限做半年甚至一年总容量做评估的。<br>注意，在服务器领域，磁盘的成本相对CPU\内存来说，成本是相对最低的，甚至有专门的存储服务器，如24硬盘位，甚至48硬盘位的服务器。<br>而2020年先在已经开发出了单磁盘12TB甚至14TB的企业硬盘，这意味着单节点机器容量上限进一步提升，存储成本也随着技术提升，成本开始降低下来。</p></blockquote><p>5、ODS中数据不意味着里面的数据就是最原始没经过处理的。一般企业开发，因为真实环境中数据上报，存储，采集中错误，bug，网络等问题，会造成原始数据的各类问题。</p><blockquote><p>字段缺失<br>数据字段不统一<br>格式错误<br>关键信息丢失等等<br>数据来源混杂<br>数据类型不一，例如json，xml，text，csv的，压缩了的，没有压缩的等等。</p></blockquote><p>6、一般企业开发时，都会对原始数据存入到ODS时，做一些最基本的处理</p><blockquote><p>数据来源区分<br>数据按照时间分区存储，一般是按照天，也有公司使用年，月，日三级分区做存储的<br>进行最基本的数据处理，如格式错误的丢弃，关键信息丢失的过滤掉等等。</p></blockquote><p>注意，有的公司ODS层不会做太多数据过滤处理，会放到DWD层来处理。<br>有的公司会在一开始时就在ODS层做数据相对精细化的过滤。这个并没有明确规定，看每个公司自己的想法和技术规范</p><p>7、ODS层建立表时，如果使用hive进行处理，一般建立外部表。</p><blockquote><p>hive的外部表，对应的是业务表;<br>hive外部表，存放数据的文件可以不是在hive的hdfs默认的位置，并且hive对应的表删除时，相应的数据文件并不会被删除。这样对于企业开发来说，可以防止因为删除表的操作而把宝贵的数据删除掉<br>hive的业务表，则相反。数据文件存放在hive对应的默认位置，表删除时，对应文件也会被删除掉。</p></blockquote><h4 id="3-2-DWD层"><a href="#3-2-DWD层" class="headerlink" title="3.2 DWD层"></a>3.2 DWD层</h4><p>1、按照建模思想，不完全星型建模。 注意，所有的框架实际实现时，都不会按照理想方式去实现，都会在实现成本，复杂度，性能，指标等方面综合考虑。</p><p>2、DWD又叫做数据明细表， 很多时候存储的都是事实表为主。</p><p>3、在DWD层，会有ETL，也就是extract transform load 提取转换加载处理，逻辑会比较复杂，这时候如果使用hive，一般无法满足要求，这些逻辑一般都是编写代码实现，然后使用脚本进行周期性如每天调用。</p><blockquote><p>1。去除废弃字段，去除格式错误的信息</p><p>2。去除丢失了关键字段的信息</p><p>3。去除不含时间信息的数据(这个看公司具体业务，但一般数据中都会带上时间戳，这样方便后续处理时，进行时间维度上信息分析处理和提取)</p><p>4。有些公司还会在这一层将数据打平，不过这具体要看业务需求。这是因为kylin适合处理展平后数据，不适合处理嵌套的表数据信息。</p><p>5。有些公司还会将数据session做切割，这个一般是app的日志数据，其他业务场景不一定适合。这是因为app有进入后台模式，例如用户上午打开app用了10分钟，然后app切入后台，晚上再打开，这时候session还是一个，实际上应该做切割才对。(也有公司会记录app进入后台，再度进入前台的记录，这样来做session切割)</p><p>6。数据规范化，因为大数据处理的数据可能来资源公司不同部门，不同项目，不同客户端，这时候可能相同业务数据字段，数据类型，空值等都不一样，这时候需要在DWD层做抹平。否则后续处理使用时，会造成很大的困扰。</p><blockquote><p>简单的，如boolean，有使用0 1标识，也有使用true false标识的<br>如字符串空值，有使用””，也有使用null，的，统一为null即可<br>如日期格式，这种就差异性更大，需要根据实际业务数据决定，不过一般都是格式化为YYYY-MM-dd HH:mm:ss 这类标准格式</p></blockquote></blockquote><p>4、注意，事实表中数据，一般不是所有维度都按照维度主键做信息存储。</p><blockquote><p>维度退化，其实从代码角度来说，就是当一个代码写死之后，失去了灵活性，维度就退化了。<br>在数仓理论中，有几个经典思想，一个是去除数据冗余。所以一般会把维度信息单独存放，其他表要使用时，记录对应维度的id即可。<br>这样，就算维度表中数据发生了变化，其他表数据因为只是记录了id，不会有影响。<br>同时，维度信息放在一张表中存放，而不是每个表中存储一份，将来需要调整，只需要做一次工作即可，降低了数据冗余。<br>这一点和代码的实现和设计思想是一致的，不要重复造轮子。</p></blockquote><p>5、在DWD层，一般还会做数据映射，</p><p>例如将GPS经纬度转换为省市区详细地址。</p><blockquote><p>业界常见GPS快速查询一般将地理位置知识库使用geohash映射，然后将需要比对的GPS转换为geohash后跟知识库中geohash比对，查找出地理位置信息。</p><p>对于geohash感兴趣，可以看我另外一篇博文。</p><p>当然，也有公司使用open api，如高德地图，百度地图的api进行GPS和地理位置信息映射，但这个达到一定次数需要花钱，所以大家都懂的。</p></blockquote><p>会将IP地址也转换为省市区详细地址。</p><blockquote><p>这个有很多快速查找库，不过基本原理都是二分查找，因为ip地址可以转换为长整数。典型的如ip2region库</p></blockquote><p>将时间转换为年，月，日甚至周，季度维度信息。</p><blockquote><p>这样带来好处就是，后续业务处理时如果需要这些信息，直接使用即可。不过会一定程度增加数据量，但一般都还可以接收，增加并不多。<br>注意，数据映射一般只映射常见指标以及明确的企业开发中后续会用到的指标，因为数据量较大，如映射的指标后续用不到，只会平白增加开发，维护成本。</p></blockquote><p>6、DWD存储数据，一般就是维度表，事实表，实体表等数据。</p><blockquote><p><strong>维度表</strong>，顾名思义，就是一些维度信息，这种表数据，一般就直接存储维度信息，很多时候维度表都不会很大。</p><p><strong>事实表</strong>，就是表述一些事实信息，如订单，收藏，添加购物车等信息。这种数据量较大，同时因为数据可能快速或者缓慢变化，这种一般存储维度主键，具体维度值在后续处理分析时再临时关联。</p><p><strong>实体表</strong>，类似javabean，用来描述信息的，如优惠券表，促销表。内部就是一些描述信息。这种一般看数据量以及变化程度，大部分时候都是全量导入，导入周期则看具体而定。</p></blockquote><h4 id="3-3-DWS层"><a href="#3-3-DWS层" class="headerlink" title="3.3 DWS层"></a>3.3 DWS层</h4><p>1、DWS，俗称的数据服务层，也有叫做数据聚合层.不过按照经典数据建模理论，一般称之为前者，也就是数据服务层，为更上层的ADS层或者直接面向需求方服务。</p><p>2、DWS建模，一般使用主题建模，维度建模等方式。</p><p>主题建模，顾名思义，围绕某一个业务主体进行数据建模，将相关数据抽离提取出来。</p><blockquote><p>如，将流量会话按照天，月进行聚合<br>将每日新用户进行聚合<br>将每日活跃用户进行聚合</p></blockquote><p>维度建模，其实也差不多，不过是根据业务需要，提前将后续数据查询处理需要的维度数据抽离处理出来，方便后续查询使用。</p><blockquote><p>如将运营位维度数据聚合<br>将渠道拉新维度数据聚合</p></blockquote><h4 id="3-4-ADS层"><a href="#3-4-ADS层" class="headerlink" title="3.4 ADS层"></a>3.4 ADS层</h4><p>1、这是应用服务层，一般就直接对接OLAP分析，或者业务层数据调用接口了。<br>2、这是最顶层，一般都是结果类型数据，可以直接拿去使用或者展示的数据了。也是对数据抽离分析程度最高的一层数据。<br>3、这一层是需求最明确的一层，根据业务需求来决定数据维度和结果分析。类似代码最外层，接口是相对最固化的。</p><h2 id="数据中台、数据仓库、大数据平台的区别是什么"><a href="#数据中台、数据仓库、大数据平台的区别是什么" class="headerlink" title="数据中台、数据仓库、大数据平台的区别是什么"></a>数据中台、数据仓库、大数据平台的区别是什么</h2><p>针对企业内部来说，构建良好的数据管理系统和资产管理系统，在提供数据服务的方面能快速获取到所需的数据，同时也提供数据存储。</p><p>数据中台、数据仓库、大数据平台的关键区别是什么？</p><p>1、数据中台是企业级的逻辑概念，体现企业 D2V（Data to Value）的能力，为业务提供服务的主要方式是数据 API，即数据中台的服务对象是企业上下游的各类业务系统。</p><p>2、数据仓库是一个相对具体的功能概念，是存储和管理一个或多个主题数据的集合，为业务提供服务的方式主要是分析报表。数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它并不是所谓的“大型数据库”，<strong>而是一整套包括了etl、调度、建模在内的完整的理论体系，</strong></p><p>3、大数据平台是在大数据基础上出现的融合了结构化和非结构化数据的数据基础平台，为业务提供服务的方式主要是直接提供数据集。</p><p>数据中台是在数据仓库和大数据平台的基础上，将数据生产为一个个数据 API 服务，以更高效的方式提供给业务，本质是一个构建在数据仓库之上的跨业务主题的业务系统。</p><h2 id="其他补充"><a href="#其他补充" class="headerlink" title="其他补充"></a>其他补充</h2><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><ul><li>ETL ：Extract-Transform-Load，用于描述将数据从来源端经过<strong>抽取、转换、加载</strong>到目的端的过程。</li></ul><h3 id="宽表"><a href="#宽表" class="headerlink" title="宽表"></a>宽表</h3><ul><li>含义：指字段比较多的数据库表。通常是指业务主体相关的指标、纬度、属性关联在一起的一张数据库表。</li><li>特点：<ul><li>宽表由于把不同的内容都放在同一张表，宽表已经不符合三范式的模型设计规范：<ul><li>坏处：数据有大量冗余</li><li>好处：查询性能的提高和便捷</li></ul></li><li>宽表的设计广泛应用于数据挖掘模型训练前的数据准备，通过把相关字段放在同一张表中，可以大大提供数据挖掘模型训练过程中迭代计算的消息问题。</li></ul></li></ul><h3 id="数据库设计三范式"><a href="#数据库设计三范式" class="headerlink" title="数据库设计三范式"></a>数据库设计三范式</h3><p>为了建立冗余较小、结构合理的数据库，设计数据库时必须遵循一定的规则。在关系型数据库中这种规则就称为范式。范式时符合某一种设计要求的总结。</p><ol><li>第一范式：确保每列保持原子性，即要求<strong>数据库表中的所有字段值都是不可分解的原子值</strong>。</li><li>第二范式：确保表中的每列都和主键相关。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。作用：减少了数据库的冗余。</li><li>第三范式：确保每列都和主键列直接相关，而不是间接相关。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>1、<a href="https://www.jianshu.com/p/9bcfab62644f">数据仓库中的ODS DW DM理解</a></p><p>2、<a href="https://blog.csdn.net/xiaohu21/article/details/109149589">数仓数据分层(ODS DWD DWS ADS)换个角度看</a></p><p>3、<a href="https://www.jianshu.com/p/0b6414f92442">数据仓库介绍</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>week6-基于模型的对话跟踪和基于规则的Dialogue Policy</title>
      <link href="2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/"/>
      <url>2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/</url>
      
        <content type="html"><![CDATA[<h1 id="Model-based-DST-and-Rule-based-Dialogue-Policy"><a href="#Model-based-DST-and-Rule-based-Dialogue-Policy" class="headerlink" title="Model-based DST and Rule-based Dialogue Policy"></a>Model-based DST and Rule-based Dialogue Policy</h1><h2 id="Model-based-Dialogue-State-Tracking-DST"><a href="#Model-based-Dialogue-State-Tracking-DST" class="headerlink" title="Model-based Dialogue State Tracking(DST)"></a>Model-based Dialogue State Tracking(DST)</h2><h3 id="Review"><a href="#Review" class="headerlink" title="Review"></a><strong>Review</strong></h3><p><strong>任务型对话系统</strong>（task-oriented dialogue system）的目标是协助用户完成特定的任务，比如订机票、打车、日程管理等。</p><p>一个典型的任务型对话系统可以分为四个模块：</p><blockquote><p><strong>自然语言理解（natural language understanding, NLU）、</strong></p><p><strong>对话状态追踪（dialogue state tracking, DST）、</strong></p><p><strong>策略学习（dialogue policy）</strong></p><p><strong>自然语言生成（natural language generation, NLG），</strong></p></blockquote><p>在这个过程中需要跟各种各样的数据库进行查询甚至更新等操作。</p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/task-oriented-dialogue.png"></p><p><strong>对话跟踪模块及其限制</strong></p><p>对话状态跟踪模块是任务型对话系统的一个核心模块，对话状态表示了用户在每轮对话所寻求的内容的关键信息，它是从对话开始到当前轮的所有信息的累积，表示形式是一些 slot-value pairs 的集合，inform 表示用户对所寻求的内容的限制，request 表示用户想要寻求哪些内容。</p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/%E5%AF%B9%E8%AF%9D%E7%8A%B6%E6%80%81%E8%A1%A8%E7%A4%BA.png"></p><h3 id="What’s-DST"><a href="#What’s-DST" class="headerlink" title="What’s DST"></a><strong>What’s DST</strong></h3><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/DST.png"></p><p>DST 的做法是先通过 NLU 模块对用户对话进行意图识别和槽位解析，然后将结果输入到 DST 模块中，由 DST 模块处理 NLU 模块带来的一些不确定性，得到最终的对话状态。目前越来越多的工作直接通过一个端到端的 DST 模型来直接处理用户对话（以及历史对话记录）得到当前的对话状态，如图 3 所示</p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84DST.png" alt="端到端的DST"></p><p>延申知识点：DSI</p><p><strong>Dialogue State Induction Using Neural Latent Variable Models</strong></p><p><strong>基于神经隐变量模型的对话状态推理</strong></p><p><strong>论文链接：</strong><a href="https://www.ijcai.org/Proceedings/2020/0532.pdf">https://www.ijcai.org/Proceedings/2020/0532.pdf</a></p><p><strong>代码链接：</strong><a href="https://github.com/taolusi/dialogue-state-induction">https://github.com/taolusi/dialogue-state-induction</a></p><p><strong>PPT链接：</strong><a href="https://taolusi.github.io/qingkai_min/assets/pdf/20-ijcai-dsi_slides.pdf">https://taolusi.github.io/qingkai_min/assets/pdf/20-ijcai-dsi_slides.pdf</a></p><p><strong>视频接：</strong><a href="https://www.bilibili.com/video/BV1fV41127tq">https://www.bilibili.com/video/BV1fV41127tq</a></p><p>论文总结：</p><p>对话状态跟踪模块是任务型对话系统中的核心部件，目前主流的对话状态跟踪的方法需要在大量人工标注的数据上进行训练。然而，对于现实世界中的各种客户服务对话系统来说，人工标注的过程存在代价高、标注慢、错误率高以及难以覆盖数量庞大的不同领域等问题。</p><p>基于这些问题，提出了一个新的任务：**对话状态推理 (dialogue state induction, DSI)**，任务的目标是从大量的生对话语料中自动推理得到对话状态，并能更好的用于下游任务比如策略学习和对话生成。</p><p>DSI 与 DST 的区别如图 5 所示，和 DST 相似的是，DSI 的输出也是 slot-value pairs 形式的对话状态，不同的是，在训练过程中 DST 依赖于对话语料以及人工标注的对话状态，而 DSI 不依赖于人工标注，可以在生对话中自动生成 slot-value pairs。</p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/DST%E4%B8%8EDSI%E5%AF%B9%E6%AF%94.png"></p><p><strong>Input and Output</strong></p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/input-and-output.png"></p><p><strong>BERT-based model for DST（基于Bert模型的对话跟踪）</strong></p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/simple_bert_model_for_dst.png"></p><p>上图显示了我们提议的BERT在DST中的应用。在较高的级别上，给定一个对话框上下文和一个候选槽位值对，我们的模型将输出一个得分，以指示该候选对象的相关性。换句话说，该方法类似于句子对的分类任务。</p><p>第一个输入对应于对话框上下文，它由上一回合的系统话语（system utterance）和当前回合的用户话语（user utterance）组成。这两个话语由[SEP] Token分隔。</p><p>第二个输入是候选的slot-value对。 我们简单地将候选对表示为标记序列（单词或单词片段）。 将两个输入段连接成一个单个令牌序列，然后简单地传递给BERT以获取输出向量（$h_1，h_2···，h_M$）。 在此，$M$表示输入令牌的总数（包括特殊标记，例如[CLS]和[SEP]）。</p><p>基于对应于第一特殊令牌-[CLS]（即h1）的输出矢量，候选时隙值对相关的概率为：<br>$$<br>y = σ(Wh_1 + b) ∈ R<br>$$<br>其中变换矩阵W和偏置项b是模型参数，σ表示Sigmoid函数。 它将分数压缩到0到1之间的概率。</p><p>在每个Turn中，使用所提出的基于BERT的模型来估计每个候选slot-value对的概率值。 此后，仅将预测概率值至少为0.5的slot-value对选择出来作为该turn的最终预测值。 为了获得当前回合的对话状态，我们使用新预测的槽值对来更新上一回合状态下的对应值。 例如，假设用户在当前回合中指定了一个food = chinese的餐馆。 如果对话框状态没有现有的food参数，则可以将food = chinese添加到对话框状态。 如果以前指定过food = korean，我们将其替换为food = chinese。</p><p><strong>论文链接：</strong><a href="https://arxiv.org/pdf/1910.12995.pdf">https://arxiv.org/pdf/1910.12995.pdf</a></p><p><strong>代码链接：</strong><a href="https://github.com/laituan245/BERT-Dialog-State-Tracking">https://github.com/laituan245/BERT-Dialog-State-Tracking</a></p><p><strong>代码链接：</strong><a href="https://github.com/BSlience/BERT-DST/">https://github.com/BSlience/BERT-DST/</a></p><h3 id="Further-DST"><a href="#Further-DST" class="headerlink" title="Further DST"></a>Further DST</h3><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/personl-Info.PNG"></p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/UserExperience.png"></p><h2 id="Rule-based-Dialogue-Policy-DP"><a href="#Rule-based-Dialogue-Policy-DP" class="headerlink" title="Rule-based Dialogue Policy(DP)"></a>Rule-based Dialogue Policy(DP)</h2><h3 id="Review-1"><a href="#Review-1" class="headerlink" title="Review"></a>Review</h3><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/DPReview.png"></p><h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>Dialogue Policy: Rule-Based, Change Slot, Repeat Intent, System state</p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/SystemState.png"></p><h3 id="RASA-Policy"><a href="#RASA-Policy" class="headerlink" title="RASA Policy"></a>RASA Policy</h3><p> <img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/Transformers.png"></p><p><img src="/2021/03/04/Week6-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E8%AF%9D%E8%B7%9F%E8%B8%AA%E5%92%8C%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84DialoguePolicy/Action.png"></p><p>详细可参考youtube视频：</p><p><a href="https://www.youtube.com/watch?v=dmD3sqzxDXs&list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb&index=30&ab_channel=Rasa">RASA Whiteboard RulePolicy</a></p><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p><a href="https://yan624.github.io/posts/89f2cf08.html">DST论文笔记</a></p><p><a href="https://blog.csdn.net/yagreenhand/article/details/100050602">DST文章学习</a></p><p><a href="https://blog.csdn.net/yizhen_nlp/article/details/108649313">Dialogue State Induction Using Neural Latent Variable Models</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 任务型对话 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 任务型对话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week5-Further NLU and Dialogue State Tracking</title>
      <link href="2021/02/07/Week5-HuggingFace&#39;s-Transformers/"/>
      <url>2021/02/07/Week5-HuggingFace&#39;s-Transformers/</url>
      
        <content type="html"><![CDATA[<h1 id="Further-NLU-and-Dialogue-State-Tracking"><a href="#Further-NLU-and-Dialogue-State-Tracking" class="headerlink" title="Further NLU and Dialogue State Tracking"></a>Further NLU and Dialogue State Tracking</h1><h2 id="Hugging-Face’s-Transformer"><a href="#Hugging-Face’s-Transformer" class="headerlink" title="Hugging Face’s Transformer"></a>Hugging Face’s Transformer</h2><h3 id="Attention-以及-Self-Attention"><a href="#Attention-以及-Self-Attention" class="headerlink" title="Attention 以及 Self-Attention"></a>Attention 以及 Self-Attention</h3><ul><li>1、Attention</li></ul><p>是神经网络中的一种机制，<strong>模型可以通过选择性地关注给定的数据集来学习做出预测。</strong>Attention的个数是通过学习权重来量化的，输出通常是一个加权平均值。</p><ul><li>2、Self-Attention</li></ul><p>是一种注意力机制，模型利用对同一样本观测到的其他部分来对数据样本的剩下部分进行预测。从概念上讲，它感觉非常类似于non-local的方式。还要注意的是，Self-attention是置换不变的；换句话说，它是对集合的一种操作。</p><p>而关于attention和self-attention存在非常多的形式，最常见的Transformer是依赖于scaled-dot-product的形式，也就是：给定query矩阵Q, key矩阵K以及value矩阵V，那么我们的输出就是值向量的加权和，其中，分配给每个值槽的权重由Query与相应Key的点积确定。</p><p>$$<br>Attention(Q,K,V) = software(\frac{QK^T} {\sqrt{d_k}}) V<br>$$</p><p>对于一个query以及一个key向量，$q_i,k_j \in R^d$，我们计算下面的值：</p><p>$$<br>a_{ij} = softmax(\frac{q_ik^Y_j}{\sqrt{d_k}}) = \frac{exp(q_ik_j^T)}{\sqrt{d_k}\sum_{r\in S_i} exp(q_ik_r^T)}<br>$$</p><p>其中，$S_i$是keys的集合。</p><ul><li>Multi-Head Self-Attention</li></ul><p>multi-head self-attention是Transformer的核心组成部分，和简单的attention不同之处在于，Multi-head机制将输入拆分为许多小的chunks，然后并行计算每个子空间的scaled dot product，最后我们将所有的attention输出进行拼接，</p><p>$$<br>\begin{align}<br>MultiheadAttention(X_q,X_k,X_v) &amp; = [head_1;…,head_h]W^0head_i \<br> &amp; = Attention(X_qW_i^q, X_kW^k_i, X_vW_i^v)<br>\end{align}<br>$$</p><p>其中，$[.;.]$是concate操作，$W^q_i, W_i^k \in R^{d*d_k/h}, W_i^v \in R^{d <em>d_v/h}$是权重矩阵，它将我们的输出embeddings(L</em>d)的映射到query,key,value矩阵，而且$W^o \in R^{d_v * d}$是输出的线性转化，这些权重都是在训练的时候进行训练的。</p><p><img src="/2021/02/07/Week5-HuggingFace's-Transformers/640.webp"></p><p>Transformer，很多时候我们也称之为”vanilla Transformer”, 它有一个encoder-decoder的结构，decoder的Transformer可以在语言建模的时候获得非常好的效果。</p><h3 id="Encoder-Decoder结构"><a href="#Encoder-Decoder结构" class="headerlink" title="Encoder-Decoder结构"></a>Encoder-Decoder结构</h3><p>Encoder生成一个基于attention的表示，能够从一个大的上下文中定位一个特定的信息片段。它由6个身份识别模块组成，每个模块包含两个子模块、一个multi-head self-attention和一个point-wise全连接前馈网络。</p><p>按point-wise来说，这意味着它对序列中的每个元素应用相同的线性变换（具有相同的权重）。<strong>这也可以看作是滤波器大小为1的卷积层</strong>。每个子模块都有一个剩余连接和layer normalization。所有子模块输出相同维度的数据。</p><p>Transformer的decoder功能是从encoder的表示中抽取信息。该结构与encoder非常相似，只是decoder包含两个多头注意子模块，而不是在每个相同的重复模块中包含一个。第一个多头注意子模块被屏蔽，以防止位置穿越。</p><p><img src="/2021/02/07/Week5-HuggingFace's-Transformers/TransformerModel.jpg"></p><h3 id="Improved-Attention-Span"><a href="#Improved-Attention-Span" class="headerlink" title="Improved Attention Span"></a>Improved Attention Span</h3><p>提高Attention Span的目的是使可用于self-attention的上下文更长、更有效、更灵活。</p><h4 id="1-Longer-Attention-Span-Transformer-XL"><a href="#1-Longer-Attention-Span-Transformer-XL" class="headerlink" title="1. Longer Attention Span(Transformer-XL)"></a>1. Longer Attention Span(Transformer-XL)</h4><p>vanilla Transformer有一个固定的和有限的注意广度。在每个更新步骤中，该模型只能处理同一段中的其他元素，并且没有任何信息可以在分离的固定长度段之间流动。<strong>也就是说层数固定不够灵活，同时对于算力需求非常大，导致其并不适合处理超长序列。</strong></p><p>这种context segmentation会导致几个问题：</p><ul><li>模型不能捕获非常长期的依赖关系;</li><li>在没有上下文或上下文很薄的情况下，很难预测每个片段中的前几个tokens。</li><li>评估是昂贵的。每当segment右移一位时，新的segment就会从头开始重新处理，尽管有很多重叠的tokens。</li></ul><p><strong>Transformer-XL</strong>解决来上下文的segmentation问题：</p><ul><li>对于segments之间的隐藏状态进行重复使用；</li><li>使用位置编码使其适用于重新使用的states;</li></ul><h4 id="2-Relative-Positional-Encoding"><a href="#2-Relative-Positional-Encoding" class="headerlink" title="2. Relative Positional Encoding"></a>2. Relative Positional Encoding</h4><p>为了处理这种新的attention span的形式，Transformer-XL提出了一种新的位置编码。如果使用相同的方法对绝对位置进行编码，则前一段和当前段将分配相同的编码，这是不需要的。</p><p>为了保持位置信息流在各段之间的一致性，Transformer XL对相对位置进行编码，因为它足以知道位置的offset，从而做出更好的预测，即$：i-j$,在一个key向量$k_{T,j}$以及它的query之间$q_{T,i}$。</p><h4 id="3-Adaptive-Attention-Span"><a href="#3-Adaptive-Attention-Span" class="headerlink" title="3. Adaptive Attention Span"></a>3. Adaptive Attention Span</h4><p>Transformer的一个关键优势是能够捕获长期依赖关系。根据上下文的不同，模型可能更愿意在某个时候比其他人更进一步地注意；或者一个attention head可能有不同于另一个attention head的注意模式。如果attention span能够灵活地调整其长度，并且只在需要时再往回看，这将有助于减少计算和内存开销，从而在模型中支持更长的最大上下文大小(这就是Adaptive Attention Span的动机)。</p><p>后来Sukhbaatar等人提出了一种self-attention机制以寻找最优的attention span，他们假设不同的attention heads可以在相同的上下文窗口中赋予不同的分数，因此最优的span可以被每个头分开训练。</p><h4 id="4-Localized-Attention-Span-Image-Transformer"><a href="#4-Localized-Attention-Span-Image-Transformer" class="headerlink" title="4. Localized Attention Span (Image Transformer)"></a>4. Localized Attention Span (Image Transformer)</h4><p>Transformer最初用于语言建模。文本序列是一维的，具有明确的时间顺序，因此attention span随着上下文大小的增加而线性增长。</p><p>然而，如果我们想在图像上使用Transformer，我们还不清楚如何定义上下文的范围或顺序。Image Transformer采用了一种图像生成公式，类似于Transformer框架内的序列建模。此外，图像Transformer将self-attention span限制在局部邻域内，因此模型可以放大以并行处理更多的图像，并保持可能性损失可控。</p><p>encoder-decoder架构保留用于image-conditioned生成：</p><ul><li>encoder生成源图像的上下文化的每像素信道表示；</li><li>decoder自回归地生成输出图像，每个时间步每像素一个通道。</li></ul><p>让我们将要生成的当前像素的表示标记为查询$q$。其表示将用于计算$q$的其他位置是关键向量$k_1, k_2……$它们一起形成一个内存矩阵$M$。$M$的范围定义了像素查询$q$的上下文窗口。</p><p>Image Transformer引入了两种类型的localized $M$，如下所示。</p><p><img src="/2021/02/07/Week5-HuggingFace's-Transformers/AttentionSpan.webp"></p><ul><li>(1)、1D Local Attention:输入图像按光栅扫描顺序（即从左到右、从上到下）展平。然后将线性化后的图像分割成不重叠的查询块。上下文窗口由与相同的查询块中的像素和在此查询块之前生成的固定数量的附加像素组成。</li><li>(2)、2D Local Attention:图像被分割成多个不重叠的矩形查询块。查询像素可以处理相同内存块中的所有其他像素。为了确保左上角的像素也可以有一个有效的上下文窗口，内存块将分别向上、左和右扩展一个固定的量。</li></ul><h2 id="Sentiment-Analysis-with-Bert"><a href="#Sentiment-Analysis-with-Bert" class="headerlink" title="Sentiment Analysis with Bert"></a>Sentiment Analysis with Bert</h2><p>上一周课程的回顾，需要动手实践，链接：</p><p>1、<a href="https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S">sentiment-analysis-with-bert</a></p><p>2、<a href="https://github.com/marchboy/Getting-Things-Done-with-Pytorch">https://github.com/marchboy/Getting-Things-Done-with-Pytorch</a>  第八节</p><h2 id="Deploy-BERT-for-Sentiment-Analysis-with-Transformers-by-Hugging-Face-and-FastAPI"><a href="#Deploy-BERT-for-Sentiment-Analysis-with-Transformers-by-Hugging-Face-and-FastAPI" class="headerlink" title="Deploy BERT for Sentiment Analysis with Transformers by Hugging Face and FastAPI"></a>Deploy BERT for Sentiment Analysis with Transformers by Hugging Face and FastAPI</h2><p>模型部署，需动手实践，链接：</p><p>1、<a href="https://github.com/curiousily/Deploy-BERT-for-Sentiment-Analysis-with-FastAPI">https://github.com/curiousily/Deploy-BERT-for-Sentiment-Analysis-with-FastAPI</a> </p><p>2、<a href="https://github.com/marchboy/Getting-Things-Done-with-Pytorch">https://github.com/marchboy/Getting-Things-Done-with-Pytorch</a> 第九节</p><p>3、<a href="https://curiousily.com/posts/deploy-bert-for-sentiment-analysis-as-rest-api-using-pytorch-transformers-by-hugging-face-and-fastapi/">Deploy BERT for Sentiment Analysis as REST API using PyTorch, Transformers by Hugging Face and FastAPI</a></p><h2 id="RASA-DIET"><a href="#RASA-DIET" class="headerlink" title="RASA DIET"></a>RASA DIET</h2><h3 id="Why-use-DIET"><a href="#Why-use-DIET" class="headerlink" title="Why use DIET"></a><strong>Why use DIET</strong></h3><p>Large-scale pre-trained language models aren’t ideal for developers building conversational AI applications.</p><p>DIET is different because it:</p><ul><li><p>Is a modular architecture that fits into a typical software development work-flow</p></li><li><p>是适合典型软件开发工作流程的模块化架构</p></li><li><p>Parallels large-scale pre-trained language models in accuracy and performance</p></li><li><p>并行大规模预训练语言模型的准确性和性能</p></li><li><p>Improves upon current state of the art and is 6X faster to train</p></li><li><p>改进了当前的技术水平，训练速度提高了6倍</p></li></ul><h3 id="How-it-works"><a href="#How-it-works" class="headerlink" title="How it works?"></a><strong>How it works?</strong></h3><p><strong>Dual Intent and Entity Transformer(DIET)</strong> as its name suggests is a transformer architecture that can handle both intent classification and entity recognition together. The best thing about DIET is its flexibility. It provides the ability to plug and play various pre-trained embeddings like BERT, GloVe, ConveRT, and so on. So, based on your data and number of training examples, you can experiment with various SOTA NLU pipelines without even writing a single line of code.</p><p><img src="/2021/02/07/Week5-HuggingFace's-Transformers/Week5-HuggingFace's-Transformers%5CDIET-widget.gif"></p><p><font color="red" size="5"><strong>重要论文参考：</strong></font></p><p><a href="https://zhuanlan.zhihu.com/p/337181983">https://zhuanlan.zhihu.com/p/337181983</a></p><p><font color="red"> <a href="https://www.yiyibooks.cn/nlp/diet/index.html">https://www.yiyibooks.cn/nlp/diet/index.html</a> </font></p><p><font color="red"> <a href="https://arxiv.org/pdf/2004.09936.pdf">https://arxiv.org/pdf/2004.09936.pdf</a> </font></p><p>视频：</p><p><a href="https://www.youtube.com/watch?v=vWStcJDuOUk&feature=emb_logo&ab_channel=Rasa">Rasa Algorithm Whiteboard - Diet Architecture 1: How it Works</a></p><p><a href="https://www.youtube.com/watch?v=KUGGuJ0aTL8&feature=emb_logo&ab_channel=Rasa">Rasa Algorithm Whiteboard - Diet Architecture 2: Design Decisions</a></p><p><a href="https://youtu.be/wWNMST6t1TA">https://youtu.be/wWNMST6t1TA</a></p><h3 id="Why-does-this-Architecture-look-this-way"><a href="#Why-does-this-Architecture-look-this-way" class="headerlink" title="Why does this Architecture look this way?"></a><strong>Why does this Architecture look this way?</strong></h3><p>Custom ~</p><p>类似于乐高玩具，可以自定义各个模块</p><h3 id="How-to-Choose-Pipeline"><a href="#How-to-Choose-Pipeline" class="headerlink" title="How to Choose Pipeline?"></a><strong>How to Choose Pipeline?</strong></h3><p><img src="/2021/02/07/Week5-HuggingFace's-Transformers/pipeline.png"></p><h3 id="You-already-have-Intent-Slot！But-…-which-slots-do-you-need"><a href="#You-already-have-Intent-Slot！But-…-which-slots-do-you-need" class="headerlink" title="You already have Intent + Slot！But … which slots do you need?"></a><strong>You already have Intent + Slot！But … which slots do you need?</strong></h3><p>API</p><p><img src="/2021/02/07/Week5-HuggingFace's-Transformers/SlotAndHiddenSlot.png"></p><p>No Slot neither Hidden Slot and How to do with Multi-turn?</p><p><img src="/2021/02/07/Week5-HuggingFace's-Transformers/NOINFO.png"></p><h2 id="FSM"><a href="#FSM" class="headerlink" title="FSM"></a>FSM</h2><p>有限状态机，也称为FSM(Finite State Machine)，其在任意时刻都处于有限状态集合中的某一状态。当其获得一个输入字符时，将从当前状态转换到另一个状态，或者仍然保持在当前状态。</p><p>任何一个FSM都可以用状态转换图来描述，图中的节点表示FSM中的一个状态，有向加权边表示输入字符时状态的变化。</p><p><img src="/2021/02/07/Week5-HuggingFace's-Transformers/FSM.png"></p><p>如果图中不存在与当前状态与输入字符对应的有向边，则FSM将进入“消亡状态(Doom State)”，此后FSM将一直保持“消亡状态”。</p><p>状态转换图中还有两个特殊状态：状态1称为“起始状态”，表示FSM的初始状态。状态6称为“结束状态”，表示成功识别了所输入的字符序列。</p><p>在启动一个FSM时，首先必须将FSM置于“起始状态”，然后输入一系列字符，最终，FSM会到达“结束状态”或者“消亡状态”。</p><blockquote><p><strong>PS：在信息工程，通信技术领域的状态机定义</strong></p><p>状态机由状态寄存器和组合逻辑电路构成，能够根据控制信号按照预先设定的状态进行状态转移，是协调相关信号动作、完成特定操作的控制中心。有限状态机简写为FSM（Finite State Machine），主要分为2大类：</p><p>第一类，若输出只和状态有关而与输入无关，则称为Moore状态机</p><p>第二类，输出不仅和状态有关而且和输入有关系，则称为Mealy状态机</p></blockquote><p>状态机就是状态转移图。举个最简单的例子，人有三个状态：健康，感冒，康复中。触发的条件有淋雨（t1），吃药（t2），打针（t3），休息（t4）。所以状态机就是健康-（t4）-&gt;健康；健康-（t1）-&gt;感冒；感冒-（t3）-&gt;健康；感冒-（t2）-&gt;康复中；康复中-（t4）-&gt;健康，等等。就是这样状态在不同的条件下跳转到自己或不同状态的图。</p><blockquote><p>状态机可归纳为4个要素，即现态、条件、动作、次态。这样的归纳，主要是出于对状态机的内在因果关系的考虑。“现态”和“条件”是因，“动作”和“次态”是果。详解如下：</p><p>①现态：是指当前所处的状态。</p><p>②条件：又称为“事件”，当一个条件被满足，将会触发一个动作，或者执行一次状态的迁移。</p><p>③动作：条件满足后执行的动作。动作执行完毕后，可以迁移到新的状态，也可以仍旧保持原状态。动作不是必需的，当条件满足后，也可以不执行任何动作，直接迁移到新状态。</p><p>④次态：条件满足后要迁往的新状态。“次态”是相对于“现态”而言的，“次态”一旦被激活，就转变成新的“现态”了。</p></blockquote><h2 id="Reference："><a href="#Reference：" class="headerlink" title="Reference："></a>Reference：</h2><p>1、<a href="https://mp.weixin.qq.com/s/qzC1QosFXdI9iIl3VHlutA">Transformer的一家！</a></p><p>2、<a href="https://www.cnblogs.com/williamjie/p/9504092.html">有限状态机FSM详解及其实现</a></p><p>3、<a href="https://zhuanlan.zhihu.com/p/44976296">用FSM轻松管理复杂状态</a></p><p>4、<a href="https://medium.com/the-research-nest/using-the-diet-classifier-for-intent-classification-in-dialogue-489c76e62804">Using the DIET classifier for intent classification in dialogue</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 任务型对话 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 任务型对话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week4-自然语言理解NLU</title>
      <link href="2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/"/>
      <url>2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/</url>
      
        <content type="html"><![CDATA[<h1 id="关于CrossWOZ数据集"><a href="#关于CrossWOZ数据集" class="headerlink" title="关于CrossWOZ数据集"></a>关于CrossWOZ数据集</h1><p>第一个大规模的中文跨域“人机交互”任务导向的数据集。</p><p>CrossWOZ包含 6K 个对话，102K 个句子，涉及 5 个领域（景点、酒店、餐馆、地铁、出租）。</p><p>将对话分成五种类型：单领域 S，多领域 M，多领域加交通 M+T，跨领域 CM，跨领域加交通 CM+T。交通代表了地铁和出租领域，M 和 CM 的区别是有没有跨领域的约束。</p><p>此外，语料库包含丰富的对话状态注释，以及用户和系统端的对话行为。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;dialog_action&quot;:[</span><br><span class="line">  &quot;Greet&quot;,</span><br><span class="line">  &quot;Domain&quot;,</span><br><span class="line">  &quot;Entity&quot;,</span><br><span class="line">  &quot;Slot&quot;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>更多关于CrossWOZ，参考：</p><p>1、<a href="https://arxiv.org/pdf/2002.11893.pdf">CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Datase</a></p><p>2、<a href="https://www.aminer.cn/research_report/5f178da221d8d82f52e5a305">这篇顶会，助你徒手搭建任务导向对话系统</a></p><h1 id="Intent-Classification"><a href="#Intent-Classification" class="headerlink" title="Intent Classification"></a>Intent Classification</h1><p>在常见的一个意图的分类情形中，实际还需要处理其他更为复杂的意图识别情形，有：</p><p><strong>情形一  NLU中意图和槽位的多样性：</strong></p><p>在对话系统的NLU中，意图识别（Intent Detection）和槽位填充（Slot Filling）是两个重要的子任务。其中，意图识别可以看做是NLP中的一个分类任务，而槽位填充可以看做是一个序列标注任务。</p><p>在早期的系统中，通常的做法是将两者拆分成两个独立的子任务。但这种做法跟人类的语言理解方式是不一致的，事实上我们在实践中发现，两者很多时候是具有较强相关性的，比如下边的例子：</p><blockquote><p>1.我要听[北京天安门, song] – Intent：播放歌曲<br>2.帮我叫个车，到[北京天安门, location] – Inent：打车<br>3.播放[忘情水, song] – Intent：播放歌曲<br>4.播放[复仇者联盟, movie] – Intent：播放视频</p></blockquote><p>例子1和2中，可以看到同样是“北京天安门”，由于意图的不同，该实体具备完全不同的槽位类型。</p><p>例子3和4中，由于槽位类型的不同，导致了最终意图的不同，这往往意味着，在对话系统中的后继流程中将展现出完全不同的行为—–打开网易音乐播放歌曲 or 打开爱奇艺播放电影。</p><p><strong>情形二  Multi Intents：</strong></p><p>在对话NLU中，可能还会出现一些多意图的情形： 如景点距离我多远，能帮我导航出来吗？</p><p>这里就涉及到用户的几个意图：找景点、找距离。</p><p>那在做多分类的时候，一般是在网络输出层（Output Layer）上，需要接上一层Softmax activation function；接着输出每一类的概率，针对每一类的概率之后，需要识别每一个分类来判断它是属于0还是1，所以还需要接上一层Sigmoid，并且Loss也不能使用交叉熵了。有：</p><p>Softmax激活函数<br>$$<br>Softmax_activation_function = \frac{e^{z_i}}{\sum_{j=1}^Ke^{z_j}}<br>$$<br>交叉熵<br>$$<br>Cross Entropy = -\sum_{c=1}^M{y_{i,c}}log(p_{i,c})<br>$$</p><p>Sigmoid激活函数<br>$$<br>Sigmoid = \frac{1}{e^x + 1}<br>$$<br>信息熵<br>$$<br>H(X)=−\sum_{i=1}^n p(xi)log(p(xi))<br>$$</p><p><strong>情形三  多轮返回（Multi Turn）</strong></p><p>包含上下文的信息，需要引入history，例如：</p><p>今天天气不好啊，我下午能去做什么呢—&gt; 去打球、在家打游戏，出去唱K</p><p>在此引出BERT框架，大致为：</p><p>预料【输入】 –&gt; 分词（Tokenization）–&gt;</p><p>CLS、               Tok1、…、Tok N –&gt; <strong>BERT</strong> –&gt;</p><p>C（vector）、 T1、   …、 T N     —&gt;  Dense Layer +Activation  –&gt; Probalities【输出】</p><h1 id="关于Bert"><a href="#关于Bert" class="headerlink" title="关于Bert"></a>关于Bert</h1><p><a href="https://www.jianshu.com/p/46cb208d45c3">彻底理解 Google BERT 模型</a></p><p>以下摘自论文【BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding】：</p><blockquote><p>BERT is designed to pre-train depp bidirectional represntations from <strong>unlabeled text</strong> by jointlly conditioning on both <strong>left and right context</strong> in all layers.</p><p>As a result, the pre-trained BERT model can be finetuned with just one <strong>additional output</strong> layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p></blockquote><blockquote><p>The masked language model <strong>randomly masks some of the tokens</strong> from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.</p></blockquote><blockquote><p>During fine-tuning, all parameters are fine-tuned. </p><p>[CLS] is a special symbol added in front of every input example, </p><p>[CLS] 一个全局的向量，放在第一个句子的首位，经过 BERT 得到的的表征向量 <strong>C</strong> 可以用于后续的分类任务。 [CLS] we must add this token to the start of each sentence, so BERT knows we’re doing classification。</p></blockquote><blockquote><p>[SEP] is a special separator token (e.g. separating questions/answers).</p><p>[SEP] 句子结尾的标记，用于分开两个输入句子，例如输入句子 A 和 B，要在句子 A，B 后面增加 [SEP] 标志。</p></blockquote><blockquote><p>[UNK] 标志指的是未知字符。</p><p>[UNK] BERT understands tokens that were in the training set. Everything else can be encoded using the <code>[UNK]</code> (unknown) token。</p></blockquote><blockquote><p>[MASK] 标志用于遮盖句子中的一些单词，将单词用 [MASK] 遮盖之后，再利用 BERT 输出的 [MASK] 向量预测单词是什么。</p></blockquote><p>摘自【<a href="https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=Tbodro8Fpmwr">sentiment-analysis-with-bert</a>】</p><blockquote><p>BERT (introduced in <a href="https://arxiv.org/abs/1810.04805">this paper</a>) stands for Bidirectional Encoder Representations from Transformers.</p><p>BERT（本文介绍）代表来自Transformers的双向编码器表示。</p></blockquote><blockquote><p>Bidirectional - to understand the text you’re looking you’ll have to look back (at the previous words) and forward (at the next words)</p><p>双向-要理解您要查找的文本，您必须向后（在前一个单词处）和向前（在后一个单词处）。</p></blockquote><blockquote><p>The Transformer reads entire sequences of tokens at once. In a sense, the model is non-directional, while LSTMs read sequentially (left-to-right or right-to-left). The attention mechanism allows for learning contextual relations between words.</p><p>Transformer一次读取整个令牌序列。从某种意义上说，该模型是无方向性的，而LSTM则按顺序读取（从左到右或从右到左）。注意机制允许学习单词之间的上下文关系。</p></blockquote><blockquote><p>BERT was trained by masking 15% of the tokens with the goal to guess them. An additional objective was to predict the next sentence.</p><p>BERT通过掩盖15％的tokens来进行训练，目的是猜测它们。另一个目标是预测下一个句子。</p></blockquote><blockquote><h4 id="Masked-Language-Modeling-Masked-LM"><a href="#Masked-Language-Modeling-Masked-LM" class="headerlink" title="Masked Language Modeling (Masked LM)"></a>Masked Language Modeling (Masked LM)</h4><p>The objective of this task is to guess the masked tokens. Let’s look at an example, and try to not make it harder than it has to be:</p><p>这个任务的目的是猜测被屏蔽的Tokens。让我们看一个例子，尽量不要让它变得比原来更难：</p><p>That’s <code>[mask]</code> she <code>[mask]</code> -&gt; That’s what she said</p></blockquote><blockquote><h4 id="Next-Sentence-Prediction-NSP"><a href="#Next-Sentence-Prediction-NSP" class="headerlink" title="Next Sentence Prediction (NSP)"></a>Next Sentence Prediction (NSP)</h4><p>Given a pair of two sentences, the task is to say whether or not the second follows the first (binary classification). Let’s continue with the example:</p><p><strong><em>Input</em> = <code>[CLS]</code> That’s <code>[mask]</code> she <code>[mask]</code>. [SEP] Hahaha, nice! [SEP]</strong></p><p><em><strong>Label* = *IsNext</strong></em></p><p><strong><em>Input</em> = <code>[CLS]</code> That’s <code>[mask]</code> she <code>[mask]</code>. [SEP] Dwight, you ignorant <code>[mask]</code>! [SEP]</strong></p><p><em><strong>Label* = *NotNext</strong></em></p><p>The training corpus was comprised of two entries: <a href="https://arxiv.org/abs/1506.06724">Toronto Book Corpus</a> (800M words) and English Wikipedia (2,500M words).</p><p>While the original Transformer has an encoder (for reading the input) and a decoder (that makes the prediction), BERT uses only the decoder.原始的Transformer具有一个编码器（用于读取输入）和一个解码器（用于进行预测），而BERT仅使用解码器。</p><p>BERT is simply a pre-trained stack of Transformer Encoders. How many Encoders? We have two versions - with 12 (BERT base) and 24 (BERT Large).</p></blockquote><blockquote><h4 id="Is-This-Thing-Useful-in-Practice"><a href="#Is-This-Thing-Useful-in-Practice" class="headerlink" title="Is This Thing Useful in Practice?"></a>Is This Thing Useful in Practice?</h4><p>The BERT paper was released along with <a href="https://github.com/google-research/bert">the source code</a> and pre-trained models.</p><p>The best part is that you can do Transfer Learning (thanks to the ideas from OpenAI Transformer) with BERT for many NLP tasks - Classification, Question Answering, Entity Recognition, etc. You can train with small amounts of data and achieve great performance!</p></blockquote><p><strong>发表BERT的动机（Motivation）：</strong></p><p>1、大量数据，没有标签</p><p>2、当前word embedding 模型能力不够强大，有较多的局限性</p><p>3、没有考虑上下文信息，word2vec没有根据上文来生成向量</p><p>4、问答中长文本的依赖，传统的RNN会导致梯度的消失（LSTM可以缓解，但也会发生梯度消失）</p><p>5、迁移学习能否进一步推广应用，做fine turn的工作</p><p>Basic idea</p><p>随机masks words in sentence and predict them</p><p>Transformer architecture</p><p>Next sentence prediction</p><p>严格来说，Bert是一种训练策略，不是新的架构设计。</p><p><strong>BERT-DATA</strong></p><p>WordPieces instead of words, （eg.playing-&gt; play+ ing）它的实现方式是是一种叫BPE（Byte-Pair Encoding）的双字节解码。</p><p>1、减少词汇的数量</p><p>2、增加每一个单词的样本数量</p><p>3、避免OOV</p><p>WordPieces带来的问题：</p><p>probability—pro+ ##bali + ##lity </p><p>如果bali被masked，则很容易预测到probability这个单词，在中文中亦是如此。</p><p>解决方案：Whole Word Masking</p><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Dingtalk_20210202211237.jpg"></p><blockquote><p>补充点：</p><p>1、关于whole word piece（WWP），它是对BERT的一个很有效的改进</p><p><a href="http://fancyerii.github.io/2019/08/02/bert-pretrain-imp/">对BERT的pretraining改进的几篇文章</a></p><p><a href="https://www.ctolib.com/ymcui-Chinese-BERT-wwm.html">中文全词覆盖（Whole Word Masking）BERT的预训练模型</a></p><p>2、BPE</p><p>现在基本性能好一些的NLP模型，例如OpenAI GPT，google的BERT，在数据预处理的时候都会有WordPiece的过程。WordPiece字面理解是把word拆成piece一片一片，其实就是这个意思。</p><p>WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。</p><p>BPE的过程可以理解为把一个单词再拆分，使得我们的此表会变得精简，并且寓意更加清晰。</p><p>比如”loved”,”loving”,”loves”这三个单词。其实本身的语义都是“爱”的意思，但是如果我们以单词为单位，那它们就算不一样的词，在英语中不同后缀的词非常的多，就会使得词表变的很大，训练速度变慢，训练的效果也不是太好。</p><p>BPE算法通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。</p><p><a href="https://www.cnblogs.com/huangyc/p/10223075.html">一文读懂BERT中的WordPiece</a></p><p><a href="https://blog.csdn.net/az9996/article/details/108858708">BERT 中的tokenizer和wordpiece和bpe（byte pair encoding）分词算法</a></p><p>3、Transformer-XL</p><p>Transformer-XL的依存关系比RNN长80％，比原始Transformer长450％，在短序列和长序列上均具有更好的性能，并且在评估期间比原始Transformer快1800倍以上。</p><p><a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p><p><a href="https://www.cnblogs.com/jiangxinyang/p/11534492.html">NLP中的预训练语言模型（三）—— XL-Net和Transformer-XL</a></p><p><a href="https://zhuanlan.zhihu.com/p/70745925">论文笔记 —— Transformer-XL</a></p></blockquote><h1 id="关于Attention"><a href="#关于Attention" class="headerlink" title="关于Attention"></a>关于Attention</h1><p>Attention的思想如同它的名字一样，就是“注意力”，<strong>在预测结果时把注意力放在不同的特征上</strong>，核心逻辑就是<strong>从【关注全部】 到 【关注重点】</strong>。</p><p>在视觉图像、文本任务中， Attention机制是<strong>将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。</strong></p><p>优点：</p><p><strong>参数少：</strong>模型复杂度跟 <a href="https://easyai.tech/ai-definition/cnn/">CNN</a>、<a href="https://easyai.tech/ai-definition/rnn/">RNN</a> 相比，复杂度更小，参数也更少。所以对算力的要求也就更小。</p><p><strong>速度快：</strong>Attention 解决了 RNN 不能并行计算的问题。Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。</p><p><strong>效果好：</strong>在 Attention 机制引入之前，有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的。</p><p>Attention 是挑重点，就算文本比较长，也能从中间抓住重点，不丢失重要的信息。</p><p><strong>Attention原理</strong></p><p>attention 引入 Encoder-Decoder 框架下，完成机器翻译任务的大致流程。</p><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/attention-encoderdecoder.gif"></p><p><strong>Attention基本原理的基本步骤:</strong></p><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Attention.png"></p><p>第一步： query 和 key 进行相似度计算，得到权值</p><p>第二步：将权值进行归一化，得到直接可用的权重</p><p>第三步：将权重和 value 进行加权求和</p><p>Attenttion 类型：</p><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Attentiontypes.png"></p><p><a href="https://easyai.tech/ai-definition/attention/">EasyAI-Attention 机制</a></p><p><a href="https://zhuanlan.zhihu.com/p/43493999">NLP中的Attention原理和源码解析</a></p><p><a href="https://www.cnblogs.com/guoyaohua/p/9429924.html">Attention Model（注意力模型）学习总结</a></p><h1 id="关于Transformer"><a href="#关于Transformer" class="headerlink" title="关于Transformer"></a>关于Transformer</h1><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/transformer.png"></p><p><strong>参考老师PPT32-45页</strong></p><p>参考：</p><p>1、<a href="https://zhuanlan.zhihu.com/p/48508221">论文详解Transformer （Attention Is All You Need）</a></p><p>2、<a href="https://zhuanlan.zhihu.com/p/345680792">Transformer 一篇就够了（一）： Self-attenstion</a></p><p>3、<a href="https://zhuanlan.zhihu.com/p/347492368">Transformer 一篇就够了（二）： Transformer中的Self-attenstion</a></p><p>4、技术细节 <a href="https://zhuanlan.zhihu.com/p/54530247">模型优化之Layer Normalization</a></p><p>5、<a href="https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d">Multi-label Text Classification using BERT – The Mighty Transformer</a></p><h1 id="NLU-Slot-Extraction"><a href="#NLU-Slot-Extraction" class="headerlink" title="NLU: Slot Extraction"></a>NLU: Slot Extraction</h1><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/sequenceLabeling.png"></p><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/slot.png"></p><h1 id="NLU-Joint-Intent-Slot-extraction"><a href="#NLU-Joint-Intent-Slot-extraction" class="headerlink" title="NLU: Joint Intent-Slot extraction"></a>NLU: Joint Intent-Slot extraction</h1><h4 id="Attention-Based-Recurrent-Neural-Network-Models-for-Joint-Intent-Detection-and-Slot-Filling（2016）"><a href="#Attention-Based-Recurrent-Neural-Network-Models-for-Joint-Intent-Detection-and-Slot-Filling（2016）" class="headerlink" title="Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling（2016）"></a>Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling（2016）</h4><p>基于注意力的编码器-解码器神经网络模型最近在机器翻译和语音识别中显示出令人鼓舞的结果。在这项工作中，我们提出了一种基于注意力的神经网络模型，用于联合意图检测和slot filling，这对于许多语音理解和对话系统都是至关重要的步骤。与机器翻译和语音识别不同，对齐在slot filling中是显式的。我们探索将对齐信息整合到编码器-解码器框架中的不同策略。从编码器-解码器模型中的注意力机制中学习，我们进一步建议将注意力引入基于对齐的RNN模型。这种关注为意图分类和slot filling预测提供了更多信息。我们的独立任务模型在ATIS任务上实现了最优的意图检测错误率和slot fillingF1分数。与独立任务模型相比，我们的联合训练模型在意图检测上进一步获得了0.56％的绝对误差（相对值23.8％的相对误差），在slot filling上获得了0.23％的绝对增益。</p><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Encode-decoder.png"></p><p>参考：<a href="https://blog.csdn.net/shengyan5515/article/details/105691543">Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling论文笔记</a></p><h4 id="Slot-Gated-Modeling-for-Joint-Slot-Filling-and-Intent-Prediction（2018）"><a href="#Slot-Gated-Modeling-for-Joint-Slot-Filling-and-Intent-Prediction（2018）" class="headerlink" title="Slot-Gated Modeling for Joint Slot Filling and Intent Prediction（2018）"></a>Slot-Gated Modeling for Joint Slot Filling and Intent Prediction（2018）</h4><p>基于注意力的递归神经网络模型用于联合意图检测和插槽填充，具有最先进的性能，同时具有独立的注意力权重。考虑到插槽和意图之间存在很强的关系，本文提出了一种插槽门，其重点是学习意图和插槽注意向量之间的关系，以便通过全局优化获得更好的语义框架结果。实验表明，与基准ATIS和Snips数据集上的注意力模型相比，我们提出的模型显著提高了句子级语义框架的准确率，相对注意度模型分别提高了4.2％和1.9％。</p><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Slot-gated.png"></p><p>笔记：<a href="https://blog.csdn.net/shengyan5515/article/details/105776303">Slot-Gated Modeling for Joint Slot Filling and Intent Prediction论文笔记</a></p><h4 id="BERT-for-Joint-Intent-Classification-and-Slot-Filling（2019）"><a href="#BERT-for-Joint-Intent-Classification-and-Slot-Filling（2019）" class="headerlink" title="BERT for Joint Intent Classification and Slot Filling（2019）"></a>BERT for Joint Intent Classification and Slot Filling（2019）</h4><p>意图分类和slot filling是自然语言理解的两个基本任务。它们经常遭受小规模的人工标签训练数据的困扰，导致泛化能力差，尤其是对于稀有单词。最近，一种新的语言表示模型BERT（来自Transformers的双向编码器表示）有助于在大型未标记的语料库上进行预训练深层的双向表征，并在处理完各种自然语言处理任务后创建了最新的模型简单的微调。但是，在探索BERT以获得自然语言理解方面并没有付出很多努力。在这项工作中，我们提出了一个基于BERT的联合意图分类和slot filling模型。实验结果表明，与基于注意力的递归神经网络模型和slot-gated模型相比，我们提出的模型在多个公共基准数据集上的意图分类准确性，slot filling F1和句子级语义准确性都有了显着提高。</p><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Bert.png"></p><p>笔记：<a href="https://blog.csdn.net/shengyan5515/article/details/105678169">BERT for Joint Intent Classification and Slot Filling论文笔记</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 任务型对话 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 任务型对话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week3-RASA源码和定制化你的对话机器人</title>
      <link href="2021/01/11/Week3-RASA%E6%BA%90%E7%A0%81%E5%92%8C%E5%AE%9A%E5%88%B6%E5%8C%96%E4%BD%A0%E7%9A%84%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
      <url>2021/01/11/Week3-RASA%E6%BA%90%E7%A0%81%E5%92%8C%E5%AE%9A%E5%88%B6%E5%8C%96%E4%BD%A0%E7%9A%84%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Setting-up-RASA-Source-Code"><a href="#一、Setting-up-RASA-Source-Code" class="headerlink" title="一、Setting up RASA Source Code"></a>一、Setting up RASA Source Code</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Step-1、Python Environment Setup</span></span></span><br><span class="line">Python环境配置、创建新的虚拟环境以及pip的安装方法，详见Week2笔记</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Step-2、Building from Source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 这里介绍源码的安装方法</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Step-3、安装poetry</span></span></span><br><span class="line">curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python</span><br><span class="line"><span class="meta">#</span><span class="bash"> 关于poetry，截至目前pypi并未托管potery的whl文件，如果上述指令执行缓慢，或者由于网络原因不能下载；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可下载poetry.py和poetry源码，执行以下命令行安装；</span></span><br><span class="line">python get-poetry.py --file poetry-1.1.4-win32.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Step-4、下载rasa源码</span></span></span><br><span class="line">git clone https://github.com/RasaHQ/rasa.git</span><br><span class="line">cd rasa</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Step-5、安装</span></span></span><br><span class="line">poetry install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 总结下安装过程：</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## poetry install对网络的要求很高，如果中途网络断开，再次执行poetry install会直接报错，这里尚不清楚为何重复执行后不会像pip那样重新请求网络资源安装，而是会直接出错。迫不得已，手动pip安装了poetry.lock文件要求的包版本，再执行poetry install至成功安装，非常耗时，前前后后折腾约2小时。</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## STEP-6:特别地，若有需要安装RASA的附加依赖，如：</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Dependencies <span class="keyword">for</span> spaCy</span></span><br><span class="line">pip3 install rasa[spacy]</span><br><span class="line">python3 -m spacy download en_core_web_md</span><br><span class="line">python3 -m spacy link en_core_web_md en</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Dependencies <span class="keyword">for</span> MITIE</span></span><br><span class="line">pip3 install git+https://github.com/mit-nlp/MITIE.git</span><br><span class="line">pip3 install rasa[mitie]    # pip list可以查看到已安装好了mitie包</span><br></pre></td></tr></table></figure><h3 id="二、Optimize-NLU"><a href="#二、Optimize-NLU" class="headerlink" title="二、Optimize NLU"></a>二、Optimize NLU</h3><h4 id="2-0-如何增强RASA-NLU-Lifestyle"><a href="#2-0-如何增强RASA-NLU-Lifestyle" class="headerlink" title="2.0 如何增强RASA NLU - Lifestyle"></a>2.0 如何增强RASA NLU - Lifestyle</h4><p><img src="/2021/01/11/Week3-RASA%E6%BA%90%E7%A0%81%E5%92%8C%E5%AE%9A%E5%88%B6%E5%8C%96%E4%BD%A0%E7%9A%84%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA/component-lifecycle-img.png"></p><p>Create: 在训练之前初始化Component</p><p>Train：当前Component使用上下文和先前Component的输出进行训练</p><p>Persist：持久化，将训练好的component保存在磁盘</p><h4 id="2-1、Improve-NLU"><a href="#2-1、Improve-NLU" class="headerlink" title="2.1、Improve NLU"></a>2.1、Improve NLU</h4><h5 id="2-1-0、RASA-NLU-模块的作用"><a href="#2-1-0、RASA-NLU-模块的作用" class="headerlink" title="2.1.0、RASA NLU 模块的作用"></a>2.1.0、RASA NLU 模块的作用</h5><p>分词、训练词向量、提取特征、命名体识别、意图识别等，这些功能都是通过不同的components实现的，然后通过pipeline将这些components组装在一起，得到NLU过程输出的结果：用户的intent和entity。</p><h5 id="2-1-1、Components组件"><a href="#2-1-1、Components组件" class="headerlink" title="2.1.1、Components组件"></a>2.1.1、Components组件</h5><p>关于Component：</p><p>在RASA源码NLU模块中（./rasa-master/rasa/nlu），其代码结构：</p><p><img src="/2021/01/11/Week3-RASA%E6%BA%90%E7%A0%81%E5%92%8C%E5%AE%9A%E5%88%B6%E5%8C%96%E4%BD%A0%E7%9A%84%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA/NLUModule.png"></p><p>其中，在components.py文件中定义了component类，每一个component都是pipeline中处理数据的一个单元，按照在pipeline中的顺序执行。在上图中包含classifiers\emulators\ extractors\featurizers\ selectors\tokenizers文件，每一个文件下面都是可选择的方法，每个方法都会继承component类，比如extractors有如下几种组件：</p><p><img src="/2021/01/11/Week3-RASA%E6%BA%90%E7%A0%81%E5%92%8C%E5%AE%9A%E5%88%B6%E5%8C%96%E4%BD%A0%E7%9A%84%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA/extractors.png"></p><p>上图中<a href="https://links.jianshu.com/go?to=http://extractor.py">crf_entity_extractor.py</a>就是一个基于条件随机场提取实体的组件，RASA提供了很多种方法。当我们不满足RASA自带的这些组件时，那么就可以自定义一个组件，同样需要继承<a href="https://links.jianshu.com/go?to=http://components.py">components.py</a>中的component类。在component类中定义了一系列实例化组件的方法，主要的方法有训练 train() 、解析 process() 、持久化 persist() 、加载 load()。</p><p>假如我们想生成一个新的组件，可以按照以下例子书写：</p><p><a href="https://blog.csdn.net/lly1122334/article/details/106119731"><strong>Rasa自定义NLU组件</strong></a></p><p><a href="https://blog.rasa.com/enhancing-rasa-nlu-with-custom-components/"><strong>Enhancing Rasa NLU models with Custom Components</strong></a></p><h5 id="2-1-2-Pipeline-Approach"><a href="#2-1-2-Pipeline-Approach" class="headerlink" title="2.1.2 Pipeline Approach"></a>2.1.2 Pipeline Approach</h5><p>【参考老师PPT课件P8-P13】补充</p><blockquote><h4 id="词向量、语言模型-Language-Models"><a href="#词向量、语言模型-Language-Models" class="headerlink" title="词向量、语言模型(Language Models)"></a>词向量、语言模型(<a href="https://rasa.com/docs/rasa/components#language-models">Language Models</a>)</h4><ol><li><p>MitieNLP：<a href="https://github.com/mit-nlp/MITIE">MIT Information Extraction</a></p><p>是MITIE initializer的简称，作用是初始化Mitie，每个mitie组件都依赖于此，因此应该将其放在任何使用mitie组件的每个管道的开头。如果用mitie的wordrep（作用类似word2vec）训练词向量需要很长时间，所以会事先下载好基于维基百科训练好的词向量文件，将路径赋给参数model。除此之外还有SpacyNLP和HFTransformersNLP。</p></li></ol><ol start="2"><li><p>SpacyNLP</p></li><li><p>HFTransformersNLP</p></li></ol><h4 id="分词器-Tokenizers"><a href="#分词器-Tokenizers" class="headerlink" title="分词器(Tokenizers)"></a>分词器(<a href="https://rasa.com/docs/rasa/components#tokenizers">Tokenizers</a>)</h4><ol><li><p>WhitespaceTokenizer：空格分词器</p></li><li><p>JiebaTokenizer：结巴分词器</p><p>用jieba进行中文的tokenize，将单词转化成id，同时需要传入自定义词典。除此之外还有MitieTokenizer、SpacyTokenizer等等。</p></li><li><p>MitieTokenizer：MITIE分词器</p></li><li><p>SpacyTokenizer：spaCy分词器</p></li><li><p>ConveRTTokenizer：ConveRT分词器</p></li><li><p>LanguageModelTokenizer</p></li></ol><h4 id="特征提取器-Featurizers"><a href="#特征提取器-Featurizers" class="headerlink" title="特征提取器(Featurizers)"></a>特征提取器(<a href="https://rasa.com/docs/rasa/components#featurizers">Featurizers</a>)</h4><ol><li><p>MitieFeaturizer：MITIE特征提取器。使用MITIE featurizer为意图分类创建特性。</p></li><li><p>SpacyFeaturizer：spaCy特征提取器</p></li><li><p>ConveRTFeaturizer：ConveRT特征提取器</p></li><li><p>LanguageModelFeaturizer</p></li><li><p>RegexFeaturizer：正则表达式特征提取器。为实体提取和意图分类创建特性。在训练期间，regex intent featurizer 以训练数据的格式创建一系列正则表达式列表。对于每个正则，都将设置一个特征，标记是否在输入中找到该表达式，然后将其输入到intent classifier / entity extractor 中以简化分类(假设分类器在训练阶段已经学习了该特征集合，该特征集合表示一定的意图)。将Regex特征用于实体提取目前仅CRFEntityExtractor组件支持。</p></li><li><p>CountVectorsFeaturizer：词袋模型特征提取器，结合用户消息、意图和响应</p></li><li><p>LexicalSyntacticFeaturizer：词法语法特征提取器</p></li></ol><h4 id="意图分类器-Intent-Classifiers"><a href="#意图分类器-Intent-Classifiers" class="headerlink" title="意图分类器(Intent Classifiers)"></a>意图分类器(<a href="https://rasa.com/docs/rasa/components#intent-classifiers">Intent Classifiers</a>)</h4><ol><li><p>MitieIntentClassifier: MITIE意图分类器。MitieIntentClassifier分类器里面已经自带Featurizer功能，所以不是必须配置的。简单来说，是基于稀疏线性核的一个多分类线性SVM。具体信息见：<a href="https://links.jianshu.com/go?to=https://github.com/mit-nlp/MITIE">https://github.com/mit-nlp/MITIE</a>。</p></li><li><p>SklearnIntentClassifier: Sklearn意图分类器。Sklearn意图分类器训练一个线性支持向量机，该支持向量机通过网格搜索得到优化。并且将每个类别的概率排名，不管是否有类别超过预设的概率阀值。SklearnIntentClassifier使用时候需要将SVM的超参数配置上。具体配置如下：</p></li></ol><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment"># Specifies the list of regularization values to</span></span><br><span class="line"><span class="comment"># cross-validate over for C-SVM.</span></span><br><span class="line">   <span class="comment"># This is used with the ``kernel`` hyperparameter in GridSearchCV.</span></span><br><span class="line"><span class="attr">C:</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">100</span>]</span><br><span class="line">   <span class="comment"># Specifies the kernel to use with C-SVM.</span></span><br><span class="line"><span class="comment"># This is used with the ``C`` hyperparameter in GridSearchCV.</span></span><br><span class="line">   <span class="attr">kernels:</span> [<span class="string">&quot;linear&quot;</span>]</span><br><span class="line"><span class="comment"># Gamma parameter of the C-SVM.</span></span><br><span class="line">   <span class="attr">&quot;gamma&quot;:</span> [<span class="number">0.1</span>]</span><br><span class="line"><span class="comment"># We try to find a good number of cross folds to use during</span></span><br><span class="line">   <span class="comment"># intent training, this specifies the max number of folds.</span></span><br><span class="line"><span class="attr">&quot;max_cross_validation_folds&quot;:</span> <span class="number">5</span></span><br><span class="line">   <span class="comment"># Scoring function used for evaluating the hyper parameters.</span></span><br><span class="line">   <span class="comment"># This can be a name or a function.</span></span><br><span class="line">   <span class="attr">&quot;scoring_function&quot;:</span> <span class="string">&quot;f1_weighted&quot;</span></span><br></pre></td></tr></table></figure><ol start="3"><li><p>EmbeddingIntentClassifier：嵌入意图分类器（即将被DIETClassifier替代）</p></li><li><p>FallbackClassifier：当意图识别的得分比较低时，使用该分类器决定是否给出nlu_fallback意图。注意，这个FallbackClassifier总是跟在其他意图分类器之后，对前一个意图分类提给出的意图及置信度进行判定。如果前一个意图分类器给出的意图预测置信度低于threshold，或者两个排名最高的意图的置信度得分接近时，<code>FallbackClassifier</code>实施回退操作。回退意图的应答，可以通过规则来实现。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rules:</span><br><span class="line">- rule: Ask the user to rephrase in case of low NLU confidence</span><br><span class="line">  steps:</span><br><span class="line">  - intent: nlu_fallback</span><br><span class="line">  - action: utter_please_rephrase</span><br></pre></td></tr></table></figure><p>FallbackClassifier的配置参数有：</p><p><strong>threshold</strong>：此参数设置预测nlu_fallback意图的阈值。如果前一个意图分类器预测的意图置信度小于threshold，则FallbackClassifier将返回一个置信度为1.0的nlu_fallback意图。</p><p><strong>ambiguity_threshold</strong>：如果两个排名最高的意图的置信度得分之差小于ambiguity_threshold，FallbackClassifier将返回一个置信度为1.0的nlu_fallback意图。</p></li><li><p>KeywordIntentClassifier：关键词意图分类器，适合小项目。</p><p>简单的关键字匹配意图分类，适用于小型项目，意图比较少的情况。当意图很多，相关性又很大的时候，关键词分类器无法区分。关键字的匹配方式是，训练数据的整句话都作为关键字，去搜索用户说的话。因此写配置数据的时候，仔细设计那个训练数据很重要，关键字不能太长，这容易匹配不上意图，也不能太短，缺少意图的区分度。</p></li><li><p><strong>DIETClassifier（Dual intent and Entity Transformer）：意图分类和实体提取的双向转换器（支持中文）</strong></p><p>DIET模型解决了对话理解问题中的2个问题，意图分类和实体识别。</p><p>DIET使用的是纯监督的方式，没有任何预训练的情况下，无须大规模预训练是关键，性能好于fine-tuning Bert, 但是训练速度是bert的6倍。输入是用户消息和可选意图的稠密或者稀疏向量。输出是实体，意图和评分。</p><p>DIET体系结构基于两个任务共享的Transformer。实体标签序列通过Transformer后，输出序列进入顶层条件随机场（CRF）标记层预测，输出每个Token成为BIOE的概率。完整话语和意图标签经过Transformer输出到单个语义向量空间中。利用点积损失最大化与目标标签的相似度，最小化与负样本的相似度。</p><p>具体DIET的算法参考：<a href="https://zhuanlan.zhihu.com/p/337181983">DIET: Dual Intent and Entity Transformer-RASA论文翻译</a></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果只想将DIETClassifier用于意图分类，请将entity_recognition设置为False。</span></span><br><span class="line"><span class="comment"># 如果只想进行实体识别，请将intent_classification设置为False。</span></span><br><span class="line"><span class="comment"># 默认情况下，DIETClassifier同时执行这两项操作，即实体识别和意图分类都设置为True。</span></span><br></pre></td></tr></table></figure><p>可以定义多个超参数来调整模型。如果要调整模型，请首先修改以下参数：</p><p>epochs：此参数设置算法将看到训练数据的次数（默认值：300）。一个epoch等于所有训练实例的一个向前传播和一个向后传播。有时模型需要更多的epoch来正确学习。epoch数越少，模型的训练速度就越快。</p><p>hidden_layers_sizes：此参数允许您为用户消息和意图定义前馈层的数量及其输出维度（默认值：文本：[]，标签：[]）。列表中的每个条目都对应一个前馈层。例如，如果设置text:[256，128]，我们将在转换器前面添加两个前馈层。输入token的向量（来自用户消息）将被传递到这些层。第一层的输出维度为256，第二层的输出维度为128。如果使用空列表（默认行为），则不会添加前馈层。确保只使用正整数值。通常使用二次幂的数字，第二个值小于或等于前一个值。</p><p>embedding_dimension：该参数定义模型内部使用的嵌入层的输出维度（默认值：20）。我们在模型架构中使用了多个嵌入层。例如，在比较和计算损失之前，将完整的话语和意图的向量传递到嵌入层。</p><p>number_of_transformer_layers：此参数设置要使用的transformer层数（默认值：2）。transformer层的数量对应于要用于模型的transformer块。</p><p>transformer_size：此参数设置transformer中的单位数（默认值：256）。来自transformer的矢量将具有给定的transformer_size。</p><p>weight_sparsity：该参数定义模型中所有前馈层的内核权重的分数（默认值：0.8）。该值应介于0和1之间。如果将weight_sparsity设置为0，则不会将内核权重设置为0，该层将充当标准的前馈层。您不应该将weight_sparsity设置为1，因为这将导致所有内核权重为0，即模型无法学习。</p><p>一般来说，调整这些参数就可以获得比较好的模型。另外还有其他可以调整的参数，具体见下表。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">+---------------------------------+------------------+--------------------------------------------------------------+</span><br><span class="line">  | Parameter                       | Default Value    | Description                                                  |</span><br><span class="line">  +=================================+==================+==============================================================+</span><br><span class="line">  | hidden<span class="emphasis">_layers_</span>sizes             | text: []         | Hidden layer sizes for layers before the embedding layers    |</span><br><span class="line">   |                                 | label: []        | for user messages and labels. The number of hidden layers is |</span><br><span class="line"><span class="code">       |                                 |                  | equal to the length of the corresponding list.               |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">       | share_hidden_layers             | False            | Whether to share the hidden layer weights between user       |</span></span><br><span class="line"><span class="code">   |                                 |                  | messages and labels.                                         |</span></span><br><span class="line"><span class="code">       +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | transformer_size                | 256              | Number of units in transformer.                              |</span></span><br><span class="line"><span class="code">  +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | number_of_transformer_layers    | 2                | Number of transformer layers.                                |</span></span><br><span class="line"><span class="code">  +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | number_of_attention_heads       | 4                | Number of attention heads in transformer.                    |</span></span><br><span class="line"><span class="code">  +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | use_key_relative_attention      | False            | If &#x27;True&#x27; use key relative embeddings in attention.          |</span></span><br><span class="line"><span class="code">  +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | use_value_relative_attention    | False            | If &#x27;True&#x27; use value relative embeddings in attention.        |</span></span><br><span class="line"><span class="code">  +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | max_relative_position           | None             | Maximum position for relative embeddings.                    |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | unidirectional_encoder          | False            | Use a unidirectional or bidirectional encoder.               |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | batch_size                      | [64, 256]        | Initial and final value for batch sizes.                     |</span></span><br><span class="line"><span class="code">   |                                 |                  | Batch size will be linearly increased for each epoch.        |</span></span><br><span class="line"><span class="code">   |                                 |                  | If constant `batch_size` is required, pass an int, e.g. `8`. |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | batch_strategy                  | &quot;balanced&quot;       | Strategy used when creating batches.                         |</span></span><br><span class="line"><span class="code">   |                                 |                  | Can be either &#x27;sequence&#x27; or &#x27;balanced&#x27;.                      |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | epochs                          | 300              | Number of epochs to train.                                   |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | random_seed                     | None             | Set random seed to any &#x27;int&#x27; to get reproducible results.    |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | learning_rate                   | 0.001            | Initial learning rate for the optimizer.                     |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | embedding_dimension             | 20               | Dimension size of embedding vectors.                         |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | dense_dimension                 | text: 128        | Dense dimension for sparse features to use.                  |</span></span><br><span class="line"><span class="code">   |                                 | label: 20        |                                                              |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | concat_dimension                | text: 128        | Concat dimension for sequence and sentence features.         |</span></span><br><span class="line"><span class="code">   |                                 | label: 20        |                                                              |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | number_of_negative_examples     | 20               | The number of incorrect labels. The algorithm will minimize  |</span></span><br><span class="line"><span class="code">   |                                 |                  | their similarity to the user input during training.          |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | similarity_type                 | &quot;auto&quot;           | Type of similarity measure to use, either &#x27;auto&#x27; or &#x27;cosine&#x27; |</span></span><br><span class="line"><span class="code">   |                                 |                  | or &#x27;inner&#x27;.                                                  |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | loss_type                       | &quot;softmax&quot;        | The type of the loss function, either &#x27;softmax&#x27; or &#x27;margin&#x27;. |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | ranking_length                  | 10               | Number of top actions to normalize scores for loss type      |</span></span><br><span class="line"><span class="code">   |                                 |                  | &#x27;softmax&#x27;. Set to 0 to turn off normalization.               |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | maximum_positive_similarity     | 0.8              | Indicates how similar the algorithm should try to make       |</span></span><br><span class="line"><span class="code">   |                                 |                  | embedding vectors for correct labels.                        |</span></span><br><span class="line"><span class="code">   |                                 |                  | Should be 0.0 &lt; ... &lt; 1.0 for &#x27;cosine&#x27; similarity type.      |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | maximum_negative_similarity     | -0.4             | Maximum negative similarity for incorrect labels.            |</span></span><br><span class="line"><span class="code">   |                                 |                  | Should be -1.0 &lt; ... &lt; 1.0 for &#x27;cosine&#x27; similarity type.     |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | use_maximum_negative_similarity | True             | If &#x27;True&#x27; the algorithm only minimizes maximum similarity    |</span></span><br><span class="line"><span class="code">   |                                 |                  | over incorrect intent labels, used only if &#x27;loss_type&#x27; is    |</span></span><br><span class="line"><span class="code">   |                                 |                  | set to &#x27;margin&#x27;.                                             |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | scale_loss                      | False            | Scale loss inverse proportionally to confidence of correct   |</span></span><br><span class="line"><span class="code">   |                                 |                  | prediction.                                                  |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | regularization_constant         | 0.002            | The scale of regularization.                                 |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | negative_margin_scale           | 0.8              | The scale of how important it is to minimize the maximum     |</span></span><br><span class="line"><span class="code">   |                                 |                  | similarity between embeddings of different labels.           |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | weight_sparsity                 | 0.8              | Sparsity of the weights in dense layers.                     |</span></span><br><span class="line"><span class="code">   |                                 |                  | Value should be between 0 and 1.                             |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | drop_rate                       | 0.2              | Dropout rate for encoder. Value should be between 0 and 1.   |</span></span><br><span class="line"><span class="code">   |                                 |                  | The higher the value the higher the regularization effect.   |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | drop_rate_attention             | 0.0              | Dropout rate for attention. Value should be between 0 and 1. |</span></span><br><span class="line"><span class="code">   |                                 |                  | The higher the value the higher the regularization effect.   |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | use_sparse_input_dropout        | True             | If &#x27;True&#x27; apply dropout to sparse input tensors.             |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | use_dense_input_dropout         | True             | If &#x27;True&#x27; apply dropout to dense input tensors.              |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | evaluate_every_number_of_epochs | 20               | How often to calculate validation accuracy.                  |</span></span><br><span class="line"><span class="code">   |                                 |                  | Set to &#x27;-1&#x27; to evaluate just once at the end of training.    |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | evaluate_on_number_of_examples  | 0                | How many examples to use for hold out validation set.        |</span></span><br><span class="line"><span class="code">   |                                 |                  | Large values may hurt performance, e.g. model accuracy.      |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | intent_classification           | True             | If &#x27;True&#x27; intent classification is trained and intents are   |</span></span><br><span class="line"><span class="code">   |                                 |                  | predicted.                                                   |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | entity_recognition              | True             | If &#x27;True&#x27; entity recognition is trained and entities are     |</span></span><br><span class="line"><span class="code">   |                                 |                  | extracted.                                                   |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | use_masked_language_model       | False            | If &#x27;True&#x27; random tokens of the input message will be masked  |</span></span><br><span class="line"><span class="code">   |                                 |                  | and the model has to predict those tokens. It acts like a    |</span></span><br><span class="line"><span class="code">   |                                 |                  | regularizer and should help to learn a better contextual     |</span></span><br><span class="line"><span class="code">   |                                 |                  | representation of the input.                                 |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | tensorboard_log_directory       | None             | If you want to use tensorboard to visualize training         |</span></span><br><span class="line"><span class="code">   |                                 |                  | metrics, set this option to a valid output directory. You    |</span></span><br><span class="line"><span class="code">   |                                 |                  | can view the training metrics after training in tensorboard  |</span></span><br><span class="line"><span class="code">   |                                 |                  | via &#x27;tensorboard --logdir &lt;path-to-given-directory&gt;&#x27;.        |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | tensorboard_log_level           | &quot;epoch&quot;          | Define when training metrics for tensorboard should be       |</span></span><br><span class="line"><span class="code">   |                                 |                  | logged. Either after every epoch (&#x27;epoch&#x27;) or for every      |</span></span><br><span class="line"><span class="code">   |                                 |                  | training step (&#x27;minibatch&#x27;).                                 |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | featurizers                     | []               | List of featurizer names (alias names). Only features        |</span></span><br><span class="line"><span class="code">   |                                 |                  | coming from the listed names are used. If list is empty      |</span></span><br><span class="line"><span class="code">   |                                 |                  | all available features are used.                             |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | checkpoint_model                | False            | Save the best performing model during training. Models are   |</span></span><br><span class="line"><span class="code">   |                                 |                  | stored to the location specified by `--out`. Only the one    |</span></span><br><span class="line"><span class="code">   |                                 |                  | best model will be saved.                                    |</span></span><br><span class="line"><span class="code">   |                                 |                  | Requires `evaluate_on_number_of_examples &gt; 0` and            |</span></span><br><span class="line"><span class="code">   |                                 |                  | `evaluate_every_number_of_epochs &gt; 0`                        |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+--------------------------------------------------------------+</span></span><br><span class="line"><span class="code">   | split_entities_by_comma         | True             | Splits a list of extracted entities by comma to treat each   |</span></span><br><span class="line"><span class="code">   |                                 |                  | one of them as a single entity. Can either be `True`/`False` |</span></span><br><span class="line"><span class="code">   |                                 |                  | globally, or set per entity type, such as:                   |</span></span><br><span class="line"><span class="code">   |                                 |                  | ```                                                          |</span></span><br><span class="line"><span class="code">   |                                 |                  | ...                                                          |</span></span><br><span class="line"><span class="code">   |                                 |                  | - name: DIETClassifier                                       |</span></span><br><span class="line"><span class="code">   |                                 |                  |   split_entities_by_comma:                                   |</span></span><br><span class="line"><span class="code">   |                                 |                  |     address: True                                            |</span></span><br><span class="line"><span class="code">   |                                 |                  |     ...                                                      |</span></span><br><span class="line"><span class="code">   |                                 |                  | ...                                                          |</span></span><br><span class="line"><span class="code">   |                                 |                  | ```                                                          |</span></span><br><span class="line"><span class="code">   +---------------------------------+------------------+------------------------------------</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="实体提取器-Entity-Extractors"><a href="#实体提取器-Entity-Extractors" class="headerlink" title="实体提取器(Entity Extractors)"></a>实体提取器(<a href="https://rasa.com/docs/rasa/components#entity-extractors">Entity Extractors</a>)</h4><ol><li><p>MitieEntityExtractor： MITIE实体提取器</p><p>Mitie是一个训练词向量和提取实体的工具库，使用分布式单词嵌入和结构支持向量机。</p><p>具体信息见：<a href="https://links.jianshu.com/go?to=https://github.com/mit-nlp/MITIE">https://github.com/mit-nlp/MITIE</a>。</p><ol start="2"><li>SpacyEntityExtractor:  spaCy实体提取器</li></ol></li><li><p>EntitySynonymMapper：同义词匹配实体提取器。</p><p>作用是将同义词映射到同一个值。比如将United States of America 和USA都映射到usa。</p></li><li><p>CRFEntityExtractor：条件随机场实体提取器</p></li><li><p>DucklingHTTPExtractor：<a href="https://xercis.blog.csdn.net/article/details/106114511">常见实体提取器</a></p></li><li><p>DIETClassifier：意图分类和实体提取的双向转换器</p></li></ol><h4 id="选择器-Selectors"><a href="#选择器-Selectors" class="headerlink" title="选择器(Selectors)"></a>选择器(<a href="https://rasa.com/docs/rasa/components#selectors">Selectors</a>)</h4><ol><li>ResponseSelector：响应选择器</li></ol><h4 id="合并的实体提取器和意图分类器-Combined-Intent-Classifiers-and-Entity-Extractors"><a href="#合并的实体提取器和意图分类器-Combined-Intent-Classifiers-and-Entity-Extractors" class="headerlink" title="合并的实体提取器和意图分类器(Combined Intent Classifiers and Entity Extractors)"></a>合并的实体提取器和意图分类器(<a href="https://rasa.com/docs/rasa/components#combined-intent-classifiers-and-entity-extractors">Combined Intent Classifiers and Entity Extractors</a>)</h4><ol><li>DIETClassifier：意图分类和实体提取的双向转换器</li></ol></blockquote><h5 id="2-1-3-Intent-Recognition"><a href="#2-1-3-Intent-Recognition" class="headerlink" title="2.1.3 Intent Recognition"></a>2.1.3 Intent Recognition</h5><p>缺少训练数据</p><p>OOV，在测试集中找不到对应词</p><p>Similar Intents 近似意图合并</p><p>Skewed data 避免数据倾斜</p><h5 id="2-1-3-Custom-Components"><a href="#2-1-3-Custom-Components" class="headerlink" title="2.1.3 Custom Components"></a>2.1.3 Custom Components</h5><p>创建自定义的类</p><p>参考：</p><p><a href="https://rasa.com/docs/rasa/components#custom-components">https://rasa.com/docs/rasa/components#custom-components</a></p><p><a href="https://blog.rasa.com/enhancing-rasa-nlu-with-custom-components/">https://blog.rasa.com/enhancing-rasa-nlu-with-custom-components/</a></p><p><img src="/2021/01/11/Week3-RASA%E6%BA%90%E7%A0%81%E5%92%8C%E5%AE%9A%E5%88%B6%E5%8C%96%E4%BD%A0%E7%9A%84%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA/NLUCOMPONENT.png"></p><h4 id="2-2-Expand-your-NLU-Data"><a href="#2-2-Expand-your-NLU-Data" class="headerlink" title="2.2 Expand your NLU Data"></a>2.2 Expand your NLU Data</h4><ol><li>Share your Bot to More people</li><li>NLU Data Augmentation 数据增强</li><li>Reinforce Learning 强化学习</li></ol><h3 id="三、Optimize-Policy"><a href="#三、Optimize-Policy" class="headerlink" title="三、Optimize Policy"></a>三、Optimize Policy</h3><h4 id="3-1-About-Policy"><a href="#3-1-About-Policy" class="headerlink" title="3.1 About Policy"></a>3.1 About Policy</h4><p>policy的作用：</p><p>决定在对话的每个步骤中应该采取的操作，可以使用的策略包括但不限于机器学习和基于规则的策略。</p><p>可以在项目的config.yml中指定一个或者多个策略。</p><p>配置中定义的每个策略都会以一定的置信度（取最高）预测下一个动作。</p><p>默认情况下，在每条用户消息之后，最多可以预测10个下一个动作，可以环境变量MAX_NUMBER_OF_PREDICTIONS设置为所需的最大预测数。</p><h4 id="3-2-About-Policy-Priority"><a href="#3-2-About-Policy-Priority" class="headerlink" title="3.2 About Policy Priority"></a>3.2 About Policy Priority</h4><p>关于策略优先级：</p><p>Rasa具有默认优先级，这些默认优先级被设置为确保平局时的预期结果，数字越高优先级越高，如下：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">6</span> <span class="bullet">-</span> <span class="string">RulePolicy</span></span><br><span class="line"><span class="number">3</span> <span class="bullet">-</span> <span class="string">MemoizationPolicy</span> <span class="string">or</span> <span class="string">AugmentedMemoizationPolicy</span></span><br><span class="line"><span class="number">1</span> <span class="bullet">-</span> <span class="string">TEDPolicy</span></span><br></pre></td></tr></table></figure><p>一般地，在配置中的每个优先级不建议使用多个策略。</p><p>如果有2个策略具有相同的优先级，并且它们以相同的置信度进行预测，则将随机选择结果操作。</p><h4 id="3-3-Policies"><a href="#3-3-Policies" class="headerlink" title="3.3 Policies"></a>3.3 Policies</h4><p>有哪些对话<a href="https://rasa.com/docs/rasa/core/policies/">策略</a>：</p><p>一、机器学习策略：</p><p><strong>1、TED(Transformer Embedding Dialogue) Policy</strong></p><p>用于下一动作预测和实体识别的多任务架构策略（具体实施细节待进一步理解）。</p><p><strong>2、Memoization Policy</strong></p><p>记忆策略只记录训练数据中的对话</p><p>若训练数据存在这样的对话，则以置信度1.0预测下一个动作，否则以0.0预测</p><p>一般不单独使用</p><p><strong>3、Augmented Memoization Policy</strong></p><p>AugmentedMemoizationPolicy可以记住训练story中的示例，直到max_history。</p><p>具有遗忘机制，可以遗忘对话历史记录中的某些步骤，并尝试在历史减少的story中找到匹配项</p><p>二、Rule-based Policies</p><p><strong>1、Rule Policy</strong></p><p>一种处理遵循固定行为（例如，业务逻辑）的对话部分的策略。它根据您的训练数据中的任何规则进行预测。</p><p>三、Custom Policies</p><p>编写自定义测策略：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In the example below, the last two lines show how to use a custom policy class and pass arguments to it.</span></span><br><span class="line"><span class="attr">policies:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;TEDPolicy&quot;</span></span><br><span class="line">    <span class="attr">max_history:</span> <span class="number">5</span></span><br><span class="line">    <span class="attr">epochs:</span> <span class="number">200</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;RulePolicy&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;path.to.your.policy.class&quot;</span></span><br><span class="line">    <span class="attr">arg1:</span> <span class="string">&quot;...&quot;</span></span><br></pre></td></tr></table></figure><p><del>*四、其他已弃用的策略</del>*</p><p><em><del>1、 Mapping Policy</del></em></p><p><em><del>映射策略将意图映射为操作</del></em></p><p><em><del>无视之前对话，一旦触发意图就操作</del></em></p><p><em><del>映射是传递intent属性给<code>triggers</code>实现的，修改<code>domain.yml</code></del></em></p> <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">intents:</span><br><span class="line"><span class="bullet">   -</span> ask<span class="emphasis">_is_</span>bot:</span><br><span class="line"><span class="code">    triggers: action_is_bot</span></span><br><span class="line"><span class="code"> 123</span></span><br></pre></td></tr></table></figure><p><em><del>一般不单独使用。</del></em></p><p><em><del>2、 Form Policy</del></em></p><p><em><del>表单策略，收集指定信息，如性别年龄地址</del></em></p><p> <em><del>需要实现FormAction，在<code>domain.yml</code>中指定，在<code>stories.md</code>中使用</del></em></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forms:</span><br><span class="line"> - facility_form</span><br></pre></td></tr></table></figure><p><em><del>3、Fallback Policy</del></em></p><p><em><del>回退策略，聊天机器人不可避免需要的回退情况，例如用户问了让机器人理解不了的东西时需要回退</del></em></p><p><em><del>需要提供阈值</del></em></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">policies:</span><br><span class="line"><span class="bullet"> -</span> name: FallbackPolicy</span><br><span class="line"><span class="code">    nlu_threshold: 0.3</span></span><br><span class="line"><span class="code">   ambiguity_threshold: 0.1</span></span><br><span class="line"><span class="code">    core_threshold: 0.3</span></span><br><span class="line"><span class="code">    fallback_action_name: &#x27;action_default_fallback&#x27;</span></span><br><span class="line"><span class="code">123456</span></span><br></pre></td></tr></table></figure><p><em><del>4、Two-Stage Fallback Policy</del></em></p><p> <del>不直接回退而是让用户选，尝试消除用户输入的歧义，从而在多个阶段处理NLU可信度较低的问题。</del></p><h4 id="3-4-Configuring-Policies"><a href="#3-4-Configuring-Policies" class="headerlink" title="3.4 Configuring Policies"></a>3.4 Configuring Policies</h4><p>配置对话策略</p><p>Max History：是RASA中一个重要的超参数，用于控制模型查看多少对话历史记录，以决定下一步应采取的行动。</p><p>在config.yml文件中，Max History的默认值为“无”，这表示自会话重新启动以来的完整对话历史记录已记入该帐户。</p><blockquote><p>RulePolicy没有max history参数，它始终考虑所提供规则的完整长度。</p></blockquote><p>例子：</p><p>假设有一个out_of_scope意图，该意图描述了主题外的用户消息。如果机器人连续多次看到此意图，则可能要告诉用户可以提供哪些帮助。</p><p>因此您的story可能如下所示：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">stories:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">story:</span> <span class="string">utter</span> <span class="string">help</span> <span class="string">after</span> <span class="number">2</span> <span class="string">fallbacks</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">out_of_scope</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_default</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">out_of_scope</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_default</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">out_of_scope</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_help_message</span></span><br></pre></td></tr></table></figure><p>为了让模型学习此模式，max_history必须至少为4<strong>（不应是3吗？）</strong>。</p><p>如果增加max_history，则模型将变大，并且训练将花费更长的时间。</p><p>如果有一些将来可能会影响对话的信息，则应将其存储为一个槽位，槽信息始终可用于每个功能块。</p><h4 id="3-5-Data-Augmentation"><a href="#3-5-Data-Augmentation" class="headerlink" title="3.5 Data Augmentation"></a>3.5 Data Augmentation</h4><p>训练模型时，Rasa Open Source将通过随机组合story文件中的story来创建更长的story，例子如下：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">stories:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">story:</span> <span class="string">thank</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">thankyou</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_youarewelcome</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">story:</span> <span class="string">say</span> <span class="string">goodbye</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">goodbye</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_goodbye</span></span><br></pre></td></tr></table></figure><p>实际上想让policy在无关的对话历史记录时忽略它，并且无论以前发生了什么，都要以相同的动作做出响应。</p><p>可以使用–augmentation标志更改此行为，该标志使您可以设置expandation_factor。 growth_factor确定在训练期间对多少个增强story进行了二次采样。扩增后的story在训练之前进行了二次抽样，因为它们的数量很快就会变得非常大，并且您希望对其进行限制。样本story的数量是增强因子x10。默认情况下，扩充设置为20，最多可生成200个扩充story。</p><p>–augmentation 0禁用所有扩充行为。基于备忘的策略不受扩充的影响（与扩充因素无关），并且将自动忽略所有扩充的story。</p><h4 id="3-6-Featurizers"><a href="#3-6-Featurizers" class="headerlink" title="3.6 Featurizers"></a>3.6 Featurizers</h4><h5 id="3-6-1-State-Featurizers"><a href="#3-6-1-State-Featurizers" class="headerlink" title="3.6.1 State Featurizers"></a>3.6.1 State Featurizers</h5><h5 id="3-6-2-Tracker-Featurizers"><a href="#3-6-2-Tracker-Featurizers" class="headerlink" title="3.6.2 Tracker Featurizers"></a>3.6.2 Tracker Featurizers</h5><h3 id="四、Action-Server"><a href="#四、Action-Server" class="headerlink" title="四、Action Server"></a>四、Action Server</h3><h4 id="4-1-ACTION"><a href="#4-1-ACTION" class="headerlink" title="4.1 ACTION"></a>4.1 ACTION</h4><p>Action类是任何自定义操作的基类。要定义自定义的action，请创建Action类的子类并覆盖两个必需的方法，即name和run。</p><p>样例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># See this guide on how to implement these action:</span></span><br><span class="line"><span class="comment"># https://rasa.com/docs/rasa/custom-actions</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># This is a simple example for a custom action which utters &quot;Hello World!&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># from typing import Any, Text, Dict, List</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># from rasa_sdk import Action, Tracker</span></span><br><span class="line"><span class="comment"># from rasa_sdk.executor import CollectingDispatcher</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># class ActionHelloWorld(Action):</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def name(self) -&gt; Text:</span></span><br><span class="line"><span class="comment">#         return &quot;action_hello_world&quot;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     def run(self, dispatcher: CollectingDispatcher,</span></span><br><span class="line"><span class="comment">#             tracker: Tracker,</span></span><br><span class="line"><span class="comment">#             domain: Dict[Text, Any]) -&gt; List[Dict[Text, Any]]:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#         dispatcher.utter_message(text=&quot;Hello World!&quot;)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#         return []</span></span><br></pre></td></tr></table></figure><p>上述<code>def name(self) -&gt; Text:</code>是<strong>函数参数注解，用于提示该函数的输入参数和返回值的类型</strong>，方便阅读。</p><p>python解释器不会对这些注解添加任何的语义。它们不会被类型检查，运行时跟没有加注解之前的效果也没有任何差距</p><p>当action server收到运行动作的请求时，它将根据其name方法的返回值调用的action。</p><h4 id="4-2-Events"><a href="#4-2-Events" class="headerlink" title="4.2 Events"></a>4.2 Events</h4><p>Rasa中的会话表示为一系列事件。自定义动作可以通过在对动作服务器请求的响应中返回事件来影响对话的过程。</p><p>并非所有事件通常都是由自定义操作返回的，因为Rasa会自动跟踪它们（例如，用户消息）。如果其他事件是由自定义操作返回的，则只能对其进行跟踪。</p><h4 id="4-3-Knowledge-Base-Actions"><a href="#4-3-Knowledge-Base-Actions" class="headerlink" title="4.3 Knowledge Base Actions"></a>4.3 Knowledge Base Actions</h4><p><code>ActionQueryKnowledgeBase</code></p><h3 id="五、More-Concepts"><a href="#五、More-Concepts" class="headerlink" title="五、More Concepts"></a>五、More Concepts</h3><h4 id="5-1-Tracker-Store"><a href="#5-1-Tracker-Store" class="headerlink" title="5.1 Tracker Store"></a>5.1 Tracker Store</h4><p>对话存储在跟踪存储中， Rasa Open Source提供了针对不同存储类型的开箱即用的实现，或者可以创建自己的自定义存储类型。</p><ul><li><p>InMemoryTrackerStore (default）：默认跟踪器存储。如果未配置其他跟踪器存储，则使用它。它将对话历史记录存储在内存中。</p></li><li><p>SQLTrackerStore：用SQLTrackerStore将助手的对话历史记录存储在SQL数据库中- </p></li><li><p>RedisTrackerStore：使用RedisTrackerStore将助手的对话历史记录存储在Redis中。 Redis是一种快速的内存中键值存储，可以选择持久存储数据。</p></li><li><p>MongoTrackerStore：使用MongoTrackerStore将助手的对话历史记录存储在MongoDB中。 MongoDB是一个免费且开源的跨平台面向文档的NoSQL数据库</p></li><li><p>DynamoTrackerStore：使用DynamoTrackerStore将助手的对话历史记录存储在DynamoDB中。 DynamoDB是Amazon Web Services（AWS）提供的托管NoSQL数据库。</p></li><li><p>Custom Tracker Store：如果需要开箱即用的跟踪器存储，则可以实施自己的跟踪器存储。这是通过扩展基类TrackerStore来完成的。</p></li></ul><h4 id="5-2-Event-Broker-事件代理"><a href="#5-2-Event-Broker-事件代理" class="headerlink" title="5.2 Event Broker 事件代理"></a>5.2 Event Broker 事件代理</h4><p>使用事件代理，可以将正在运行的助手连接到其他服务，这些服务处理来自对话的数据。例如，您可以将实时助手连接到Rasa X，以查看和注释对话或将消息转发到外部分析服务。事件代理将消息发布到消息流服务（也称为消息代理），以将Rasa事件从Rasa服务器转发到其他服务。</p><h4 id="5-3-Model-Storage"><a href="#5-3-Model-Storage" class="headerlink" title="5.3 Model Storage"></a>5.3 Model Storage</h4><p>模型可以存储在不同的位置，有三种不同的方式加载训练好的模型：</p><p>1、从本地磁盘加载模型（请参阅从磁盘加载模型） </p><p>2、从自己的HTTP服务器获取模型（请参阅从服务器加载模型）</p><p>3、从像S3这样的云存储中获取模型（请参阅从云中加载模型）</p><h4 id="5-4-Lock-Store"><a href="#5-4-Lock-Store" class="headerlink" title="5.4 Lock Store"></a>5.4 Lock Store</h4><p>Rasa使用票证锁定机制来确保以正确的顺序处理给定对话ID的传入消息，并在主动处理消息时锁定对话。这意味着多个Rasa服务器可以作为复制服务并行运行，并且在发送给定对话ID的消息时，客户端不一定需要寻址同一节点。</p><ul><li>InMemoryLockStore是默认的锁存储。它在单个进程中维护会话锁定。</li><li>RedisLockStore使用Redis作为持久层来维护会话锁定。建议使用此锁存储来运行一组复制的Rasa服务器。</li></ul><h4 id="5-4-Importer"><a href="#5-4-Importer" class="headerlink" title="5.4 Importer"></a>5.4 Importer</h4><p>Rasa Open Source具有内置的逻辑来收集和加载以Rasa格式编写的训练数据，但是您也可以使用自定义训练数据导入器自定义如何导入训练数据。</p><ul><li><p>RasaFileImporter</p><p>默认情况下，Rasa使用导入程序RasaFileImporter。如果要单独使用它，则无需在配置文件中指定任何内容。如果要与其他导入程序一起使用，请将其添加到配置文件中</p></li><li><p>MultiProjectImporter</p><p>使用此导入器，可以通过组合多个可重复使用的Rasa项目来训练模型。例如，可能用一个项目处理闲聊，而用另一个项目问候您的用户</p></li><li><p>Writing a Custom Importer</p><p>如果要编写自定义导入器，则需要实现TrainingDataImporter的接口。</p></li></ul><h4 id="5-4-Dispatcher"><a href="#5-4-Dispatcher" class="headerlink" title="5.4 Dispatcher"></a>5.4 Dispatcher</h4><p>调度程序是CollectingDispatcher类的实例，用于生成响应以发送回用户。</p><p>CollectingDispatcher具有一种方法utter_message和一种属性（消息）。</p><p>在action的run方法中使用它来添加对返回到Rasa服务器的有效负载的响应。</p><p>Rasa服务器将依次为每个响应将BotUttered事件添加到跟踪器。因此，使用分派器添加的响应不应作为事件显式返回。</p><h3 id="六、Deployment"><a href="#六、Deployment" class="headerlink" title="六、Deployment"></a>六、Deployment</h3><h4 id="6-1-推荐的部署方法"><a href="#6-1-推荐的部署方法" class="headerlink" title="6.1 推荐的部署方法"></a>6.1 推荐的部署方法</h4><ul><li>快速安装一个服务：</li></ul><p>服务器快速安装脚本是部署Rasa X和您的助手的最简单方法。它会使用合理的默认值在您的机器上安装Kubernetes集群，使您可以通过一条命令启动并运行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -s get-rasa-x.rasa.com | sudo bash</span><br></pre></td></tr></table></figure><ul><li>Helm Chart</li></ul><p>对于将吸引大量用户流量的助手，通过我们的Helm图表设置Kubernetes或Openshift部署是最佳选择。Helm提供了可伸缩的体系结构，该体系结构也易于部署。</p><h4 id="6-2-其他可选方法"><a href="#6-2-其他可选方法" class="headerlink" title="6.2 其他可选方法"></a>6.2 其他可选方法</h4><ul><li>Docker Compose</li></ul><h4 id="6-3-部署Action-Server"><a href="#6-3-部署Action-Server" class="headerlink" title="6.3 部署Action Server"></a>6.3 部署Action Server</h4><ul><li><a href="https://rasa.com/docs/rasa/how-to-deploy#building-an-action-server-image">Building an Action Server Image</a></li><li><a href="https://rasa.com/docs/rasa/how-to-deploy#using-your-custom-action-server-image">Using your Custom Action Server Image</a></li></ul><h3 id="其他补充："><a href="#其他补充：" class="headerlink" title="其他补充："></a>其他补充：</h3><blockquote><h4 id="2-0-1、Training-Data-Format"><a href="#2-0-1、Training-Data-Format" class="headerlink" title="2.0.1、Training Data Format"></a>2.0.1、Training Data Format</h4><h5 id="2-1-1-training-data"><a href="#2-1-1-training-data" class="headerlink" title="2.1.1 [training data]"></a>2.1.1 [training data]</h5><p>Including NLU data, stories and rules.</p><p>You can split the training data over any number of YAML files, and each file can contain any combination of NLU data, stories, and rules.</p><p>你可以将训练数据划分为任意数量的YAML文件，并且每个文件可以包含NLU数据，stories和Rules的任意组合。训练数据的类型由数据最外层的key决定。</p><p>a short example which keeps all training data in a single file:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&gt;version:</span> <span class="string">&quot;2.0&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">nlu:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">greet</span></span><br><span class="line">  <span class="attr">examples:</span> <span class="string">|</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Hey</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Hi</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">hey</span> <span class="string">there</span> [<span class="string">Sara</span>]<span class="string">(name)</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">faq/language</span></span><br><span class="line">  <span class="attr">examples:</span> <span class="string">|</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">What</span> <span class="string">language</span> <span class="string">do</span> <span class="string">you</span> <span class="string">speak?</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Do</span> <span class="string">you</span> <span class="string">only</span> <span class="string">handle</span> <span class="string">english?</span></span><br><span class="line"></span><br><span class="line"><span class="attr">stories:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">story:</span> <span class="string">greet</span> <span class="string">and</span> <span class="string">faq</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">greet</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_greet</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">faq</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_faq</span></span><br><span class="line"></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">rule:</span> <span class="string">Greet</span> <span class="string">user</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">greet</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_greet</span></span><br></pre></td></tr></table></figure><h5 id="2-1-2-Domain"><a href="#2-1-2-Domain" class="headerlink" title="2.1.2 [Domain]"></a>2.1.2 [Domain]</h5><p>结论：**<a href="https://rasa.com/docs/rasa/domain">Domain文件</a>**定义了意图，实体，位置，响应，形式和动作。</p><p>The domain defines the universe(宇宙，全局，世界) in which your assistant operates. It specifies(指定，列举) the intents, entities, slots, responses, forms, and actions your bot should know about. It also defines a configuration for conversation sessions.</p><p>The <a href="https://rasa.com/docs/rasa/glossary#domain">domain</a> uses the same YAML format as the training data and can also be split across multiple files or combined in one file. The domain includes the definitions for <a href="https://rasa.com/docs/rasa/responses">responses</a> and <a href="https://rasa.com/docs/rasa/forms">forms</a>. </p><p>a full example of a domain, taken from the <a href="https://github.com/RasaHQ/rasa/tree/master/examples/concertbot">concertbot</a> example:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&gt;version:</span> <span class="string">&quot;2.0&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&gt;intents:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">affirm</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">deny</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">greet</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">thankyou</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">goodbye</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">search_concerts</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">search_venues</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">compare_reviews</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">bot_challenge</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">nlu_fallback</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">how_to_get_started</span></span><br><span class="line"></span><br><span class="line"><span class="attr">entities:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name</span></span><br><span class="line"></span><br><span class="line"><span class="attr">slots:</span></span><br><span class="line">  <span class="attr">concerts:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">list</span></span><br><span class="line">    <span class="attr">influence_conversation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">venues:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">list</span></span><br><span class="line">    <span class="attr">influence_conversation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">likes_music:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">bool</span></span><br><span class="line">    <span class="attr">influence_conversation:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">responses:</span></span><br><span class="line">  <span class="attr">utter_greet:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">&quot;Hey there!&quot;</span></span><br><span class="line">  <span class="attr">utter_goodbye:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">&quot;Goodbye :(&quot;</span></span><br><span class="line">  <span class="attr">utter_default:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">&quot;Sorry, I didn&#x27;t get that, can you rephrase?&quot;</span></span><br><span class="line">  <span class="attr">utter_youarewelcome:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">&quot;You&#x27;re very welcome.&quot;</span></span><br><span class="line">  <span class="attr">utter_iamabot:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">&quot;I am a bot, powered by Rasa.&quot;</span></span><br><span class="line">  <span class="attr">utter_get_started:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">&quot;I can help you find concerts and venues. Do you like music?&quot;</span></span><br><span class="line">  <span class="attr">utter_awesome:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">&quot;Awesome! You can ask me things like \&quot;Find me some concerts\&quot; or \&quot;What&#x27;s a good venue\&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">actions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">action_search_concerts</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">action_search_venues</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">action_show_concert_reviews</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">action_show_venue_reviews</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">action_set_music_preference</span></span><br><span class="line"></span><br><span class="line"><span class="attr">session_config:</span></span><br><span class="line">  <span class="attr">session_expiration_time:</span> <span class="number">60</span>  <span class="comment"># value in minutes</span></span><br><span class="line">  <span class="attr">carry_over_slots_to_new_session:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>Tokenizer：分词器；编译器 </p><p>Token：<br>【计算机科学技术】单点登陆，记号，权杖，</p><p>【电子、通信与自动控制技术】令牌，权标 ，许可证 </p><p>【经济学】代价券，礼券 </p><p>【化学】表征 </p><p>【能源科学技术】表征 </p><p>【历史学】私铸货币 </p><p>Tokenization:</p><p>标记化</p></blockquote><blockquote><p>WhitespaceTokenizer：</p><p>Tokenizer using whitespace as a separator</p><p>NER：Name Entity Recognition</p></blockquote><p><strong>Reference：</strong></p><p>1、<a href="https://blog.csdn.net/ljp1919/article/details/103954937">Rasa教程系列-NLU-1-训练集格式</a></p><p>2、<a href="https://blog.csdn.net/ljp1919/article/details/103960020/">Rasa教程系列-NLU-2- 选择pipeline</a></p><p>3、<a href="https://blog.csdn.net/ljp1919/article/details/103962384">Rasa教程系列-NLU-3-实体抽取</a></p><p>4、<a href="https://blog.csdn.net/ljp1919/article/details/103975263">Rasa教程系列-NLU-4-组件</a></p><blockquote><p><a href="https://blog.csdn.net/ljp1919/article/details/103912756">Rasa教程系列-0-Rasa安装和项目创建</a></p><p><a href="https://blog.csdn.net/ljp1919/article/details/103915763">Rasa教程系列-1-命令行交互</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/ljp1919/article/details/103977395">Rasa教程系列-Core-1-Stories</a></p><p><a href="https://blog.csdn.net/ljp1919/article/details/103989971">Rasa教程系列-Core-2-Domains</a></p><p><a href="https://blog.csdn.net/ljp1919/article/details/103991346">Rasa教程系列-Core-3-Responses</a></p><p><a href="https://blog.csdn.net/ljp1919/article/details/103993800">Rasa教程系列-Core-4-Actions</a></p><p><a href="https://blog.csdn.net/ljp1919/article/details/104002385">Rasa教程系列-Core-5-Policies</a></p><p><a href="https://blog.csdn.net/ljp1919/article/details/104018940">Rasa教程系列-Core-6-Slots</a></p></blockquote><blockquote><p><a href="https://zhuanlan.zhihu.com/p/88112269">rasa文章导引</a></p></blockquote><ul><li><a href="https://jiangdg.blog.csdn.net/article/details/104328946">Rasa中文聊天机器人开发指南(1)：入门篇</a></li><li><a href="https://jiangdg.blog.csdn.net/article/details/104530994">Rasa中文聊天机器人开发指南(2)：NLU篇</a></li><li><a href="https://jiangdg.blog.csdn.net/article/details/105434136">Rasa中文聊天机器人开发指南(3)：Core篇</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 任务型对话 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 任务型对话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week2-使用RASA制作你的第一个对话机器人</title>
      <link href="2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
      <url>2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="RASA学习分享"><a href="#RASA学习分享" class="headerlink" title="RASA学习分享"></a>RASA学习分享</h2><h3 id="一、Rasa-Installation"><a href="#一、Rasa-Installation" class="headerlink" title="一、Rasa Installation"></a>一、Rasa Installation</h3><p>Environment：Windows 10 64Bit</p><h4 id="Step-1-Create-virtual-environmnent"><a href="#Step-1-Create-virtual-environmnent" class="headerlink" title="Step 1:  Create virtual environmnent"></a>Step 1:  Create virtual environmnent</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">conda create -n rasa_pyenv python=3.6</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> Windows 默认的虚拟环境位于anaconda安装目录下的env目录中；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Linux则是位于<span class="variable">$HOME</span>/.conda/envs/rasa_pyenv中；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 若要指定安装位置，则：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> conda create -p (/opt/environment/.conda/envs/env_name) (python=2.7)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 若要删除虚拟环境，则：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> conda env remove -p D:/Programs/Python/Miniconda3/envs/rasa_poetry</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 或者</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> conda env remove -n rasa_pyenv3</span></span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><h4 id="Step-2-Activate-virtual-environment-and-install-rasa-x"><a href="#Step-2-Activate-virtual-environment-and-install-rasa-x" class="headerlink" title="Step 2: Activate virtual environment and install rasa-x"></a>Step 2: Activate virtual environment and install rasa-x</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">conda activate rasa_pyenv</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Plan A:</span></span><br><span class="line">pip3 install rasa-x --extra-index-url https://pypi.rasa.com/simple</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ==================================================================</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Plan B:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1. Clone Repo:</span></span></span><br><span class="line">git clone https://github.com/RasaHQ/rasa.git</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2. Install Poetry(弊端：经常断线，重新执行命令会报错，只能手动pip安装对应的包之后再重复执行命令):</span></span></span><br><span class="line">curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python</span><br><span class="line">source ~/.poetry/env</span><br><span class="line">poetry install</span><br><span class="line"><span class="meta">#</span><span class="bash"> ==================================================================</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Plan C:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2. (option) PyCharm install Poetry Plugin:</span></span></span><br><span class="line">https://plugins.jetbrains.com/plugin/14307-poetry</span><br></pre></td></tr></table></figure><p>Windows系统下的环境要求：</p><p>确保安装了Microsoft vc++编译器，这样python就可以编译任何依赖项。你可以从Visual Studio获得编译器。下载安装程序并在列表中选择vc++构建工具。</p><p><strong>遇到的坑：</strong></p><p>① 提示ujson在编译时出错，原因是没有安装microsoft vc++编译器。根据提示去<a href="https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/%E4%B8%8B%E8%BD%BD%E4%BA%86%E7%BC%96%E8%AF%91%E5%99%A8%E5%B9%B6%E5%AE%89%E8%A3%85%EF%BC%8C%E5%9C%A8%E5%AE%89%E8%A3%85%E5%88%97%E8%A1%A8%E4%B8%AD%E5%8B%BE%E9%80%89vc++%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7%EF%BC%8C%E5%AE%8C%E6%88%90vc++%E7%9A%84%E5%AE%89%E8%A3%85%E3%80%82">https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/下载了编译器并安装，在安装列表中勾选vc++生成工具，完成vc++的安装。</a></p><p>② 安装过程中，经常网络出错，尤其装tensorflow时，这个多试几次，找个网络稳定的时间就好了。</p><p>③ 重新安装rasa x，成功。</p><h4 id="Step-3-Initial-rasa"><a href="#Step-3-Initial-rasa" class="headerlink" title="Step 3: Initial rasa"></a>Step 3: Initial rasa</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rasa init</span><br></pre></td></tr></table></figure><p>创建rasa项目所需的所有文件，并根据一些示例数据训练一个简单的机器人。如果你省略了<code>——no-prompt</code>参数，将会询问你一些关于项目设置的问题（可以补充项目路径，训练rasa初始模型）。</p><p>将创建以下文件:</p><p>.<br>├── <strong>init</strong>.py<br>├── actions.py<br>├── config.yml<br>├─ credentials.yml<br>├── data<br>│   ├── nlu.md<br>│   └── stories.md<br>├── domain.yml<br>├── endpoints.yml<br>└── models<br>    └── <timestamp>.tar.gz</timestamp></p><table><thead><tr><th>文件名称</th><th>作用说明</th></tr></thead><tbody><tr><td><strong>init</strong>.py</td><td>帮助python查找操作的空文件</td></tr><tr><td>actions.py</td><td>为你的自定义操作编写代码</td></tr><tr><td>config.yml ‘*’</td><td>配置NLU和Core模型</td></tr><tr><td>credentials.yml</td><td>连接到其他服务的详细信息</td></tr><tr><td>data/nlu.md ‘*’</td><td>你的NLU训练数据</td></tr><tr><td>data/stories.md ‘*’</td><td>你的故事</td></tr><tr><td>domain.yml ‘*’</td><td>你的助手的域</td></tr><tr><td>endpoints.yml</td><td>接到fb messenger等通道的详细信息</td></tr><tr><td>models/.tar.gz</td><td>你的初始模型</td></tr></tbody></table><h4 id="Step-4-Start-rasa"><a href="#Step-4-Start-rasa" class="headerlink" title="Step 4: Start rasa"></a>Step 4: Start rasa</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rasa x</span><br></pre></td></tr></table></figure><p>在本机启动rasa（需要切换至Step 3 rasa init后指定的目录）。</p><p>问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Event loop is closed</span><br></pre></td></tr></table></figure><p><img src="/2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210105110432.png"></p><p><del>原因暂时未知。</del></p><p><del>老师补充：<a href="https://forum.rasa.com/t/error-this-event-loop-is-already-running/24017/2">https://forum.rasa.com/t/error-this-event-loop-is-already-running/24017/2</a></del></p><p><del>（问题暂定为rasa x的依赖包不兼容）：</del></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install google-auth==1.10.1 prompt-toolkit==2.0.10 questionary==1.4.0 SQLAlchemy==1.3.12 urllib3==1.25.7</span><br></pre></td></tr></table></figure><p><del>实际并未解决。</del></p><p><del>解决方案：创建python环境为py3.6。</del></p><h3 id="二、Why-rasa"><a href="#二、Why-rasa" class="headerlink" title="二、Why rasa"></a>二、Why rasa</h3><h4 id="Runs-locally"><a href="#Runs-locally" class="headerlink" title="Runs locally"></a>Runs locally</h4><p>No Network overhead没有网络开销</p><p>Control QoS（Quality of Service ）可控的服务质量</p><p>Deploy anywhere</p><h4 id="Own-your-data"><a href="#Own-your-data" class="headerlink" title="Own your data"></a>Own your data</h4><p>Don’t hand data over to big tech co’s（不要将数据交给大型技术公司）</p><p>avoid vender lock-in（避免供应商锁定）</p><h4 id="Hackable（可控的）"><a href="#Hackable（可控的）" class="headerlink" title="Hackable（可控的）"></a>Hackable（可控的）</h4><p>Tune models for your use case</p><h3 id="三、FIVE-LEVELS-OF-AI-ASSISTANT-ENTERPRISE"><a href="#三、FIVE-LEVELS-OF-AI-ASSISTANT-ENTERPRISE" class="headerlink" title="三、FIVE LEVELS OF AI ASSISTANT ENTERPRISE"></a>三、FIVE LEVELS OF AI ASSISTANT ENTERPRISE</h3><p>企业AI助手的五个等级</p><p>level1：Notification Assistants（通知助手）</p><p>level2：FAQ Assistants （常见问题助手）Frequently Asked Question</p><p>level3:  Contextual Assistants （可基于上下文语义理解的助手）</p><p>level4：Personalized Assistants （个性化的助理）</p><p>level5：Autonomous Organization of Assistants（自治组织的助手）</p><h3 id="四、RASA-Architecture"><a href="#四、RASA-Architecture" class="headerlink" title="四、RASA Architecture"></a>四、RASA Architecture</h3><h4 id="4-0-Rasa框架"><a href="#4-0-Rasa框架" class="headerlink" title="4.0 Rasa框架"></a>4.0 Rasa框架</h4><p>下图显示了使用Rasa构建的助手如何响应消息的基本步骤：</p><p><img src="/2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/rasa-arch.png"></p><p>步骤如下：</p><ol><li>收到消息并将其传递给解释器，解释器将其转换为包含原始文本，意图和找到的任何实体的字典。这部分由NLU处理。</li><li><code>Tracker</code>是跟踪对话状态的对象。它接收新消息进入的信息。</li><li>策略接收<code>Tracker</code>的当前状态。</li><li>该政策选择接下来采取的行动。</li><li>选择的操作由<code>Tracker</code>记录。</li><li>响应被发送给用户。</li></ol><blockquote><p>消息可以是人类输入的文本，</p><p>也可以是按钮按下等结构化输入。</p></blockquote><p>Rasa提供对话系统中的两个核心模块：NLU和对话管理。</p><p>NLU：利用规则、机器学习，统计学习，深度学习等方法，对一条人类语言进行文本分析，分析得到的主要结果为意图intent以及实体entity信息。其中，意图对应task-orient对话系统中的intent。而实体信息则用于对话系统中的槽填充。</p><p>对话管理：在rasa中，对话管理的主要职责是通过NLU的分析得到的意图和实体信息，进行槽位填充，然后结合前几轮对话的状态，根据某种策略（策略可以是人工规则，或者机器学习，深度学习，强化学习训练得到的策略模型），决定应当如何对当前用户的对话进行回应。因此rasa的对话管理是包括槽填充的。</p><p><img src="/2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/rasa-architecture.png"></p><p><strong>RASA的组成模块</strong></p><p><strong>1、NLU:</strong> </p><p>determines what the user wants and captures key contextual information</p><p><strong>2、Core:</strong> </p><p>selects the next best response or action based on conversation history</p><p><strong>3、Channels and integrations and action server:</strong> </p><p>connect assistant to users and backend systems<br><strong>4、NLG</strong></p><h4 id="4-1-Rasa-config-file"><a href="#4-1-Rasa-config-file" class="headerlink" title="4.1 Rasa config file"></a>4.1 Rasa config file</h4><p>Rasa配置文件（./configu.yml）定义了模型要用到的 Rasa NLU 和 Rasa Core 组件，官方示例这个配置文件，这里面 NLU 模型将使用 supervised_embeddings pipeline。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Configuration for Rasa NLU.</span><br><span class="line"># https:&#x2F;&#x2F;rasa.com&#x2F;docs&#x2F;rasa&#x2F;nlu&#x2F;components&#x2F;</span><br><span class="line">language: en</span><br><span class="line">pipeline: supervised_embeddings</span><br><span class="line"></span><br><span class="line"># Configuration for Rasa Core.</span><br><span class="line"># https:&#x2F;&#x2F;rasa.com&#x2F;docs&#x2F;rasa&#x2F;core&#x2F;policies&#x2F;</span><br><span class="line">policies:</span><br><span class="line">  - name: MemoizationPolicy</span><br><span class="line">  - name: KerasPolicy</span><br><span class="line">  - name: MappingPolicy</span><br></pre></td></tr></table></figure><h4 id="4-2-Rasa-NLU"><a href="#4-2-Rasa-NLU" class="headerlink" title="4.2 Rasa NLU"></a>4.2 Rasa NLU</h4><p>Rasa NLU是Rasa的核心模块之一，这个模块用于对用户消息内容进行语义理解，并将结果转换成结构化的数据。在 Rasa 这里，需要提供一份训练数据，Rasa NLU 会基于这份数据进行模型训练，然后通过模型对用户消息进行语义理解，主要是意图识别和槽值提取。</p><p>在对话系统的NLU中，意图识别（Intent Detection，简写为ID）和槽位填充（Slot Filling，简写为SF）是两个重要的子任务。其中，意图识别可以看做是NLP中的一个分类任务，而槽位填充可以看做是一个序列标注任务，在早期的系统中，通常的做法是将两者拆分成两个独立的子任务。但这种做法跟人类的语言理解方式是不一致的，事实上我们在实践中发现，两者很多时候是具有较强相关性的，比如下边的例子：</p><blockquote><p>1.我要听[北京天安门, song] – Intent：播放歌曲<br>2.帮我叫个车，到[北京天安门, location] – Inent：打车<br>3.播放[忘情水, song] – Intent：播放歌曲<br>4.播放[复仇者联盟, movie] – Intent：播放视频</p></blockquote><p>1和2中，可以看到同样是“北京天安门”，由于意图的不同，该实体具备完全不同的槽位类型。3和4中，由于槽位类型的不同，导致了最终意图的不同，这往往意味着，在对话系统中的后继流程中将展现出完全不同的行为—–打开网易音乐播放歌曲 or 打开爱奇艺播放电影。</p><p>随着对话系统的热度逐渐上升，研究的重点也逐渐倾向于将两个任务进行联合，以充分利用意图和槽位中的语义关联。那么，问题来了，我们该如何进行联合呢？从目前的趋势来看，大体上有两大类方法：</p><ol><li>多任务学习：按Multi-Task Learning的套路，在学习时最终的loss等于两个任务的loss的weight sum，两者在模型架构上仍然完全独立，或者仅共享特征编码器。</li><li>交互式模型：将模型中Slot和Intent的隐层表示进行交互，引入更强的归纳偏置，最近的研究显示，这种方法的联合NLU准确率更高。</li></ol><p>接下来，我们将对这两类方法涉及到的部分文献进行分析，为大家的研究提供参考。</p><p>参考：<a href="https://zhuanlan.zhihu.com/p/75228411">Intent Detection and Slot Filling</a></p><p><img src="/2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/NLU.png"></p><p><img src="/2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/NLU2.png"></p><p>Rasa NLU 怎么识别语义【entity】的：</p><p><strong>entity标注：</strong></p><p>中括号，how much do I have on my [savings] (“account”) account</p><p>大括号，即字典的形式，描述能力更强 how much money is in my [checking] [“entity”: “account”] account</p><p><strong>同义词标注：</strong></p><p>Synonym</p><p><strong>正则表达式：</strong></p><p>Regular Expression</p><p>Rasa NLU训练数据样例（./data/nlu.yml）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;2.0&quot;</span><br><span class="line">nlu:</span><br><span class="line">- intent: greet</span><br><span class="line">  examples: |</span><br><span class="line">    - hey</span><br><span class="line">    - hello</span><br><span class="line">    - hi</span><br><span class="line">    - hello there</span><br><span class="line">    - good morning</span><br><span class="line">    - good evening</span><br><span class="line">    - moin</span><br><span class="line">    - hey there</span><br><span class="line">    - let&#39;s go</span><br><span class="line">    - hey dude</span><br><span class="line">    - goodmorning</span><br><span class="line">    - goodevening</span><br><span class="line">    - good afternoon</span><br><span class="line">    - yhugy</span><br><span class="line">- intent: goodbye</span><br><span class="line">  examples: |</span><br><span class="line">    - good afternoon</span><br><span class="line">    - cu</span><br><span class="line">    - good by</span><br><span class="line">    - cee you later</span><br><span class="line">    - good night</span><br><span class="line">    - bye</span><br><span class="line">    - goodbye</span><br><span class="line">    - have a nice day</span><br><span class="line">    - see you around</span><br><span class="line">    - bye bye</span><br><span class="line">    - see you later</span><br></pre></td></tr></table></figure><p>其中：</p><table><thead><tr><th align="center">标识</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">intents</td><td align="center">意图</td></tr><tr><td align="center">actions</td><td align="center">动作</td></tr><tr><td align="center">templates</td><td align="center">回答模板</td></tr><tr><td align="center">entities</td><td align="center">实体</td></tr><tr><td align="center">slots</td><td align="center">词槽</td></tr></tbody></table><h4 id="4-3-Rasa-Core"><a href="#4-3-Rasa-Core" class="headerlink" title="4.3 Rasa Core"></a>4.3 Rasa Core</h4><p>(Dialogue Manager = Dialogue State Tracking + Dialogue Ploicy)</p><p>![](Week2-使用RASA制作你的第一个机器人/dialogue policey.png)</p><p>![](Week2-使用RASA制作你的第一个机器人/rasa core.png)</p><p>![](Week2-使用RASA制作你的第一个机器人/rasa core 2.png)</p><p>Rasa  core主要包含两个内容，stories和domain。</p><h5 id="4-3-1-Stories"><a href="#4-3-1-Stories" class="headerlink" title="4.3.1 Stories"></a>4.3.1 Stories</h5><p>stories可以理解为<strong>对话的场景流程</strong>，我们需要告诉机器我们的多轮场景是怎么样的，例如，在下文的例子中，我们希望的流程是这样的：用户问好 -&gt; 机器问用户今天过得怎么样 -&gt; 用户反馈情绪 -&gt; 机器根据不同的情绪进行回复，这里其实包含两个流程，一个正面情绪的流程与一个负面情绪的流程，因此，我们也需要编写两个story，接下来我们看下怎么编写story。</p><p><strong>对话管理（dialogue management）是对话系统或者聊天机器人的核心，在 Rasa 中由 Rasa Core 负责，而这部分的训练数据在Rasa 中由 Stories 提供。</strong>Stories可以理解为对话的场景流程，一个 story 是一个用户和AI小助手之间真实的对话，这里面包含了可以反映用户输入（信息）的意图和实体以及小助手在回复中应该采取的 action（行动）。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;2.0&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">stories:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">story:</span> <span class="string">happy</span> <span class="string">path</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">greet</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_greet</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">mood_great</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_happy</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">story:</span> <span class="string">sad</span> <span class="string">path</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">greet</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_greet</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">mood_unhappy</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_cheer_up</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_did_that_help</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">intent:</span> <span class="string">affirm</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">utter_happy</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="4-3-2-Rasa-Domain"><a href="#4-3-2-Rasa-Domain" class="headerlink" title="4.3.2 Rasa Domain"></a>4.3.2 Rasa Domain</h5><p>Domain 可以理解为机器的知识库，其中定义了意图（intents)，动作（actions)，以及对应动作所反馈的内容模板（templates)，例如它能预测的用户意图，它可以处理的 actions，以及对应 actions 的响应内容。</p><p><font color="red"><strong><code>Domain</code>定义了机器人助手所处的世界。</strong></font></p><p><font color="red"><strong>它指定了机器人应该知道的<code>意图(intents)、实体(entities)、槽位(slots)和操作(actions)</code>。另外，它还可以包含机器人能够说的内容的<code>模板(templates)</code>。</strong>PS：新版本的actions和templates合并成为response。</font></p><p>为AI小助手准备的 domain 存储在 domain.yml 文件中，可以观察一下这份样例数据：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ession_config:</span></span><br><span class="line">  <span class="attr">session_expiration_time:</span> <span class="number">60</span></span><br><span class="line">  <span class="attr">carry_over_slots_to_new_session:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">intents:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">greet</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">goodbye</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">affirm</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">deny</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">mood_great</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">mood_unhappy</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">bot_challenge</span></span><br><span class="line"><span class="attr">responses:</span></span><br><span class="line">  <span class="attr">utter_greet:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">Hey!</span> <span class="string">How</span> <span class="string">are</span> <span class="string">you?</span></span><br><span class="line">  <span class="attr">utter_cheer_up:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">https://i.imgur.com/nGF1K8f.jpg</span></span><br><span class="line">    <span class="attr">text:</span> <span class="string">&#x27;Here is something to cheer you up:&#x27;</span></span><br><span class="line">  <span class="attr">utter_did_that_help:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">Did</span> <span class="string">that</span> <span class="string">help</span> <span class="string">you?</span></span><br><span class="line">  <span class="attr">utter_happy:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">Great,</span> <span class="string">carry</span> <span class="string">on!</span></span><br><span class="line">  <span class="attr">utter_goodbye:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">text:</span> <span class="string">Bye</span></span><br></pre></td></tr></table></figure><p>这些将通过 Rasa Core 管理，Rasa Core 的核心工作就是在对话的每一步选择正确的 action 去执行。在这个例子中，actions 是简单的向用户发送一条消息，这些 actions 定义在domain中，以 utter_开头，AI小助手将会根据内容模板（templates) 回复消息。</p><h4 id="4-4-Rasa-Core-Train"><a href="#4-4-Rasa-Core-Train" class="headerlink" title="4.4 Rasa Core Train"></a>4.4 Rasa Core Train</h4><p>如果添加了 NLU 或者 Core 数据，或者修改了domain和配置文件，需要重新训练模型，用下面的这条命令即可，这个命令将调用Rasa Core或者NLU的训练函数以及在 models/ 目录下存储训练模型。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rasa train</span><br></pre></td></tr></table></figure><h3 id="其他补充"><a href="#其他补充" class="headerlink" title="其他补充"></a>其他补充</h3><p>1、Rasa常见任务命令</p><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td>rasa_init</td><td>创建一个新的项目，包含示例训练数据，动作和配置文件</td></tr><tr><td>rasa_train</td><td>使用NLU数据和stories训练模型，保存模型在<code>./models</code>中</td></tr><tr><td>rasa interactive</td><td>通过交谈开启一个新的交互学习会话来创建新的训练数据</td></tr><tr><td>rasa shell</td><td>加载训练模型，与助手通过命令行交谈</td></tr><tr><td>rasa run</td><td>使用训练的模型开启一个Rasa服务</td></tr><tr><td>rasa run actions</td><td>使用Rasa SDK开启action服务器</td></tr><tr><td>rasa visualize</td><td>可视化stories</td></tr><tr><td>rasa test</td><td>使用测试NLU数据和故事来测试训练好的Rasa模型</td></tr><tr><td>rasa data split nlu</td><td>根据指定的百分比执行NLU数据的拆分</td></tr><tr><td>rasa data convert nlu</td><td>在不同格式之间转换NLU训练数据</td></tr><tr><td>rasa x</td><td>在本地启动Rasa X</td></tr><tr><td>rasa -h</td><td>显示所有可用命令</td></tr></tbody></table><p>2、wikipedia和百度百科语料生成了一个total_word_feature_extractor_chi.dat，分享如下。”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">链接：http:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1micEF0G 密码：opli</span><br></pre></td></tr></table></figure><p>放入到data/total_word_feature_extractor_zh.dat</p><p>3、<a href="https://www.cnblogs.com/wqbin/p/11611469.html">pipeline和baseline是什么？</a></p><p>4、<a href="https://zhuanlan.zhihu.com/p/77868938">nlp中的实体关系抽取方法总结</a></p><p>5、domain.yaml中intents actions response是怎么对应的</p><p>助教答案：</p><p><img src="/2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/sthMapping.png"></p><p><img src="/2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/mapping1.png"></p><p>NLP 路线</p><p><img src="/2021/01/04/Week2-%E4%BD%BF%E7%94%A8RASA%E5%88%B6%E4%BD%9C%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/NLP.jpg"></p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1、<a href="https://juejin.cn/post/6844903922042142734">RASA用户指南</a></p><p>2、<a href="https://zhuanlan.zhihu.com/p/75517803">基于RASA的task-orient对话系统解析（一）</a></p><p>3、<a href="https://zhuanlan.zhihu.com/p/78665885">基于RASA的task-orient对话系统解析（二）——对话管理核心模块</a></p><p>4、<a href="https://zhuanlan.zhihu.com/p/81430436">基于RASA的task-orient对话系统解析（三）——基于rasa的会议室预定对话系统实例</a></p><p>5、<a href="https://www.52nlp.cn/rasa%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97-%E5%88%9D%E8%AF%86rasa">Rasa 入坑指南一：初识 Rasa</a></p><p>6、<a href="https://www.52nlp.cn/rasa%E5%85%A5%E5%9D%91%E6%8C%87%E5%8D%97%E4%BA%8C-%E5%9F%BA%E4%BA%8E-rasa-%E6%9E%84%E5%BB%BA%E5%A4%A9%E6%B0%94%E6%9F%A5%E8%AF%A2%E6%9C%BA%E5%99%A8%E4%BA%BA">Rasa 入坑指南二：基于 Rasa 构建天气查询机器人</a></p><p>7、<a href="https://github.com/xiaoxiong74/rasa_chatbot">Rasa ChatBot</a></p><p>8、<a href="https://github.com/NLP-LOVE/ML-NLP">NLP学习路线</a></p><p>9、<a href="https://blog.csdn.net/weixin_41510260/article/details/99876405">自然语言处理(NLP)的一般处理流程</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 任务型对话 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 任务型对话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>week1-对话系统导论</title>
      <link href="2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/"/>
      <url>2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="对话系统导论学习分享"><a href="#对话系统导论学习分享" class="headerlink" title="对话系统导论学习分享"></a>对话系统导论学习分享</h2><h3 id="一、对话系统的分类"><a href="#一、对话系统的分类" class="headerlink" title="一、对话系统的分类"></a>一、对话系统的分类</h3><p>1、模块化对话系统，即分模块串行处理对话任务，每个模块负责特定的任务，并将结果传递给下一个模块。在具体的实现上，可以针对任一模块采用基于规则的人工设计方式，或者基于数据驱动的模型方式。</p><p>2、端到端的对话系统，由输入直接到输出的端到端对话系统，忽略中间过程，采用数据驱动的模型实现。</p><p>Task Type</p><blockquote><p>1、Task-Oriented/Task-completed Bot</p><p>2、QA(Question Answering) Bot</p><p>3、Chatbot/Chitchat/Social Chat</p></blockquote><p>Methodology of Building bots （机器人程序的方法论）</p><p><strong>Categories of Conversational AI （AI会话的分类）</strong></p><blockquote><p><strong>1、Chatbot（闲聊型对话，微软小冰）</strong></p><p>闲聊型对话大多为开放域的对话，主要以满足用户的情感需求为主，通过产生有趣、富有个性化的答复内容，与用户进行互动。</p><p><strong>2、QA Bot（问答型对话， 搜索）</strong></p><p>主要为一问一答的形式，机器人对用户提出的问题进行解析，在知识库已有的内容中查找并返回正确答案。对于机器人而言，每次问答均是独立的，与上下文信息无关。</p><p><strong>3、Task-oriented Bot （面向任务的机器人，多轮对话系统）</strong></p><p>主要指机器人为满足用户某一需求（带有明确目的）而产生的多轮对话（如查流量，查话费，订餐，订票，咨询等任务型场景），机器人通过理解、澄清等方式确定用户意图，继而通过答复、调用API等方式完成该任务。在该任务内，机器人需要理解上下文信息并作出下一步的动作。</p><p>由于用户的需求较为复杂，通常情况下需<strong>分多轮互动</strong>，用户也可能在对话过程中不断修改与完善自己的需求，任务型机器人需要通过询问、澄清和确认来帮助用户明确目的。</p><p>任务型对话系统也常被称为<strong>多轮对话系统</strong>，目前工业界有两种实现方式，一种是基于<strong>规则</strong>的实现方式，另一种则是基于<strong>End-to-End</strong>的实现方式。基于End-to-End的实现方式试图训练一个从用户端自然语言输入到机器端自然语言输出的整体映射关系，从而提高系统的灵活性与可拓展性，但该模型对数据的质量和数量要求非常高，并且存在不可解释性，因此，<strong>目前工业界大多采用基于规则的实现方式</strong>。本文以下内容中所提到的任务型对话系统也是基于规则的实现方式。</p></blockquote><p>Category of QA</p><blockquote><p>KB-QA</p><blockquote><p>KB(Knowledge Base)</p><p>KG(Knowledge Graph)</p></blockquote><p>Text-QA</p></blockquote><p>Methodology on KB</p><blockquote><p>Semantic Parsing （语义解析）</p><p>Information Extraction （信息提取）</p><p>Embedding-based Methods （基于嵌入的方法）</p><p>Neural Methods （神经网络方法）</p></blockquote><p><a href="https://coffee.pmcaff.com/article/971158746030208/pmcaff">Dialogue Action</a></p><blockquote><p>System Action</p><p>User Action</p></blockquote><p>State</p><blockquote><p>User State</p><p>System State</p></blockquote><p>NLG </p><blockquote><p>Template-based</p><p>Model-based</p></blockquote><h3 id="二、Modular-Pipeline-Approach-Architecture【模块化-管道方法架构】"><a href="#二、Modular-Pipeline-Approach-Architecture【模块化-管道方法架构】" class="headerlink" title="二、Modular/Pipeline Approach Architecture【模块化/管道方法架构】"></a>二、Modular/Pipeline Approach Architecture【模块化/管道方法架构】</h3><p>主要组成部分【参考PPT第34页】</p><p>Dialogue Input(对话输入) –&gt; </p><blockquote><p><strong>Natural Language Understanding(NLU) –&gt;</strong> </p><p><strong>Dialogue Manager（DM）</strong></p><blockquote><p><strong>Dialogue State Tracking(DST)–&gt;</strong>  </p><p><strong>Action Generation(Dialogue Policy) –&gt;</strong> </p></blockquote><p><strong>Natural Language Generation(NLG) –&gt;</strong> </p></blockquote><p>Dialogue Output(对话输出)</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/Architecture.png"></p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/%E9%9D%A2%E5%90%91%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9D%97.png" alt="image-20201228182151761"></p><h4 id="2-1-NLU（Natural-Language-Understanding）自然语言理解"><a href="#2-1-NLU（Natural-Language-Understanding）自然语言理解" class="headerlink" title="2.1 NLU（Natural Language Understanding）自然语言理解"></a>2.1 NLU（Natural Language Understanding）自然语言理解</h4><h5 id="什么是NLU"><a href="#什么是NLU" class="headerlink" title="什么是NLU"></a>什么是NLU</h5><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/NLU.png"></p><p>NLU是将用户输入的自然语言语句映射为机器可读的结构化语义表述，这种结构化语义一般由两部分构成，分别是<strong>用户意图</strong>（user intention）和<strong>槽值</strong>（slot-value）。</p><p>对面向任务的对话系统来说，NLU模块的主要任务是将用户输入的自然语言映射为用户的意图和相应的槽位值。因此，NLU模块的输入是讲用户对话语句$X_n$，输出是解析$X_n$后得到的用户动作$U_n$，该模块涉及的主要技术是<strong>意图识别和槽位填充</strong>，这两种技术分别对应用户动作的两项结构化参数，即意图和槽位。</p><p>【做好NLU需要掌握两项关键技能：意图识别和实体提取】</p><p>下面主要讨论如何针对面向任务的对话系统设计NLU模块，包括针对特定任务定义意图和相应的槽位，以及后续从用户的输入中获取任务目标的意图识别方法和对应的槽位填充方法。</p><p><strong>（1）意图和槽位的定义</strong></p><p>意图和槽位共同构成了“用户动作”，机器是无法直接理解自然语言的，因此用户动作的作用便是将自然语言映射为机器能够理解的结构化语义表示。</p><p>意图识别，也被称为SUC（Spoken Utterance Classification），顾名思义，是将用户输入的自然语言会话进行划分，类别（classification）对应的就是用户意图。例如“今天天气如何”，其意图为“询问天气”。自然地，可以将意图识别看作一个典型的分类问题。意图的分类和定义可参考ISO-24617-2标准，其中共有56种详细的定义。面向任务的对话系统中的意图识别通常可以视为文本分类任务。同时，意图的定义与对话系统自身的定位和所具有的知识库有很大关系，即意图的定义具有非常强的领域相关性。</p><p>槽位，即意图所带的参数。一个意图可能对应若干个槽位，例如询问公交车路线时，需要给出出发地、目的地、时间等必要参数。以上参数即“询问公交车路线”这一意图对应的槽位。语义槽位填充任务的主要目标是在已知特定领域或特定意图的语义框架（semantic frame）的前提下，从输入语句中抽取该语义框架中预先定义好的语义槽的值。语义槽位填充任务可以转化为序列标注任务，即运用经典的IOB标记法，标记某一个词是某一语义槽的开始（begin）、延续（inside），或是非语义槽（outside）。</p><p>要使一个面向任务的对话系统能正常工作，首先要设计意图和槽位。意图和槽位能够让系统知道该执行哪项特定任务，并且给出执行该任务时需要的参数类型。为了方便与问答系统做异同对比，我们依然以一个具体的“询问天气”的需求为例，介绍面向任务的对话系统中对意图和槽位的设计。</p><p><strong>用户输入示例</strong>：“今天上海天气怎么样”</p><p><strong>用户意图定义</strong>：询问天气，Ask_Weather</p><p><strong>槽位定义</strong></p><p>槽位一：时间，Date</p><p>槽位二：地点，Location</p><p>“询问天气”的需求对应的意图和槽位如下图所示。</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/%E6%84%8F%E5%9B%BE%E4%B8%8E%E6%A7%BD%E4%BD%8D%E7%9A%84%E5%AE%9A%E4%B9%89.png" alt="image-20201228182151761"></p><p>在上述示例中，针对“询问天气”任务定义了两个必要的槽位，它们分别是“时间”和“地点”。</p><p>对于一个单一的任务，上述定义便可解决任务需求。但在真实的业务环境下，一个面向任务的对话系统往往需要能够同时处理若干个任务，例如气象台除了能够回答“询问天气”的问题，也应该能够回答“询问温度”的问题。</p><p>对于同一系统处理多种任务的复杂情况，一种优化的策略是定义更上层的领域，如将“询问天气”意图和“询问温度”意图均归属于“天气”领域。在这种情况下，可以简单地将领域理解为意图的集合。定义领域并先进行领域识别的优点是可以约束领域知识范围，减少后续意图识别和槽位填充的搜索空间。此外，对于每一个领域进行更深入的理解，利用好任务及领域相关的特定知识和特征，往往能够显著地提升NLU模块的效果。据此，对图2的示例进行改进，加入“天气”领域。</p><p><strong>用户输入示例</strong></p><p>1、“今天上海天气怎么样”</p><p>2、“上海现在气温多少度”</p><p><strong>领域定义</strong>：天气，Weather</p><p><strong>用户意图定义</strong></p><p>1、询问天气，Ask_Weather</p><p>2、询问温度，Ask_Temperature</p><p><strong>槽位定义</strong></p><p>槽位一：时间，Date</p><p>槽位二：地点，Location</p><p>改进后的“询问天气”的需求对应的意图和槽位如下图所示。</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/%E6%84%8F%E5%9B%BE%E4%B8%8E%E6%A7%BD%E4%BD%8D%E7%9A%84%E5%AE%9A%E4%B9%892.png" alt="image-20201228182151761"></p><p><strong>（2）意图识别和槽位填充</strong></p><p>做好意图和槽位的定义后，需要从用户输入中提取用户意图和相应槽对应的槽值。意图识别的目标是从用户输入的语句中提取用户意图，单一任务可以简单地建模为一个二分类问题，如“询问天气”意图，在意图识别时可以被建模为“是询问天气”或者“不是询问天气”二分类问题。当涉及需要对话系统处理多种任务时，系统需要能够判别各个意图，在这种情况下，二分类问题就转化成了多分类问题。</p><p>槽位填充的任务是从自然语言中提取信息并填充到事先定义好的槽位中，例如在图2中已经定义好了意图和相应的槽位，对于用户输入“今天上海天气怎么样”系统应当能够提取出“今天”和“上海”并分别将其填充到“时间”和“地点”槽位。基于特征提取的传统机器学习模型已经在槽位填充任务上得到了广泛应用。近年来，随着深度学习技术在自然语言处理领域的发展，基于深度学习的方法也逐渐被应用于槽位填充任务。相比于传统的机器学习方法，深度学习模型能够自动学习输入数据的隐含特征。例如，将可以利用更多上下文特征的最大熵马尔可夫模型引入槽位填充的过程中[8]，类似地，也有研究将条件随机场模型引入槽位填充。</p><p>例子：在生活中，如果想要订机票，人们会有很多种自然的表达：</p><blockquote><p>“订机票”；</p><p>“有去上海的航班么？”；</p><p>“看看航班，下周二出发去纽约的”；</p><p>“要出差，帮我查下机票”；</p><p>等等等等</p></blockquote><p>“自然的表达” 有无穷多的组合（自然语言）都是在代表 “订机票” 这个意图的。而听到这些表达的人，可以准确理解这些表达指的是“订机票”这件事。</p><p>而要理解这么多种不同的表达，对机器是个挑战。在过去，机器只能处理“结构化的数据”（比如关键词），也就是说如果要听懂人在讲什么，必须要用户输入精确的指令。</p><p>所以，无论你说“我要出差”还是“帮我看看去北京的航班”，<strong>只要这些字里面没有包含提前设定好的关键词“订机票”，系统都无法处理</strong>。而且，只要出现了关键词，比如“我要退订机票”里也有这三个字，也会被处理成用户想要订机票。</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/image-20201228182151761.png" alt="image-20201228182151761"></p><p>自然语言理解这个技能出现后，可以让机器从各种自然语言的表达中，区分出来，哪些话归属于这个意图；而那些表达不是归于这一类的，而不再依赖那么死板的关键词。比如经过训练后，机器能够识别“帮我推荐一家附近的餐厅”，就不属于“订机票”这个意图的表达。</p><p>并且，通过训练，机器还能够在句子当中自动提取出来“上海”，这两个字指的是目的地这个概念（即实体）；“下周二”指的是出发时间。</p><p>这样一来，看上去“机器就能听懂人话啦！”。</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/image-20201228182339504.png" alt="image-20201228182339504"></p><h5 id="NLU的应用场景"><a href="#NLU的应用场景" class="headerlink" title="NLU的应用场景"></a>NLU的应用场景</h5><p>几乎所有跟文字语言和语音相关的应用都会用到 NLU，如机器翻译、机器客服、智能音箱。</p><p>例：</p><blockquote><p>“我冷了”</p><p>机器：帮您把空调调高1度</p></blockquote><p>用户并没有提到空调，但是机器需要知道用户的意图——空调有点冷，需要把温度调高。</p><h5 id="NLU的难点"><a href="#NLU的难点" class="headerlink" title="NLU的难点"></a>NLU的难点</h5><p><strong>语言的多样性</strong>：组合方式非常灵活，字、词、短语、句子、段落…不同的组合可以表达出很多的含义</p><blockquote><p>给唱一首大王叫我来巡山；</p><p>放音乐大王叫我来巡山；</p></blockquote><p><strong>语言的歧义性</strong>：如果不联系上下文，缺少环境的约束，语言有很大的歧义性</p><blockquote><p>我要去拉萨</p></blockquote><ul><li>需要火车票？</li><li>需要飞机票？</li><li>想听音乐？</li><li>还是想查找景点？</li></ul><p><strong>语言的鲁棒性</strong>：自然语言在输入的过程中，尤其是通过语音识别获得的文本，会存在多字、少字、错字、噪音等问题。例如：</p><blockquote><p>大王叫我来新山</p><p>大王叫让我来巡山</p><p>大王叫我巡山</p></blockquote><p><strong>语言的知识依赖</strong>：语言是对世界的符号化描述，语言天然连接着世界知识，例如：</p><blockquote><p>苹果</p></blockquote><p>除了表示水果，还可以表示科技公司</p><blockquote><p>7天</p></blockquote><p>可以表示时间，也可以表示酒店名</p><blockquote><p>晚安</p></blockquote><p>有一首歌也叫《晚安》</p><p><strong>语言的上下文</strong>：上下文的概念包括很多种：对话的上下文、设备的上下文、应用的上下文、用户画像…</p><blockquote><p>U：买张火车票</p><p>A：请问你要去哪里？</p><p>U：宁夏</p></blockquote><blockquote><p>U：来首歌听</p><p>A：请问你想听什么歌？</p><p>U：宁夏</p></blockquote><h5 id="NLU-的实现方式"><a href="#NLU-的实现方式" class="headerlink" title="NLU 的实现方式"></a>NLU 的实现方式</h5><p>1、基于规则的方法</p><p>早期大家通过总结规律来判断自然语言的意图，常见的方法有：CFG、JSGF等。</p><p>2、基于统计的方法</p><p>后来出现了基于统计学的 NLU 方式，常见的方法有：<a href="https://easyai.tech/ai-definition/svm/">SVM</a>、ME等。</p><p>3、基于深度学习的方法</p><p>随着深度学习的爆发，<a href="https://easyai.tech/ai-definition/cnn/">CNN</a>、<a href="https://easyai.tech/ai-definition/rnn/">RNN</a>、<a href="https://easyai.tech/ai-definition/lstm/">LSTM</a> 都成为了最新的”统治者”。到了2019年，<a href="https://easyai.tech/ai-definition/bert/">BERT</a> 和 GPT-2 的表现震惊了业界，他们都是用了 <a href="https://easyai.tech/ai-definition/transformer/">Transformer</a>，Transformer是目前「最先进」的方法。</p><h4 id="2-2-Dialogue-State-Tracking"><a href="#2-2-Dialogue-State-Tracking" class="headerlink" title="2.2 Dialogue State Tracking"></a>2.2 Dialogue State Tracking</h4><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/DST.png"></p><p><strong>DST</strong>：这一模块的目标是追踪用户需求并判断当前的对话状态。该模块以<strong>多轮对话</strong>历史、当前的用户动作为输入，通过总结和推理理解在上下文的环境下用户当前输入自然语言的具体含义。对于对话系统来说，这一模块有着重大意义，很多时候需要综合考虑用户的多轮输入才能让对话系统理解用户的真正需求。</p><p>DST模块以当前的用户动作$U_n$、$n-1$前轮的对话状态和相应的系统动作作为输入，输出是DST模块判定得到的当前对话状态$S_n$。</p><p>对话状态的表示（DST-State Representation）通常由以下3部分构成。</p><p>（1）目前为止的槽位填充情况。</p><p>（2）本轮对话过程中的用户动作。</p><p>（3）对话历史。</p><p>其中，槽位的填充情况通常是最重要的状态表示指标。</p><h4 id="2-3-Action-Generation-Dialogue-Policy"><a href="#2-3-Action-Generation-Dialogue-Policy" class="headerlink" title="2.3 Action Generation(Dialogue Policy)"></a>2.3 Action Generation(Dialogue Policy)</h4><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/DPL.png"></p><p><strong>DPL</strong>：也被称为对话策略优化（optimization），根据当前的对话状态，对话策略决定下一步执行什么系统动作。系统行动与用户意图类似，也由意图和槽位构成。</p><p>DPL模块的输入是DST模块输出的当前对话状态$S_n$，通过预设的对话策略，选择系统动作$a_n$作为输出。下面结合具体案例介绍基于规则的DPL方法，也就是通过人工设计有限状态自动机的方法实现DPL。</p><p><strong>案例：询问天气</strong></p><p>以有限状态自动机的方法进行规则的设计，有两种不同的方案：一种以点表示数据，以边表示操作；另一种以点表示操作，以边表示数据，这两种方案各有优点，在具体实现时可以根据实际情况进行选择。</p><h4 id="2-4-NLG（Natural-Language-Generating）自然语言生成"><a href="#2-4-NLG（Natural-Language-Generating）自然语言生成" class="headerlink" title="2.4 NLG（Natural Language Generating）自然语言生成"></a>2.4 NLG（Natural Language Generating）自然语言生成</h4><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/NLG.png"></p><p>NLG模块的输入是DPL模块输出的系统动作$a_n$，输出是系统对用户输入$X_n$的回复$Y_n$。</p><p>目前，NLG模块仍广泛采用传统的基于规则的方法，下表给出了3个示例规则的定义。根据规则可以将各个系统动作映射成自然语言表达。</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/NLG%E6%A8%A1%E5%9D%97%E4%BB%8D%E5%B9%BF%E6%B3%9B%E9%87%87%E7%94%A8%E4%BC%A0%E7%BB%9F%E7%9A%84%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E6%96%B9%E6%B3%95.png"></p><p>对于NLG，则是将对话策略模块选择的系统动作（非语言格式的数据）转换成人类可以理解的语言格式，如文章，报告等。</p><h5 id="什么是NLG"><a href="#什么是NLG" class="headerlink" title="什么是NLG"></a>什么是NLG</h5><p>NLG 是为了跨越人类和机器之间的沟通鸿沟，将非语言格式的数据转换成人类可以理解的语言格式，如文章、报告等。</p><h5 id="NLG的2种方式"><a href="#NLG的2种方式" class="headerlink" title="NLG的2种方式"></a>NLG的2种方式</h5><p>text – to – text：文本到语言的生成</p><p>data – to – text ：数据到语言的生成</p><h5 id="NLG的3个Level"><a href="#NLG的3个Level" class="headerlink" title="NLG的3个Level"></a>NLG的3个Level</h5><p><strong>简单的数据合并：</strong>自然语言处理的简化形式，这将允许将数据转换为文本（通过类似Excel的函数）。为了关联，以<a href="https://baike.baidu.com/item/%E9%82%AE%E4%BB%B6%E5%90%88%E5%B9%B6/7804213?fr=aladdin">邮件合并</a>（MS Word mailmerge）为例，其中间隙填充了一些数据，这些数据是从另一个源（例如MS Excel中的表格）中检索的。</p><p><strong>模板化的 NLG</strong> ：这种形式的NLG使用模板驱动模式来显示输出。以足球比赛得分板为例。数据动态地保持更改，并由预定义的业务规则集（如if / else循环语句）生成。</p><p><strong>高级 NLG</strong> ：这种形式的自然语言生成就像人类一样。它理解意图，添加智能，考虑上下文，并将结果呈现在用户可以轻松阅读和理解的富有洞察力的叙述中。</p><h5 id="NLG的6个步骤"><a href="#NLG的6个步骤" class="headerlink" title="NLG的6个步骤"></a>NLG的6个步骤</h5><p><strong>第一步：内容确定 – Content Determination</strong></p><p>作为第一步，NLG 系统需要决定哪些信息应该包含在正在构建的文本中，哪些不应该包含。通常数据中包含的信息比最终传达的信息要多。</p><p><strong>第二步：文本结构 – Text Structuring</strong></p><p>确定需要传达哪些信息后，NLG 系统需要合理的组织文本的顺序。例如在报道一场篮球比赛时，会优先表达「什么时间」「什么地点」「哪2支球队」，然后再表达「比赛的概况」，最后表达「比赛的结局」。</p><p><strong>第三步：句子聚合 – Sentence Aggregation</strong></p><p>不是每一条信息都需要一个独立的句子来表达，将多个信息合并到一个句子里表达可能会更加流畅，也更易于阅读。</p><p><strong>第四步：语法化 – Lexicalisation</strong></p><p>当每一句的内容确定下来后，就可以将这些信息组织成自然语言了。这个步骤会在各种信息之间加一些连接词，看起来更像是一个完整的句子。</p><p><strong>第五步：参考表达式生成 – Referring Expression Generation|REG</strong></p><p>这个步骤跟语法化很相似，都是选择一些单词和短语来构成一个完整的句子。不过他跟语法化的本质区别在于“REG需要识别出内容的领域，然后使用该领域（而不是其他领域）的词汇”。</p><p><strong>第六步：语言实现 – Linguistic Realisation</strong></p><p>最后，当所有相关的单词和短语都已经确定时，需要将它们组合起来形成一个结构良好的完整句子。</p><h5 id="NLG-的3种典型应用"><a href="#NLG-的3种典型应用" class="headerlink" title="NLG 的3种典型应用"></a>NLG 的3种典型应用</h5><p>NLG 的不管如何应用，大部分都是下面的3种目的：</p><ol><li>能够大规模的产生个性化内容</li><li>帮助人类洞察数据，让数据更容易理解</li><li>加速内容生产</li></ol><h3 id="三、Single-Turn-Question-Answering-Systems"><a href="#三、Single-Turn-Question-Answering-Systems" class="headerlink" title="三、Single-Turn Question Answering Systems"></a>三、Single-Turn Question Answering Systems</h3><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/image-20201228220532162.png" alt="image-20201228220532162"></p><h3 id="四、Multi-Turn-Question-Answering"><a href="#四、Multi-Turn-Question-Answering" class="headerlink" title="四、Multi-Turn Question Answering"></a>四、Multi-Turn Question Answering</h3><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/image-20201228220555871.png" alt="image-20201228220555871"></p><h3 id="五、其他补充要点"><a href="#五、其他补充要点" class="headerlink" title="五、其他补充要点"></a>五、其他补充要点</h3><p>关于Transformer：</p><p>《<a href="https://zhuanlan.zhihu.com/p/54356280">BERT大火却不懂Transformer？读这一篇就够了</a>》</p><p>《<a href="https://arxiv.org/abs/1808.08946">why Self-Attention？A Targeted Evaluation of Neural Machine Translation Architectures</a>》</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/image-20201228200737059.png" alt="image-20201228200737059"></p><p>从语义特征提取能力来说，Transformer在这方面的能力非常显著地超过RNN和CNN（在考察语义类能力的任务WSD中，Transformer超过RNN和CNN大约4-8个绝对百分点），RNN和CNN两者能力差不太多。</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/image-20201228200913207.png" alt="image-20201228200913207"></p><p>原生CNN特征抽取器在这方面极为显著地弱于RNN和Transformer，Transformer微弱优于RNN模型(尤其在主语谓语距离小于13时)，能力由强到弱排序为Transformer &gt; RNN &gt;&gt; CNN; 但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer，所以综合看，可以认为Transformer和RNN在这方面能力差不太多，而CNN则显著弱于前两者。</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/image-20201228201042585.png" alt="image-20201228201042585"></p><p>Transformer综合能力要明显强于RNN和CNN（技术发展到现在阶段，BLEU绝对值提升1个点是很难的事情），而RNN和CNN看上去表现基本相当，貌似CNN表现略好一些。</p><p><img src="/2020/12/25/Week1-%E6%99%BA%E8%83%BD%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/image-20201228202302119.png" alt="image-20201228202302119"></p><p>Transformer Base最快，CNN次之，再次Transformer Big，最慢的是RNN。RNN比前两者慢了3倍到几十倍之间。</p><p><strong>Reference</strong></p><p>1、开课吧–人工智能对话系统week 1课件</p><p>2、<a href="https://easyai.tech/ai-definition/nlu/">自然语言理解 – NLU</a></p><p>3、<a href="https://zhuanlan.zhihu.com/p/51476362">一文看懂任务型对话系统中的状态追踪（DST）</a></p><p>3、<a href="https://easyai.tech/ai-definition/nlg/">自然语言生成 – NLG</a></p><p>4、<a href="https://www.pianshen.com/article/7389296829/">NLP实践：对话系统技术原理和应用</a></p><p>5、<a href="https://zhuanlan.zhihu.com/p/115019681">一起来看看最新的对话状态追踪(DST)模型</a></p><p>6、<a href="https://zhuanlan.zhihu.com/p/143221527">自然语言理解（NLU）是啥？</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 任务型对话 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 任务型对话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark集群的搭建</title>
      <link href="2020/11/21/Spark%E7%AC%94%E8%AE%B0-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
      <url>2020/11/21/Spark%E7%AC%94%E8%AE%B0-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="JDK安装与配置"><a href="#JDK安装与配置" class="headerlink" title="JDK安装与配置"></a>JDK安装与配置</h2><p>1、将JDK包解压至/usr/local目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf jdk-8u271-linux-x64.tar.gz -C /usr/local/</span><br></pre></td></tr></table></figure><p>2、配置环境变量，在/etc/profile或者用户的.bash_profile文件，在文件末尾处添加路径变量如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#JAVA Environment</span></span></span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_271</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib/dt.jar:$&#123;JAVA_HOME&#125;/lib/tools.jar</span><br><span class="line">export PATH=$PATH:$&#123;JAVA_HOME&#125;/bin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ---------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_271</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib:$CLASSPATH</span><br><span class="line">export JAVA_PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JRE_HOME&#125;/bin</span><br><span class="line">export PATH=$PATH:$&#123;JAVA_PATH&#125;</span><br></pre></td></tr></table></figure><ul><li>你要将/usr/local/jdk1.8.0_271改为你的jdk安装目录；</li><li>linux下用冒号“:”来分隔路径；</li><li> ${PATH}、${CLASSPATH}、${JAVA_HOME}是用来引用原来的环境变量的值，在设置环境变量时特别要注意不能把原来的值给覆盖掉了，这是一种常见的错误；</li><li>CLASSPATH中当前目录“.”不能丢,把当前目录丢掉也是常见的错误；</li><li>export是把这三个变量导出为全局变量；</li><li>大小写必须严格区分。</li></ul><p>3、让修改的环境变量生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="设置SSH无密码登录"><a href="#设置SSH无密码登录" class="headerlink" title="设置SSH无密码登录"></a>设置SSH无密码登录</h2><p>Hadoop是由很多台服务器所组成的，当启动Hadoop系统时，NameNode必须与DataNode连接并管理这些节点（DataNode）。此时系统会要求我们输入密码，为了让系统顺利运行而不需要手动输入密码，需要将SSH设置成为无密码登录。（无密码登录是以事先交换的SSH Key秘钥来进行身份登录）。Hadoop使用SSH（Secure Shell）连接，目前是最可靠、专为远程登录其他服务器提供的安全性协议。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、检查ssh和rsync是否安装</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ssh协议，rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件</span></span><br><span class="line">rpm -qa | grep openssh</span><br><span class="line">rpm -qa | grep rsync</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、若显示没有，则安装SSH和rsync</span></span><br><span class="line">dnf install ssh</span><br><span class="line">dnf install rsync</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">------------------------------------------------</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、配置Master节点免密登录本机、所有的Slave</span></span><br><span class="line"><span class="meta">#</span><span class="bash">------------------------------------------------</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (3.1)、产生SSH Key秘钥进行后续身份验证(可简写为ssh-keygen -t rsa -P <span class="string">&#x27;&#x27;</span>)，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成的密钥对：id_rsa和id_rsa.pub，默认存储在<span class="string">&quot;/root/.ssh&quot;</span>目录下。</span></span><br><span class="line">ssh-keygen -t dsa -P &#x27;&#x27; -f ~/.ssh/id_dsa</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认在 ~/.ssh目录生成两个文件：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> id_rsa ：私钥</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> id_rsa.pub ：公钥</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将产生的Key放置到许可证文件中</span></span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> (3.2)、对于CentOS7.2及早期版本，（CentOS7.3及以后的版本跳过此步骤）；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改ssh配置文件<span class="string">&quot;/etc/ssh/sshd_config&quot;</span>的下列内容，将以下内容的注释去掉：</span></span><br><span class="line"></span><br><span class="line">RSAAuthentication yes # 启用 RSA 认证</span><br><span class="line">PubkeyAuthentication yes # 启用公钥私钥配对认证方式</span><br><span class="line">AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启ssh服务，才能使刚才设置有效。</span></span><br><span class="line">service sshd restart</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> (3.3)、对于CentOS7.3以后的版本，在服务器上更改权限；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (很重要，如果不这么设置，就是不让你免密登录)</span></span><br><span class="line">chmod 700 ~/.ssh</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> (3.4)、验证无密码登录本机是否成功。</span></span><br><span class="line">ssh localhost</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> (3.5)、导入要免密码登录的Slave服务器</span></span><br><span class="line"><span class="meta">#</span><span class="bash">首先将公钥复制到服务器</span></span><br><span class="line">scp ~/.ssh/id_rsa.pub xxx@host:/home/id_rsa.pub</span><br><span class="line"><span class="meta">#</span><span class="bash">然后，将公钥导入到认证文件(这一步的操作在服务器上进行)</span></span><br><span class="line">cat /home/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"><span class="meta">#</span><span class="bash">最后在本地登录服务器</span></span><br><span class="line">ssh -v hostname@hostip</span><br></pre></td></tr></table></figure><h2 id="Hadoop集群环境搭建"><a href="#Hadoop集群环境搭建" class="headerlink" title="Hadoop集群环境搭建"></a>Hadoop集群环境搭建</h2><h3 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建议读者专门建立一个存放应用程序安装包文件夹，本文使用的是/opt，进入/opt目录下</span></span><br><span class="line">cd /opt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载Hadoop： http://hadoop.apache.org/</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.5/hadoop-2.10.1.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压缩</span></span><br><span class="line">tar -zxvf hadoop-2.10.1.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移动hadoop到/usr/<span class="built_in">local</span>/hadoop</span></span><br><span class="line">mv hadoop-2.10.1/* /usr/local/hadoop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看hadoop安装目录/usr/<span class="built_in">local</span>/hadoop-2.10.1hadoop</span></span><br><span class="line">ll /usr/local/hadoop</span><br></pre></td></tr></table></figure><h3 id="配置Hadoop环境变量"><a href="#配置Hadoop环境变量" class="headerlink" title="配置Hadoop环境变量"></a>配置Hadoop环境变量</h3><p>在/etc/profile或者~/.bashrc添加文件内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置JDK安装路径</span></span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_271</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置HADOOP_HOME为Hadoop的安装路径/usr/<span class="built_in">local</span>/hadoop</span></span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop-2.10.1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置PATH,这样在其他目录时仍然可以运行Hadoop</span></span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Hadoop其他环境变量</span></span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export YARN_HOME=$HADOOP_HOME</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 链接库的相关设置</span></span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;</span><br><span class="line">export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY</span><br></pre></td></tr></table></figure><p>让~/.bashrc生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="修改Hadoop配置设置文件"><a href="#修改Hadoop配置设置文件" class="headerlink" title="修改Hadoop配置设置文件"></a>修改Hadoop配置设置文件</h3><p>主要包括：hadoop-env.sh、core-site.xml、yarn-site.xml、mapred-site.xml、hdfs-site.xml</p><p>（1）设置hadoop-env.sh文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hadoop-2.10.1/etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure><p>修改JAVA_HOME</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_271</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 其中有博客文章配置的变量为：(https://www.cnblogs.com/nswdxpg/p/8526920.html)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_271</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-2.10.1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">&quot;-Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib&quot;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_CONF_DIR:-&quot;/etc/hadoop&quot;&#125;</span></span></span><br></pre></td></tr></table></figure><p>（2）设置core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hadoop-2.10.1/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure><p>设置HDFS的默认名称:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这里需要注意的是：使用 fs.default.name 还是使用 fs.defaultFS ，要首先判断是否开启了 NN 的HA (namenode 的 highavaliable)，如果开启了nn ha，那么就用fs.defaultFS，在单一namenode的情况下，就用 fs.default.name , 如果在单一namenode节点的情况使用 fs.defaultFS ，系统将报错误如下：</p><blockquote><p>ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.<br>java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:/// has no authority.</p></blockquote><p>（3）设置yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hadoop-2.10.1/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><p>在之间输入：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）设置mapred-site.xml<br>mapred-site.xml用于设置监控Map与Reduce程序的JobTracker任务分配情况以及TaskTracker任务运行情况。Hadoop提供了设置的模板文件，可以自行复制修改。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/hadoop-2.10.1/etc/hadoop/mapred-site.xml.template /usr/local/hadoop-2.10.1/etc/hadoop/mapred-site.xml</span><br><span class="line"></span><br><span class="line">nano /usr/local/hadoop-2.10.1/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure><p>在之间输入：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（5）hdfs-site.xml<br>hdfs-site.xml用于设置HDFS分布式文件系统.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hadoop-2.10.1/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure><p>在之间输入,默认的blocks副本备份数量是每一个文件在其他node的备份数量，默认值为3。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定hdfs保存数据的副本数量--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!--指定hdfs中namenode的存储位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop-2.10.1/hadoop_data/hdfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!--指定hdfs中datanode的存储位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop-2.10.1/hadoop_data/hdfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="创建并格式化HDFS目录"><a href="#创建并格式化HDFS目录" class="headerlink" title="创建并格式化HDFS目录"></a>创建并格式化HDFS目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建namenode数据存储目录</span></span><br><span class="line">mkdir -p /usr/local/hadoop-2.10.1/hadoop_data/hdfs/namenode</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建datanode数据存储目录</span></span><br><span class="line">mkdir -p /usr/local/hadoop-2.10.1/hadoop_data/hdfs/datanode</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将Hadoop的所有者更改为root</span></span><br><span class="line">chown root:root -R /usr/local/hadoop-2.10.1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 格式化HDFS，如果HDFS已有数据，格式化操作会删除所有的数据</span></span><br><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure><h3 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h3><p>分别启动HDFS和YARN，使用start-dfs.sh(启动HDFS)和使用start-yarn.sh(启动YARN)<br>同时启动HDFS和YARN，使用start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动HDFS</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> start-dfs.sh</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动YARN</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> start-yarn.sh</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 或者同时启动HDFS和YARN</span></span><br><span class="line">start-all.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用jps查看已经启动的进程,查看NameNode和DataNode进程是否启动</span></span><br><span class="line">jps</span><br></pre></td></tr></table></figure><p>HDFS功能：NameNode、SecondaryNameNode和DataNode<br>YARN功能：ResourceManager、NodeManager</p><h3 id="Hadoop-ResourceManager-Web界面查看Hadoop运行状态：node、application和status"><a href="#Hadoop-ResourceManager-Web界面查看Hadoop运行状态：node、application和status" class="headerlink" title="Hadoop ResourceManager Web界面查看Hadoop运行状态：node、application和status."></a>Hadoop ResourceManager Web界面查看Hadoop运行状态：node、application和status.</h3><p><a href="http://master_ip:8088/">http://master_ip:8088</a></p><p>在VMware虚拟的CentOS8中，宿主机并不能访问虚拟机的IP端口，但是能ping得通，centos7 防火墙由firewalld管理，service firewalld stop关闭防火墙即可。</p><p>参考：</p><p>1、<a href="https://blog.csdn.net/weixin_40814247/article/details/95042886">https://blog.csdn.net/weixin_40814247/article/details/95042886</a></p><p>2、<a href="https://blog.csdn.net/suibianshen2012/article/details/47616095">win7无法访问虚拟机中的hadoop2.x的web管理界面</a></p><h3 id="Hadoop集群的搭建"><a href="#Hadoop集群的搭建" class="headerlink" title="Hadoop集群的搭建"></a>Hadoop集群的搭建</h3><p>前面我们只用了一台机器，实际生产中，不可能这样操作，我们的通常做法是：<br>1、有一台主要的计算机master，在HDFS担任NameNode的角色，在YARN担任ResourceManager角色。【注：通常会安排一台机器当做SecondaryNameNode使用，像上文中的<a href="http://192.168.111.227:8088/cluster%E4%B8%80%E6%A0%B7%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%BE%85%E5%8A%A9%E8%8A%82%E7%82%B9%E3%80%82%E3%80%91">http://192.168.111.227:8088/cluster一样，就是辅助节点。】</a><br>2、有多台机器data1、data2、data3，在HDFS担任DataNode角色，在YARN担任NodeManager角色。</p><p><img src="/2020/11/21/Spark%E7%AC%94%E8%AE%B0-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/Hadoop%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84.png" alt="img"></p><p><strong>Reference：</strong></p><p>1、<a href="https://www.cnblogs.com/xuliangxing/p/7428382.html">Ubuntu下SSH无密码验证配置</a></p><p>2、<a href="https://www.cnblogs.com/Leroscox/p/9627809.html">CentOS7.4配置SSH登录密码与密钥身份验证踩坑</a></p><p>3、<a href="https://www.cnblogs.com/smoggy/p/12733453.html">本地CentOS8 配置ssh免密登录服务器</a></p><p>2、<a href="https://www.cnblogs.com/xuliangxing/p/7234014.html">Spark学习笔记–Linux安装Spark集群详解</a></p><p>1、<a href="https://www.zhihu.com/question/20458233">推荐系统中的A/B测试，谁能详细说下思路？</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark笔记-实时数据流计算引擎Flink和Spark剖析</title>
      <link href="2020/11/19/Spark%E7%AC%94%E8%AE%B0-%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8EFlink%E5%92%8CSpark%E5%89%96%E6%9E%90/"/>
      <url>2020/11/19/Spark%E7%AC%94%E8%AE%B0-%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8EFlink%E5%92%8CSpark%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p><strong>Reference：</strong></p><p>1、<a href="https://zhuanlan.zhihu.com/p/210826594">实时数据流计算引擎Flink和Spark剖析</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark笔记-SparkStreaming整合Kafka指南</title>
      <link href="2020/11/19/Spark%E7%AC%94%E8%AE%B0-SparkStreaming%E6%95%B4%E5%90%88Kafka%E6%8C%87%E5%8D%97/"/>
      <url>2020/11/19/Spark%E7%AC%94%E8%AE%B0-SparkStreaming%E6%95%B4%E5%90%88Kafka%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p><strong>Reference：</strong></p><p>1、<a href="https://blog.csdn.net/a805814077/article/details/106531020">Spark Streaming整合Kafka指南</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据库逻辑设计三范式</title>
      <link href="2020/10/30/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1%E4%B8%89%E8%8C%83%E5%BC%8F/"/>
      <url>2020/10/30/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1%E4%B8%89%E8%8C%83%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="一、第一范式"><a href="#一、第一范式" class="headerlink" title="一、第一范式"></a>一、第一范式</h2><p>1NF是对属性的**<code>原子性</code>**，要求属性具有原子性，不可再分解；</p><blockquote><p>表：字段1、 字段2(字段2.1、字段2.2)、字段3 ……</p></blockquote><p>如学生（学号，姓名，性别，出生年月日），如果认为最后一列还可以再分成（出生年，出生月，出生日），它就不是一范式了，否则就是；</p><h2 id="二、第二范式"><a href="#二、第二范式" class="headerlink" title="二、第二范式"></a>二、第二范式</h2><p>2NF是对记录的**<code>惟一性</code>**，要求记录有惟一标识，即实体的惟一性，即不存在部分依赖；</p><blockquote><p>表：学号、课程号、姓名、学分;</p></blockquote><p>这个表明显说明了两个事务：学生信息， 课程信息；由于非主键字段必须依赖主键，这里<strong>学分依赖课程号</strong>，<strong>姓名依赖与学号</strong>，所以不符合二范式。</p><p><strong>可能会存在问题：</strong></p><ul><li><code>数据冗余:</code>，每条记录都含有相同信息；</li><li><code>删除异常：</code>删除所有学生成绩，就把课程信息全删除了；</li><li><code>插入异常：</code>学生未选课，无法记录进数据库；</li><li><code>更新异常：</code>调整课程学分，所有行都调整。</li></ul><p><strong>正确做法:</strong><br>学生：<code>Student</code>(学号, 姓名)；<br>课程：<code>Course</code>(课程号, 学分)；<br>选课关系：<code>StudentCourse</code>(学号, 课程号, 成绩)。</p><h2 id="三、第三范式"><a href="#三、第三范式" class="headerlink" title="三、第三范式"></a>三、第三范式</h2><p>3NF是对字段的**<code>冗余性</code>**，要求任何字段不能由其他字段派生出来，它要求字段没有冗余，即不存在传递依赖；</p><blockquote><p>表: 学号, 姓名, 年龄, 学院名称, 学院电话</p></blockquote><p>因为存在<strong>依赖传递</strong>: (学号) → (学生)→(所在学院) → (学院电话) 。</p><p><strong>可能会存在问题：</strong></p><ul><li><code>数据冗余:</code>有重复值；</li><li><code>更新异常：</code>有重复的冗余信息，修改时需要同时修改多条记录，否则会出现<strong>数据不一致的情况</strong> 。</li></ul><p><strong>正确做法：</strong></p><p>学生：(学号, 姓名, 年龄, 所在学院)；</p><p>学院：(学院, 电话)。</p><h2 id="四、反范式化"><a href="#四、反范式化" class="headerlink" title="四、反范式化"></a>四、反范式化</h2><p><strong>一般说来，数据库只需满足第三范式（<code>3NF</code>）就行了。</strong></p><p>没有冗余的数据库设计可以做到。但是，没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是：在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，允许冗余，**<code>达到以空间换时间的目的</code>**。</p><p>【例】：有一张存放商品的基本表，“金额”这个字段的存在，表明该表的设计不满足第三范式，因为“金额”可以由“单价”乘以“数量”得到，说明“金额”是冗余字段。但是，增加“金额”这个冗余字段，可以提高查询统计的速度，这就是以空间换时间的作法。</p><p>在<code>Rose 2002</code>中，规定列有两种类型：<strong>数据列</strong>和<strong>计算列</strong>。“金额”这样的列被称为“计算列”，而“单价”和“数量”这样的列被称为“数据列”。</p><h2 id="五、范式化设计和反范式化设计的优缺点"><a href="#五、范式化设计和反范式化设计的优缺点" class="headerlink" title="五、范式化设计和反范式化设计的优缺点"></a>五、范式化设计和反范式化设计的优缺点</h2><h3 id="5-1-范式化"><a href="#5-1-范式化" class="headerlink" title="5.1 范式化"></a>5.1 范式化</h3><p><strong>优点：</strong></p><p><img src="/2020/10/30/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1%E4%B8%89%E8%8C%83%E5%BC%8F/%E8%8C%83%E5%BC%8F%E5%8C%96%E4%BC%98%E7%82%B9.png" alt="clipboard.png"></p><p><strong>缺点：</strong></p><p><img src="/2020/10/30/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1%E4%B8%89%E8%8C%83%E5%BC%8F/%E8%8C%83%E5%BC%8F%E5%8C%96%E7%BC%BA%E7%82%B9.png" alt="clipboard.png"></p><h3 id="5-2-反范式化"><a href="#5-2-反范式化" class="headerlink" title="5.2 反范式化"></a>5.2 反范式化</h3><p><strong>优点：</strong></p><p><img src="/2020/10/30/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1%E4%B8%89%E8%8C%83%E5%BC%8F/%E5%8F%8D%E8%8C%83%E5%BC%8F%E5%8C%96%E4%BC%98%E7%82%B9.png" alt="clipboard.png"></p><p><strong>缺点：</strong></p><p><img src="/2020/10/30/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%BB%E8%BE%91%E8%AE%BE%E8%AE%A1%E4%B8%89%E8%8C%83%E5%BC%8F/%E5%8F%8D%E8%8C%83%E5%BC%8F%E5%8C%96%E7%BC%BA%E7%82%B9.png" alt="clipboard.png"></p><p><strong>参考资料如下：</strong></p><p>1、<a href="http://blog.csdn.net/taorui/article/details/2418761">通俗地理解数据库三个范式</a><br>2、<a href="http://blog.csdn.net/wuyanxiaxia/article/details/22933021">数据库模型设计，第一范式、第二范式、第三范式简单例子理解</a><br>3、<a href="http://blog.csdn.net/andywuchuanlong/article/details/25913235">数据库三大范式最简单的解释</a><br>4、<a href="https://segmentfault.com/a/1190000013695030">数据库逻辑设计之三大范式通俗理解</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark笔记-玩转SparkSQL</title>
      <link href="2020/10/20/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACSparkSQL/"/>
      <url>2020/10/20/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACSparkSQL/</url>
      
        <content type="html"><![CDATA[<h2 id="DataFrames"><a href="#DataFrames" class="headerlink" title="DataFrames"></a>DataFrames</h2><h3 id="Generate"><a href="#Generate" class="headerlink" title="Generate"></a>Generate</h3><p>创建一个RDD对象stringRDD，然后通过spark.read.json将stringRDD转换为DataFrame。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">stringJSONRDD = sc.parallelize((<span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">  &#123; &quot;id&quot;: &quot;123&quot;,</span></span><br><span class="line"><span class="string">    &quot;name&quot;: &quot;Katie&quot;,</span></span><br><span class="line"><span class="string">    &quot;age&quot;: 19,</span></span><br><span class="line"><span class="string">    &quot;eyeColor&quot;: &quot;brown&quot;</span></span><br><span class="line"><span class="string">  &#125;&quot;&quot;&quot;</span>,</span><br><span class="line">   <span class="string">&quot;&quot;&quot;&#123;</span></span><br><span class="line"><span class="string">    &quot;id&quot;: &quot;234&quot;,</span></span><br><span class="line"><span class="string">    &quot;name&quot;: &quot;Michael&quot;,</span></span><br><span class="line"><span class="string">    &quot;age&quot;: 22,</span></span><br><span class="line"><span class="string">    &quot;eyeColor&quot;: &quot;green&quot;</span></span><br><span class="line"><span class="string">  &#125;&quot;&quot;&quot;</span>, </span><br><span class="line">  <span class="string">&quot;&quot;&quot;&#123;</span></span><br><span class="line"><span class="string">    &quot;id&quot;: &quot;345&quot;,</span></span><br><span class="line"><span class="string">    &quot;name&quot;: &quot;Simone&quot;,</span></span><br><span class="line"><span class="string">    &quot;age&quot;: 23,</span></span><br><span class="line"><span class="string">    &quot;eyeColor&quot;: &quot;blue&quot;</span></span><br><span class="line"><span class="string">  &#125;&quot;&quot;&quot;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create DataFrame</span></span><br><span class="line">swimmersJSON = spark.read.json(stringJSONRDD)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create temporary table</span></span><br><span class="line">swimmersJSON.createOrReplaceTempView(<span class="string">&quot;swimmersJSON&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame API</span></span><br><span class="line">swimmersJSON.show()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">|age|eyeColor| id|   name|</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">| 19|   brown|123|  Katie|</span></span><br><span class="line"><span class="string">| 22|   green|234|Michael|</span></span><br><span class="line"><span class="string">| 23|    blue|345| Simone|</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL Query</span></span><br><span class="line">spark.sql(<span class="string">&quot;select * from swimmersJSON where age=19&quot;</span>).collect()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[Row(age=19, eyeColor=&#x27;brown&#x27;, id=&#x27;123&#x27;, name=&#x27;Katie&#x27;)]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the schema</span></span><br><span class="line">swimmersJSON.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">root</span></span><br><span class="line"><span class="string"> |-- age: long (nullable = true)</span></span><br><span class="line"><span class="string"> |-- eyeColor: string (nullable = true)</span></span><br><span class="line"><span class="string"> |-- id: string (nullable = true)</span></span><br><span class="line"><span class="string"> |-- name: string (nullable = true)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="Programmatically-Specifying-the-Schema"><a href="#Programmatically-Specifying-the-Schema" class="headerlink" title="Programmatically Specifying the Schema"></a>Programmatically Specifying the Schema</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate our own CSV data </span></span><br><span class="line"><span class="comment">#   This way we don&#x27;t have to access the file system yet.</span></span><br><span class="line">stringCSVRDD = sc.parallelize([(<span class="number">123</span>, <span class="string">&#x27;Katie&#x27;</span>, <span class="number">19</span>, <span class="string">&#x27;brown&#x27;</span>), (<span class="number">234</span>, <span class="string">&#x27;Michael&#x27;</span>, <span class="number">22</span>, <span class="string">&#x27;green&#x27;</span>), (<span class="number">345</span>, <span class="string">&#x27;Simone&#x27;</span>, <span class="number">23</span>, <span class="string">&#x27;blue&#x27;</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types</span></span><br><span class="line"><span class="comment"># schemaString = &quot;id name age eyeColor&quot;</span></span><br><span class="line">schema = StructType([</span><br><span class="line">    StructField(<span class="string">&quot;id&quot;</span>, LongType(), <span class="literal">True</span>),    </span><br><span class="line">    StructField(<span class="string">&quot;name&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;age&quot;</span>, LongType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;eyeColor&quot;</span>, StringType(), <span class="literal">True</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply the schema to the RDD and Create DataFrame</span></span><br><span class="line">swimmers = spark.createDataFrame(stringCSVRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creates a temporary view using the DataFrame</span></span><br><span class="line">swimmers.createOrReplaceTempView(<span class="string">&quot;swimmers&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h3><p>Notice that we’re no longer using <code>sqlContext.read...</code> but instead <code>spark.read...</code>. This is because as part of Spark 2.0, <code>HiveContext</code>, <code>SQLContext</code>, <code>StreamingContext</code>, <code>SparkContext</code> have been merged together into the Spark Session <code>spark</code>.</p><ul><li>Entry point for reading data</li><li>Working with metadata</li><li>Configuration</li><li>Cluster resource management</li></ul><p>For more information, please refer to <a href="http://bit.ly/2br0Fr1">How to use SparkSession in Apache Spark 2.0</a> (<a href="http://bit.ly/2br0Fr1">http://bit.ly/2br0Fr1</a>).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;D:\\Programs\\Spark\\spark-2.4.5-bin-hadoop2.7\\python&quot;</span>)</span><br><span class="line"></span><br><span class="line">sc.stop()</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">sc = SparkContext(<span class="string">&#x27;local&#x27;</span>, <span class="string">&#x27;sparkApp&#x27;</span>)</span><br><span class="line">print(sc.version)</span><br><span class="line"></span><br><span class="line">spark = SparkSession(sc)</span><br></pre></td></tr></table></figure><h3 id="Querying-with-SQL"><a href="#Querying-with-SQL" class="headerlink" title="Querying with SQL"></a>Querying with SQL</h3><p>With DataFrames, you can start writing your queries using <code>Spark SQL</code> - a SQL dialect that is compatible with the Hive Query Language (or HiveQL).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Query id and age for swimmers with age = 22 via DataFrame API,and Get count of rows in SQL</span></span><br><span class="line">spark.sql(<span class="string">&quot;select * from swimmers&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;select count(1) from swimmers&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">&quot;select id, age from swimmers where age = 22&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;select name, eyeColor from swimmers where eyeColor like &#x27;b%&#x27;&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Query id and age for swimmers with age = 22 via DataFrame API</span></span><br><span class="line">swimmers.select(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;age&quot;</span>).<span class="built_in">filter</span>(<span class="string">&quot;age = 22&quot;</span>).show()</span><br></pre></td></tr></table></figure><h3 id="Querying-with-the-DataFrame-API"><a href="#Querying-with-the-DataFrame-API" class="headerlink" title="Querying with the DataFrame API"></a>Querying with the DataFrame API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">swimmers.show()</span><br><span class="line">display(swimmers)</span><br><span class="line">swimmers.count()</span><br><span class="line"></span><br><span class="line">swimmers.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;eyeColor&quot;</span>).<span class="built_in">filter</span>(<span class="string">&quot;eyeColor like &#x27;b%&#x27;&quot;</span>).show()</span><br></pre></td></tr></table></figure><h3 id="DataFrame-Queries"><a href="#DataFrame-Queries" class="headerlink" title="DataFrame Queries"></a>DataFrame Queries</h3><p>let’s first build the DataFrames from the source datasets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set File Paths</span></span><br><span class="line">flightPerfFilePath = <span class="string">&quot;./flight-data/departuredelays.csv&quot;</span></span><br><span class="line">airportsFilePath = <span class="string">&quot;./flight-data/airport-codes-na.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obtain Airports dataset</span></span><br><span class="line">airports = spark.read.csv(airportsFilePath, header=<span class="string">&#x27;true&#x27;</span>, inferSchema=<span class="string">&#x27;true&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">airports.createOrReplaceTempView(<span class="string">&quot;airports&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Obtain Departure Delays dataset</span></span><br><span class="line">flightPerf = spark.read.csv(flightPerfFilePath, header=<span class="string">&#x27;true&#x27;</span>)</span><br><span class="line">flightPerf.createOrReplaceTempView(<span class="string">&quot;FlightPerformance&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cache the Departure Delays dataset </span></span><br><span class="line">flightPerf.cache()</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]</span></span><br></pre></td></tr></table></figure><p>Query Sum of Flight Delays by City and Origin Code (for Washington State)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">&quot;select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = &#x27;WA&#x27; group by a.City, f.origin order by sum(f.delay) desc&quot;</span>).show()</span><br></pre></td></tr></table></figure><p>Query Sum of Flight Delays by State (for the US)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">&quot;select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = &#x27;USA&#x27; group by a.State &quot;</span>).show()</span><br></pre></td></tr></table></figure><p>Note, you can make use of <code>%sql</code> within the notebook cells of a Databricks notebook</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%sql</span><br><span class="line">-- Query Sum of Flight Delays by State (<span class="keyword">for</span> the US)</span><br><span class="line">select a.State, <span class="built_in">sum</span>(f.delay) <span class="keyword">as</span> Delays</span><br><span class="line">  <span class="keyword">from</span> FlightPerformance f</span><br><span class="line">    join airports a</span><br><span class="line">      on a.IATA = f.origin</span><br><span class="line"> where a.Country = <span class="string">&#x27;USA&#x27;</span></span><br><span class="line"> group by a.State</span><br></pre></td></tr></table></figure><p>For more information, please refer to:</p><ul><li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#sql">Spark SQL, DataFrames and Datasets Guide</a></li><li><a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">PySpark SQL Module: DataFrame</a></li><li><a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions">PySpark SQL Functions Module</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> SparkSQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark笔记-玩转RDD操作</title>
      <link href="2020/10/14/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACRDD%E6%93%8D%E4%BD%9C/"/>
      <url>2020/10/14/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACRDD%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>RDD（Resilient Distributed Dataset）译作<strong>弹性分布式数据集</strong>，是Spark中最常用的数据抽象，是一个只可读、可分区、可并行计算的数据集合。RDD允许将工作集缓存在内存中进行复用，大大地提升了查询速度。</p></blockquote><h2 id="RDD简介"><a href="#RDD简介" class="headerlink" title="RDD简介"></a>RDD简介</h2><p>1、MapReduce 在面对日益复杂的业务逻辑时已经表现出严重的不足：<br>1.1）维护成本高昂，每一次数据处理都需要编写复杂的Map和Reduce步骤，中间某一步骤出错就要重试以处理异常；<br>1.2）难以上手，造成处理性能低；</p><p>2、因此人们提出用<font color="red"><strong>有向无环图（DAG）</strong></font>来抽象表达复杂的数据处理逻辑，各个数据处理步骤表示成图中的节点与边依赖关系，形成数据流的抽象表示，而把复杂的性能优化提交给后台自动处理；</p><p>3、RDD也即分布式对象集合，是一个<strong>只读</strong>的分区记录集合，每个RDD可以划分成多个<strong>分区</strong>，每个分区就是数据集的一部分，同时不同分区可以存储在集群中不同的节点上，从而利用集群节点优势进行并行计算；</p><p>4、RDD提供了丰富的操作以支持常见的数据处理，即<code>“转换”（Transformation）和“行动”（Action）</code>两种类型操作：</p><ul><li><font color="red"><strong>转换操作：指定RDD的依赖关系，通过接受RDD并返回RDD，即从一个RDD转换到另外一个RDD；</strong></font></li><li><font color="red"><strong>行动操作：执行计算并指定输出的形式，通过接受RDD返回输出值或结果（非RDD）；</strong></font></li></ul><p>5、通过Spark的API可以使用不同的语言调用RDD的操作，常见过程流程如下：</p><ul><li>从各种数据源创建RDD；</li><li>对RDD指定一系列的转换操作；</li><li>最后调用行动操作，输出结果或写入外部数据源；</li></ul><p>6、RDD操作的<font color="red"><strong>惰性机制</strong></font>，是指在RDD执行操作时，只有触发<code>行动操作</code>才会做真正的计算，而在行动前的所有转换操作都只是记录下相互的依赖关系，形成数据流的管道化（pipeline），而不会做真正的计算；</p><img src="/2020/10/14/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACRDD%E6%93%8D%E4%BD%9C/Spark-Scheduling-Process.png" alt="SparkRDD的执行过程" title="SparkRDD的执行过程"><h2 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h2><h3 id="通过数据集合转化为RDD"><a href="#通过数据集合转化为RDD" class="headerlink" title="通过数据集合转化为RDD"></a>通过数据集合转化为RDD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sc = SparkContext(<span class="string">&quot;local&quot;</span>, <span class="string">&quot;create_rdd&quot;</span>)</span><br><span class="line"></span><br><span class="line">ints = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line">rdd = sc.parallelize(ints)</span><br><span class="line"><span class="comment"># parallelize()函数将一个List列表转化为了一个RDD对象</span></span><br><span class="line"></span><br><span class="line">strings = [<span class="string">&#x27;spark&#x27;</span>,<span class="string">&#x27;hadoop&#x27;</span>,<span class="string">&#x27;rdd&#x27;</span>]</span><br><span class="line">rdd_1 = sc.parallelize(strings, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># parallelize()函数的第二个参数表示分区，默认是1，此处为2，表示将列表对应的RDD对象分为两个区。</span></span><br><span class="line"><span class="comment"># 后面的glom()函数就是要显示出RDD对象的分区情况，可以看出分了两个区，如果没有glom()函数，则不显示分区，如第一个结果所示。</span></span><br></pre></td></tr></table></figure><h3 id="从HDFS数据源或本地文件创建"><a href="#从HDFS数据源或本地文件创建" class="headerlink" title="从HDFS数据源或本地文件创建"></a>从HDFS数据源或本地文件创建</h3><p>从一篇CNN新闻报道 <a href="https://us.cnn.com/2019/04/11/tech/uber-lyft-businesses/index.html">Uber and Lyft may look the same, but their visions are not</a> 中抽取新闻主体并放到文件<code>news_sep.txt</code>中，一段话为一行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">At first blush, it can be hard to tell Uber and ...</span><br><span class="line">But look under the hood and there&#39;s a clear difference ...</span><br><span class="line">Uber filed paperwork on Thursday for what is expected ...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>通过指定文件路径读取为RDD：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sc = SparkContext(<span class="string">&quot;local&quot;</span>, <span class="string">&quot;create_rdd&quot;</span>)</span><br><span class="line"><span class="comment"># 从本地文件系统地址加载</span></span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;file:///path/to/news_sep.txt&quot;</span>)</span><br><span class="line"><span class="comment"># 从分布式文件系统HDFS地址加载，下面三种方式等价</span></span><br><span class="line"></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;hdfs://localhost:9000/path/to/news_sep.txt&quot;</span>)</span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;/path/to/news_sep.txt&quot;</span>)</span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;news_sep.txt&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="从其他数据库读取数据创建"><a href="#从其他数据库读取数据创建" class="headerlink" title="从其他数据库读取数据创建"></a>从其他数据库读取数据创建</h3><p>（待更新）</p><h3 id="使用数据流创建"><a href="#使用数据流创建" class="headerlink" title="使用数据流创建"></a>使用数据流创建</h3><p>结合流数据处理技术，如Spark Streaming、Kafka以及flume等，通过接收实时的输入数据流创建RDD。</p><h2 id="一般RDD的转换操作（Transformation）"><a href="#一般RDD的转换操作（Transformation）" class="headerlink" title="一般RDD的转换操作（Transformation）"></a>一般RDD的转换操作（Transformation）</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations">官方API文档</a>详细列出转换操作函数，下面简单介绍RDD常用的转换操作：</p><h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap()"></a>flatMap()</h3><p><code>flatMap(func)</code>：对于每一个输入元素，通过指定函数映射到0或多个元素，输出新的RDD。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将上述新闻文本的每一行每一段话根据空格进行分词</span></span><br><span class="line">rdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br></pre></td></tr></table></figure><h3 id="map"><a href="#map" class="headerlink" title="map()"></a>map()</h3><p><code>map(func)</code>：对于每一个输入元素，通过执行指定函数映射到唯一输出（1v1关系），产生新的RDD。</p><ul><li><p>map()类似于Python中的map，针对RDD对应的列表的每一个元素，进行map()函数里面的尼玛函数（这个函数是map函数的一个参数）对应的操作，返回的仍然是一个RDD对象；</p></li><li><p>reduce()则是针对RDD对应的列表中的元素，递归地选择第一个和第二个元素进行操作，操作的结果作为一个元素用来替换这两个元素，注意，reduce返回的是一个Python可以识别的对象，非RDD对象。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对上述分词结果的RDD中每一个单词去除标点符号，并转化为小写</span></span><br><span class="line">rdd = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: w.strip(<span class="string">&quot;\&quot;&quot;</span>).strip(<span class="string">&quot;,&quot;</span>).strip(<span class="string">&quot;.&quot;</span>).lower())</span><br></pre></td></tr></table></figure><h3 id="filter"><a href="#filter" class="headerlink" title="filter()"></a>filter()</h3><p><code>filter(func)</code>：是对RDD元素进行过滤，把经过指定函数后返回值为true的元素组成一个新的RDD。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对上述结果筛选掉一些常见的介词，如</span></span><br><span class="line">rm_words = [<span class="string">&quot;and&quot;</span>,<span class="string">&quot;in&quot;</span>,<span class="string">&quot;at&quot;</span>,<span class="string">&quot;a&quot;</span>,<span class="string">&quot;an&quot;</span>,<span class="string">&quot;is&quot;</span>,<span class="string">&quot;are&quot;</span>,<span class="string">&quot;may&quot;</span>,<span class="string">&quot;that&quot;</span>,<span class="string">&quot;this&quot;</span>,<span class="string">&quot;to&quot;</span>,<span class="string">&quot;as&quot;</span>,<span class="string">&quot;with&quot;</span>,<span class="string">&quot;of&quot;</span>,<span class="string">&quot;can&quot;</span>,<span class="string">&quot;be&quot;</span>]</span><br><span class="line">rdd = rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> w: w <span class="keyword">not</span> <span class="keyword">in</span> rm_words)</span><br></pre></td></tr></table></figure><h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct()"></a>distinct()</h3><p><code>distinct([numPartitions])</code>：对数据进行去重，返回一个新的RDD，numPartitions参数用于设置任务并行数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出新闻中所有提及过的单词</span></span><br><span class="line">rdd = rdd.distinct(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="sample"><a href="#sample" class="headerlink" title="sample()"></a>sample()</h3><p><code>sample(withReplacement,fraction,seed=None)</code>：对数据进行采样，withReplacement参数表示是否放回抽样；fraction参数表示抽样比例；seed表示随机种子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机抽取新闻提及部分单词</span></span><br><span class="line">rdd1 = rdd.sample(<span class="literal">False</span>, <span class="number">0.2</span>, <span class="number">2019</span>)</span><br></pre></td></tr></table></figure><h3 id="union"><a href="#union" class="headerlink" title="union()"></a>union()</h3><p><code>union(otherRDD)</code>：可以与另一个RDD数据集合并，合并后并不会去掉重复的数值，它返回一个新的RDD。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="string">&quot;union&quot;</span>,<span class="string">&quot;other&quot;</span>,<span class="string">&quot;rdd&quot;</span>])</span><br><span class="line">rdd = rdd.union(rdd1)</span><br></pre></td></tr></table></figure><h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection()"></a>intersection()</h3><p><code>intersection(otherRDD)</code>：可以与另一个RDD数据集进行求交集计算，返回新的RDD。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>])</span><br><span class="line">rdd3 = rdd1.intersection(rdd2)</span><br><span class="line"><span class="comment"># 最后rdd3的结果为 [3,4,5]</span></span><br></pre></td></tr></table></figure><h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract()"></a>subtract()</h3><p><code>subtract(otherRDD,[numPartitions])</code>：是对otherRDD进行减法操作，将原始RDD的元素减去新输入RDD的元素，将差值返回新RDD。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line">rdd3 = rdd1.subtract(rdd2)</span><br><span class="line"><span class="comment"># 最后rdd3的结果为 [1,3,5]</span></span><br></pre></td></tr></table></figure><h3 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian()"></a>cartesian()</h3><p><code>cartesian(otherRDD)</code>：可以对两个RDD数据集U，V求笛卡尔积，返回一个新的RDD数据集，其中每个元素为(u,v)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">rdd3 = rdd1.cartesian(rdd2)</span><br><span class="line"><span class="comment"># 最后rdd3的结果为 [(1,3),(1,4),(2,3),(2,4)]</span></span><br></pre></td></tr></table></figure><h2 id="键值对RDD的转换操作（Transformation）"><a href="#键值对RDD的转换操作（Transformation）" class="headerlink" title="键值对RDD的转换操作（Transformation）"></a>键值对RDD的转换操作（Transformation）</h2><h3 id="map-1"><a href="#map-1" class="headerlink" title="map()"></a>map()</h3><p><code>map(func)</code>：操作可以将一般RDD转换为键值对RDD，元素变成(K,V)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对上述新闻分词结果的RDD中每一个单词转化为(w,1)键值对</span></span><br><span class="line">rdd = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: (w,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey()"></a>reduceByKey()</h3><p><code>reduceByKey(func,[numPartitions])</code>：可以对具有相同键的值进行合并，返回一个新的键值对RDD，numPartitions用于设置任务并行数。</p><p>需要区分的是：<br>reduce()最终只返回一个值，reduceByKey()和reduceByKeyLocally()均是将Key相同的元素合并。<br>区别在于，reduce()和reduceByKeyLocally()函数均是将RDD转化为非RDD对象，而reduceByKey()将RDD对象转化为另一个RDD对象，需要collect()函数才能输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对新闻出现过的单词进行词频统计</span></span><br><span class="line">rdd = rdd.reduceByKey(<span class="keyword">lambda</span> w1,w2: w1+w2)</span><br></pre></td></tr></table></figure><h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey()"></a>groupByKey()</h3><p><code>groupByKey([numPartitions])</code>：可以对具有相同键的值进行分组，返回一个元素为(K,[Iterable])的键值对RDD，numPartitions用于指定任务并行数，默认为8。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对新闻出现过的单词进行词频统计</span></span><br><span class="line">rdd = rdd.groupByKey()</span><br></pre></td></tr></table></figure><h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey()"></a>aggregateByKey()</h3><p><code>aggregateByKey(zeroValue,seqFunc,combFunc,[numPartitions])</code>：可以对具有相同键的值进行聚合，把(K,V)键值对RDD转换为新的(K,U)键值对RDD，其中U由给定的combFunc和中立零值zeroValue聚合而成，U可以有与V不一致的形式；</p><ul><li>zeroValue 可以是0如果聚合的目的是求和，可以是List如果目的是对值进行统合，可以是Set如果目的是聚合唯一值；</li><li>seqFunc: (U,V) =&gt; U 对分区内的元素进行聚合（操作发生在每个分区内部）；</li><li>combFunc: (U,U) =&gt; U 对不同分区的聚合结果做进一步的聚合（操作发生在全部分区的聚合结果间）；</li><li>numPartitions 用于设置任务并行数；</li></ul><blockquote><p>为什么使用两个函数？见<a href="https://backtobazics.com/big-data/spark/apache-spark-aggregatebykey-example/">Apache Spark aggregateByKey Example</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同样对新闻出现过的单词进行词频统计</span></span><br><span class="line">rdd = rdd.aggregateByKey(<span class="number">0</span>,<span class="keyword">lambda</span> v1,v2: v1+v2, <span class="keyword">lambda</span> a1,a2: a1+a2)</span><br></pre></td></tr></table></figure><h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey()"></a>combineByKey()</h3><p><code>combineByKey(createCombiner,mergeValue,mergeCombiners,[numPartitions])</code>：可以对具有相同键的值按照自定义函数的逻辑进行聚合，把(K,V)键值对RDD转换为新的(K,U)键值对RDD，U可以有与V不一致的形式；</p><ul><li>createCombiner: V =&gt; C 创建新的聚合器方便后续步骤操作，对原始值进行附加操作并返回，跟flatMap()类似；</li><li>mergeValue: (C,V) =&gt; C 对分区内的元素进行聚合（操作发生在每个分区内部）；</li><li>mergeCombiners: (C,C) =&gt; C 对不同分区的聚合结果做进一步的聚合（操作发生在全部分区的聚合结果间）；</li><li>numPartitions 用于设置任务并行数；</li></ul><blockquote><p>groupByKey(), groupByKey(), aggregateByKey() 等都不同程度上依赖于 combineByKey() 操作</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对词频统计的结果求频数均值，例如</span></span><br><span class="line">strings = [<span class="string">&#x27;spark&#x27;</span>,<span class="string">&#x27;hadoop&#x27;</span>,<span class="string">&#x27;rdd&#x27;</span>,<span class="string">&#x27;rdd&#x27;</span>,<span class="string">&#x27;spark&#x27;</span>,<span class="string">&#x27;rdd&#x27;</span>]</span><br><span class="line">rdd = sc.parallelize(strings)</span><br><span class="line">rdd = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: (w,<span class="number">1</span>))</span><br><span class="line">rdd = rdd.reduceByKey(<span class="keyword">lambda</span> v1,v2: v1 + v2)</span><br><span class="line">rdd = rdd.combineByKey(<span class="keyword">lambda</span> v: (<span class="number">1</span>,v), <span class="keyword">lambda</span> c, v: (c[<span class="number">0</span>]+<span class="number">1</span>,c[<span class="number">1</span>]+v), <span class="keyword">lambda</span> c1, c2: (c1[<span class="number">0</span>]+c1[<span class="number">0</span>],c2[<span class="number">1</span>]+c2[<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># 最后的结果为 [(&#x27;spark&#x27;, (1, 2)), (&#x27;hadoop&#x27;, (1, 1)), (&#x27;rdd&#x27;, (1, 3))]</span></span><br></pre></td></tr></table></figure><h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey()"></a>sortByKey()</h3><p><code>sortByKey(ascending,[numPartitions])</code>：可以对键值对RDD按照键进行排序操作，其中K需要实现Ordered方法。</p><ul><li>ascending 决定RDD中的元素按升序还是降序排序，默认是True升序；</li><li>numPartitions 用于设置任务并行数；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对词频统计结果按降序排序（先把K-V值互换）</span></span><br><span class="line">rdd = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> v: (v[<span class="number">1</span>],v[<span class="number">0</span>]))</span><br><span class="line">rdd = rdd.sortByKey(<span class="literal">False</span>)</span><br><span class="line">rdd = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> v: (v[<span class="number">1</span>],v[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><h3 id="keys-与values"><a href="#keys-与values" class="headerlink" title="keys()与values()"></a>keys()与values()</h3><p><code>keys(),values()</code>：分别把键值对RDD的key和value返回形成一个新的RDD。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获得上一步排序结果的key或value</span></span><br><span class="line">rdd = rdd.keys()</span><br><span class="line">rdd = rdd.values()</span><br></pre></td></tr></table></figure><h3 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues()"></a>mapValues()</h3><p><code>mapValues(func)</code>：可以对键值对RDD每个元素的value加载到预定义函数进行操作，而不改变key。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如对词频统计结果值都减去1</span></span><br><span class="line">rdd = rdd.mapValues(<span class="keyword">lambda</span> v: v - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="join"><a href="#join" class="headerlink" title="join()"></a>join()</h3><p><code>join(otherRDD,[numPartitions])</code>：与关系数据库查询一样，表示内连接，给定两个键值对RDD如(K,V1)和(K,V2)，对于两个数据集都存在的key才对其输出，得到一个新的RDD，元素为(K,(V1,V2))。除此以外，还包括其他情形：</p><ul><li>fullOuterJoin(otherRDD,[numPartitions]) 全连接</li><li>leftOuterJoin(otherRDD,[numPartitions]) 左外连接</li><li>rightOuterJoin(otherRDD,[numPartitions]) 右外连接</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&#x27;spark&#x27;</span>,<span class="number">1</span>),(<span class="string">&#x27;hadoop&#x27;</span>,<span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&#x27;spark&#x27;</span>,<span class="number">1</span>)])</span><br><span class="line">rdd1 = rdd1.join(rdd2)</span><br><span class="line"><span class="comment"># 最后结果为 [(&#x27;spark&#x27;, (1,1))]</span></span><br></pre></td></tr></table></figure><h2 id="RDD的行动操作（Action）"><a href="#RDD的行动操作（Action）" class="headerlink" title="RDD的行动操作（Action）"></a>RDD的行动操作（Action）</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions">官方API文档</a>详细列出行动操作函数，下面简单介绍常用的行动操作：</p><h3 id="count"><a href="#count" class="headerlink" title="count()"></a>count()</h3><p><code>count()</code>：返回RDD数据集中元素的个数。</p><h3 id="collect"><a href="#collect" class="headerlink" title="collect()"></a>collect()</h3><p><code>collect()</code>：以数组的形式返回RDD数据集的所有元素。</p><h3 id="first"><a href="#first" class="headerlink" title="first()"></a>first()</h3><p><code>first()</code>：返回RDD数据集的第一个元素。</p><p>top()</p><p><code>top(num,key=None)</code>：以数组的形式返回RDD数据集的前num个元素，默认按<strong>降序</strong>，或者通过key函数指定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">24</span>,<span class="number">3</span>,<span class="number">12</span>,<span class="number">46</span>])</span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 输出 [46,24,12]</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">24</span>,<span class="number">3</span>,<span class="number">12</span>,<span class="number">46</span>])</span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>, key=<span class="built_in">str</span>))</span><br><span class="line"><span class="comment"># 输出 [5,46,3]</span></span><br></pre></td></tr></table></figure><h3 id="take"><a href="#take" class="headerlink" title="take()"></a>take()</h3><p><code>take(num)</code>：以数组的形式返回RDD数据集的前num个元素。</p><h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered()"></a>takeOrdered()</h3><p><code>takeOrdered(num,key=None)</code>：以数组的形式返回RDD数据集的前nu</p><p>m个元素，默认按<strong>升序</strong>排序，或者通过key函数指定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 输出 [1,2,3]</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">print(rdd.takeOrdered(<span class="number">3</span>, key=<span class="keyword">lambda</span> x: -x))</span><br><span class="line"><span class="comment"># 输出 [5,4,3]</span></span><br></pre></td></tr></table></figure><h3 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample()"></a>takeSample()</h3><p><code>takeSample(withReplacement,num,seed=None)</code>：对RDD数据集进行采样，并以数组的形式返回，withReplacement参数表示是否放回抽样；num参数表示抽样个数；seed表示随机种子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">print(rdd.takeSample(<span class="literal">False</span>,<span class="number">3</span>,seed=<span class="number">2019</span>))</span><br></pre></td></tr></table></figure><h3 id="lookup"><a href="#lookup" class="headerlink" title="lookup()"></a>lookup()</h3><p><code>lookup(key)</code>：以数组的形式返回键值对RDD中键为key的所有值，如果RDD数据集经过特定转换操作按照key进行了分区，那么此行动操作效率会很高。</p><h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach()"></a>foreach()</h3><p><code>foreach(func)</code>：将RDD数据集中的每个元素加载到指定函数进行操作，无返回值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对转换结果逐个输出</span></span><br><span class="line">rdd.foreach(<span class="built_in">print</span>)</span><br></pre></td></tr></table></figure><h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce()"></a>reduce()</h3><p><code>reduce(func)</code>：通过指定函数（如求和、统计）对RDD数据集元素进行聚合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对所有元素进行求和</span></span><br><span class="line"><span class="built_in">sum</span> = rdd.reduce(<span class="keyword">lambda</span> v1,v2: v1 + v2)</span><br></pre></td></tr></table></figure><h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate()"></a>aggregate()</h3><p><code>aggregate(zeroValue,seqOp,combOp)</code>：对RDD数据集的元素进行聚合，不要求返回值类型与RDD类型一致；</p><ul><li>zeroValue: U 给定初始值，形式与最终返回值U一致；</li><li>seqOp: (U,V) =&gt; U 对分区内的元素进行聚合（操作发生在每个分区内部）；</li><li>combOp: (U,U) =&gt; U 对不同分区的聚合结果做进一步的聚合（操作发生在全部分区的聚合结果间）；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求一个数组元素的均值</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">res = rdd.aggregate((<span class="number">0</span>,<span class="number">0</span>), <span class="keyword">lambda</span> u,v: (u[<span class="number">0</span>]+v,u[<span class="number">1</span>]+<span class="number">1</span>), <span class="keyword">lambda</span> u1,u2: (u1[<span class="number">0</span>]+u2[<span class="number">0</span>],u1[<span class="number">1</span>]+u2[<span class="number">1</span>]))</span><br><span class="line">print(res[<span class="number">0</span>]/res[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey()"></a>countByKey()</h3><p><code>countByKey()</code>：以字典的形式返回键值对RDD数据集中每个键的元素的统计数，即(K,count)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;rdd&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;rdd&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;spark&quot;</span>,<span class="number">2</span>)])</span><br><span class="line">res = rdd.countByKey()</span><br><span class="line"><span class="comment"># 输出结果 &#123;&#x27;rdd&#x27;: 2, &#x27;spark&#x27;: 1&#125;</span></span><br></pre></td></tr></table></figure><h3 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue()"></a>countByValue()</h3><p><code>countByValue()</code>：以字典的形式返回RDD数据集中每个元素的统计数，即(V,count)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">res = rdd.countByValue()</span><br><span class="line"><span class="comment"># 输出结果 &#123;2: 2, 3: 1, 1: 2&#125;</span></span><br></pre></td></tr></table></figure><h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile()"></a>saveAsTextFile()</h3><p><code>saveAsTextFile(path, compressionCodecClass=None)</code>：把RDD数据集保存为文本文件，并可以指定是否压缩。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f = NamedTemporaryFile(delete=<span class="literal">True</span>)</span><br><span class="line">f.close()</span><br><span class="line">codec = <span class="string">&quot;org.apache.hadoop.io.compress.GzipCodec&quot;</span></span><br><span class="line">sc.parallelize([<span class="string">&#x27;spark&#x27;</span>, <span class="string">&#x27;rdd&#x27;</span>]).saveAsTextFile(f.name, codec)</span><br></pre></td></tr></table></figure><h2 id="RDD的持久化"><a href="#RDD的持久化" class="headerlink" title="RDD的持久化"></a>RDD的持久化</h2><ul><li>由于RDD采用惰性机制，每次遇到行动操作都会根据DAG的依赖关系从头开始执行计算，如果遇到迭代计算，需要重复调用中间数据，会造成极大的计算开销；</li><li>可以通过持久化操作来解决以上的问题，用<code>persist()</code>方法对需要重复使用的RDD标记为持久化，当遇到第一次行动操作后，会把计算结果持久化，保存在计算节点的内存备用；</li></ul><h3 id="persist"><a href="#persist" class="headerlink" title="persist()"></a>persist()</h3><p><code>persist(storageLevel)</code>：storageLevel参数表示持久化级别，通过使用不同的级别可以把数据缓存到不同的位置，详见 <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence">RDD Persistence</a>；其中使用<code>cache()</code>函数会调用默认的持久化方法，即<code>persist(MEMORY_ONLY)</code>将RDD作为反序列化的对象存储在JVM中；而<code>unpersist()</code>方法则可以把持久化的RDD从缓存中删除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="string">&#x27;spark&#x27;</span>,<span class="string">&#x27;rdd&#x27;</span>,<span class="string">&#x27;hadoop&#x27;</span>])</span><br><span class="line">rdd.cache()</span><br><span class="line">print(rdd.take(<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 第一次行动操作，触发完整计算，同时把rdd放入缓存</span></span><br><span class="line"><span class="comment"># 输出 [&#x27;spark&#x27;]</span></span><br><span class="line">print(rdd.collect())</span><br><span class="line"><span class="comment"># 第二次行动操作，此时直接重复利用上述的缓存rdd</span></span><br><span class="line"><span class="comment"># 输出 [&#x27;spark&#x27;,&#x27;rdd&#x27;,&#x27;hadoop&#x27;]</span></span><br><span class="line">rdd.unpersist()</span><br></pre></td></tr></table></figure><p>**Reference: **</p><p>1、<a href="https://yxnchen.github.io/technique/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACRDD%E6%93%8D%E4%BD%9C/">Spark笔记-玩转RDD操作</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> RDD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>epoch-batchSize-iterations</title>
      <link href="2020/10/12/epoch-batchSize-iterations/"/>
      <url>2020/10/12/epoch-batchSize-iterations/</url>
      
        <content type="html"><![CDATA[<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>1、梯度下降是在机器学习中用于寻找最佳结果（曲线的最小值）的迭代优化算法。</p><p>2、梯度的含义是斜率或者斜坡的倾斜度；下降的含义是代价函数的下降。</p><p>3、算法是迭代的，意思是需要多次使用算法获取结果，以得到最优化结果。梯度下降的迭代性质能使欠拟合的图示演化以获得对数据的最佳拟合。</p><p><img src="/2020/10/12/epoch-batchSize-iterations/Gradient_Descent.png"></p><p>4、梯度下降中有一个称为学习率的参量。如上图左所示，刚开始学习率更大，因此下降步长更大。随着点下降，学习率变得越来越小，从而下降步长也变小。</p><p>同时，代价函数也在减小，或者说代价在减小，有时候也称为损失函数或者代价函数，两者都是一样的。（损失/代价的减小是一件好事）</p><p>5、只有在数据很庞大的时候（在机器学习中，几乎任何时候都是），我们才需要使用 epochs，batch size，迭代这些术语，在这种情况下，一次性将数据输入计算机是不可能的。因此，为了解决这个问题，我们需要把数据分成小块，一块一块的传递给计算机，在每一步的末端更新神经网络的权重，拟合给定的数据。</p><h3 id="EPOCH"><a href="#EPOCH" class="headerlink" title="EPOCH"></a>EPOCH</h3><p>当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个 epoch。</p><p>然而，当一个 epoch 对于计算机而言太庞大的时候，就需要把它分成多个小块。</p><h4 id="为什么要使用多于一个-epoch？"><a href="#为什么要使用多于一个-epoch？" class="headerlink" title="为什么要使用多于一个 epoch？"></a>为什么要使用多于一个 epoch？</h4><p>我知道这刚开始听起来会很奇怪，在神经网络中传递完整的数据集一次是不够的，而且我们需要<code>将完整的数据集在同样的神经网络中传递多次</code>。但是请记住，我们使用的是有限的数据集，并且我们使用一个迭代过程即梯度下降，优化学习过程和图示。因此仅仅更新权重一次或者说使用一个 epoch 是不够的。</p><p><img src="/2020/10/12/epoch-batchSize-iterations/Fitting.png"></p><p>随着 epoch 数量增加，神经网络中的权重的更新次数也增加，曲线从欠拟合变得过拟合。</p><p>那么，几个 epoch 才是合适的呢？</p><p>不幸的是，这个问题并没有正确的答案。对于不同的数据集，答案是不一样的。但是数据的多样性会影响合适的 epoch 的数量。比如，只有黑色的猫的数据集，以及有各种颜色的猫的数据集。</p><h3 id="BATCH-SIZE"><a href="#BATCH-SIZE" class="headerlink" title="BATCH SIZE"></a>BATCH SIZE</h3><p>一个 batch 中的样本总数。记住：batch size 和 number of batches 是不同的。</p><h4 id="BATCH-是什么？"><a href="#BATCH-是什么？" class="headerlink" title="BATCH 是什么？"></a>BATCH 是什么？</h4><p>在不能将数据一次性通过神经网络的时候，就需要将数据集分成几个 batch。</p><p>正如将这篇文章分成几个部分，如介绍、梯度下降、Epoch、Batch size 和迭代，从而使文章更容易阅读和理解。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">batch_size(int, optional):</span><br><span class="line">每个batch有多少个样本;</span><br><span class="line"></span><br><span class="line">shuffle(bool, optional): </span><br><span class="line">在每个epoch开始的时候，对数据进行重新排序;</span><br><span class="line"></span><br><span class="line">sampler(Sampler, optional): </span><br><span class="line">自定义从数据集中取样本的策略，如果指定这个参数，那么shuffle必须为False;</span><br><span class="line"></span><br><span class="line">batch_sampler(Sampler, optional): </span><br><span class="line">与sampler类似，但是一次只返回一个batch的indices（索引），需要注意的是，一旦指定了这个参数，那么batch_size,shuffle,sampler,drop_last就不能再制定了（互斥——Mutually exclusive）;</span><br><span class="line"></span><br><span class="line">num_workers (int, optional): </span><br><span class="line">这个参数决定了有几个进程来处理data loading。0意味着所有的数据都会被load进主进程。（默认为0）;</span><br><span class="line"></span><br><span class="line">collate_fn (callable, optional): </span><br><span class="line">将一个list的sample组成一个mini-batch的函数;</span><br><span class="line"></span><br><span class="line">pin_memory (bool, optional)： </span><br><span class="line">如果设置为True，那么data loader将会在返回它们之前，将tensors拷贝到CUDA中的固定内存（CUDA pinned memory）中;</span><br><span class="line"></span><br><span class="line">drop_last (bool, optional): </span><br><span class="line">如果设置为True：这个是对最后的未完成的batch来说的，比如你的batch_size设置为64，而一个epoch只有100个样本，那么训练的时候后面的36个就被扔掉了…如果为False（默认），那么会继续正常执行，只是最后的batch_size会小一点。</span><br><span class="line"></span><br><span class="line">timeout(numeric, optional): 如果是正数，表明等待从worker进程中收集一个batch等待的时间，若超出设定的时间还没有收集到，那就不收集这个内容了。这个numeric应总是大于等于0。默认为0;</span><br><span class="line"></span><br><span class="line">worker_init_fn (callable, optional): 每个worker初始化函数。</span><br></pre></td></tr></table></figure><h3 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h3><p>理解迭代，只需要知道乘法表或者一个计算器就可以了。迭代是 batch 需要完成一个 epoch 的次数。记住：在一个 epoch 中，batch 数和迭代数是相等的。</p><p>比如对于一个有 2000 个训练样本的数据集。将 2000 个样本分成大小为 500 的 batch，那么完成一个 epoch 需要 4 个 iteration。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark笔记-玩转SparkSession</title>
      <link href="2020/10/12/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACSparkSession/"/>
      <url>2020/10/12/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACSparkSession/</url>
      
        <content type="html"><![CDATA[<h3 id="关于SparkConf和SparkContext"><a href="#关于SparkConf和SparkContext" class="headerlink" title="关于SparkConf和SparkContext"></a>关于SparkConf和SparkContext</h3><p>1、每个Spark应用程序都需要一个Spark环境，这是Spark RDD API的主要入口点。Spark Shell提供了一个名为“sc”的预配置Spark环境和一个名为“spark”的预配置Spark会话；</p><p>2、任何Spark程序都是SparkContext开始的（SparkContext是允许驱动程序【spark drive】通过资源管理器访问集群）；</p><p>3、SparkContext的初始化需要一个SparkConf对象，<strong>SparkConf包含了Spark集群配置的各种参数；</strong></p><p>4、初始化后，就可以使用SparkContext对象所包含的各种方法来创建和操作RDD和共享变量。一旦设置完成SparkConf，就不可被使用者修改。</p><p><strong>Scala写法：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;master&quot;</span>).setAppName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 或者</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">&quot;master&quot;</span>,<span class="string">&quot;appName&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>Python写法：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&#x27;local&#x27;</span>).setAppName(<span class="string">&#x27;My App&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line">sc.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">sc = SparkContext(<span class="string">&#x27;local&#x27;</span>, <span class="string">&#x27;My APP&#x27;</span>)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h4 id="SQLContext-是什么"><a href="#SQLContext-是什么" class="headerlink" title="SQLContext 是什么?"></a>SQLContext 是什么?</h4><p>SQLContext是通往SparkSQL的入口。下面是如何使用SparkContext创建SQLContext。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sc is an existing SparkContext.</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br></pre></td></tr></table></figure><p>一旦有了SQLContext，就可以开始处理DataFrame、DataSet等。</p><h4 id="HiveContext-是什么"><a href="#HiveContext-是什么" class="headerlink" title="HiveContext 是什么?"></a>HiveContext 是什么?</h4><p>HiveContext是通往hive入口。<br>HiveContext具有SQLContext的所有功能。<br>实际上，如果查看API文档，就会发现HiveContext扩展了SQLContext，这意味着它支持SQLContext支持的功能以及更多(Hive特定的功能)。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveContext</span> <span class="keyword">extends</span> <span class="title">SQLContext</span> <span class="keyword">implements</span> <span class="title">Logging</span></span></span><br></pre></td></tr></table></figure><p>下面是如何使用SparkContext获得HiveContext。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sc is an existing SparkContext.</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span>(sc)</span><br></pre></td></tr></table></figure><h3 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h3><p>SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。</p><h4 id="SparkSession的简单示例"><a href="#SparkSession的简单示例" class="headerlink" title="SparkSession的简单示例"></a>SparkSession的简单示例</h4><p>SparkSession内部封装了SparkContext，所以计算实际上是由SparkContext完成的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder</span><br><span class="line">                    .master(<span class="string">&quot;master&quot;</span>)</span><br><span class="line">                    .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">                    .getOrCreate()</span><br><span class="line"><span class="comment">//或者SparkSession.builder.config(conf=SparkConf())</span></span><br></pre></td></tr></table></figure><p>上面代码类似于创建一个SparkContext，master设置为”master”，然后创建了一个SQLContext封装它。</p><h4 id="创建支持Hive的SparkSession"><a href="#创建支持Hive的SparkSession" class="headerlink" title="创建支持Hive的SparkSession"></a>创建支持Hive的SparkSession</h4><p>如果你想创建hiveContext，可以使用下面的方法来创建SparkSession，以使得它支持Hive(HiveContext)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val sparkSession = SparkSession.builder</span><br><span class="line">                    .master(<span class="string">&quot;master&quot;</span>)</span><br><span class="line">                    .appName(<span class="string">&quot;appName&quot;</span>)</span><br><span class="line">                    .enableHiveSupport()</span><br><span class="line">                    .getOrCreate()</span><br><span class="line">//sparkSession 从csv读取数据：</span><br><span class="line">val dq = sparkSession.read.option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>).csv(<span class="string">&quot;src/main/resources/scala.csv&quot;</span>)</span><br></pre></td></tr></table></figure><p>上面代码中的getOrCreate()方法表示有就拿过来，没有就创建，类似于单例模式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val s1 = SparkSession().builder.config(<span class="string">&quot;k1&quot;</span>, <span class="string">&quot;v1&quot;</span>).getOrCreat()</span><br><span class="line">val s2 = SparkSession().builder.config(<span class="string">&quot;k2&quot;</span>, <span class="string">&quot;v2&quot;</span>).getOrCreat()</span><br><span class="line"><span class="keyword">return</span> s1.conf.get(<span class="string">&quot;k1&quot;</span>) == s2.conf.get(<span class="string">&quot;k2&quot;</span>)</span><br></pre></td></tr></table></figure><p>上面代码类似于创建一个SparkContext，master设置为”master”，然后创建了一个SQLContext封装它。</p><p>**Reference: **</p><p>1、<a href="https://www.cnblogs.com/lillcol/p/11233456.html">SparkSession、SparkContext、SQLContext和HiveContext之间的区别。</a></p><p>2、<a href="https://www.cnblogs.com/Allen-rg/p/11364683.html">SparkConf和SparkContext</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> RDD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark笔记-Spark任务提交方式</title>
      <link href="2020/10/11/Spark%E7%AC%94%E8%AE%B0-Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/"/>
      <url>2020/10/11/Spark%E7%AC%94%E8%AE%B0-Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="Spark任务提交方式"><a href="#Spark任务提交方式" class="headerlink" title="Spark任务提交方式"></a>Spark任务提交方式</h2><h3 id="Standalone模式两种提交任务方式"><a href="#Standalone模式两种提交任务方式" class="headerlink" title="Standalone模式两种提交任务方式"></a>Standalone模式两种提交任务方式</h3><h4 id="Standalone-client提交方式"><a href="#Standalone-client提交方式" class="headerlink" title="Standalone-client提交方式"></a>Standalone-client提交方式</h4><p><strong>提交命令：</strong></p><p><code>...</code>代表spark安装目录</p><blockquote><p>./spark-submit –master spark://hadoop101:7077 –class org.apache.saprk.examples.SparkPi …/examlpes/jars/spark-examples_2.11-2.2.0.jar 100</p><p>./spark-submit –master spark://hadoop101:7077 –depoly-mode client –class org.apache.saprk.examples.SparkPi …/examlpes/jars/spark-examples_2.11-2.2.0.jar 100</p></blockquote><p><strong>执行流程：</strong><br>1、在客户端提交Spark应用程序，会在客户端启动Driver。<br>2、客户端向Master申请资源，Master找到资源返回。<br>3、Driver向worker节点发送task，监控task执行，回收结果。</p><p><strong>执行原理图解：</strong></p><p><img src="/2020/10/11/Spark%E7%AC%94%E8%AE%B0-Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/Standalone-client.png"></p><p><strong>总结：</strong><br>1、client方式提交任务，在客户端提交多个application，客户端会为每个application都启动一个Driver， Driver与集群Worker节点有大量通信，这样会造成客户端网卡流量激增。</p><p>2、在客户端可以看到task执行情况和计算结果。</p><p>3、client方式提交任务适用于程序测试，<code>不适用于真实生产环境</code>。</p><h4 id="Standalone-cluster提交任务方式"><a href="#Standalone-cluster提交任务方式" class="headerlink" title="Standalone-cluster提交任务方式"></a>Standalone-cluster提交任务方式</h4><p><strong>提交命令：</strong></p><blockquote><p>./spark-submit –master spark://hadoop101:7077 –depoly-mode cluster –class org.apache.saprk.examples.SparkPi …/examlpes/jars/spark-examples_2.11-2.2.0.jar 100</p></blockquote><p><strong>执行流程：</strong><br>1、客户端提交application，客户端首先向Master申请启动Driver。<br>2、Master收到请求之后，随机在一台Worker节点上启动Driver。<br>3、Driver启动之后，向Master申请资源，Master返回资源。<br>4、Driver发送task，监控task执行，回收结果。</p><p><strong>执行原理图解:</strong></p><p><img src="/2020/10/11/Spark%E7%AC%94%E8%AE%B0-Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/Standalone-cluster.png"></p><p><strong>总结：</strong><br>cluster方式提交任务，Driver在集群中的<code>随机</code>一台Worker节点上启动，分散了client方式的网卡流量激增问题。 cluster方式<code>适用于真实生产环境</code>，在客户端看不到task执行情况和执行结果，要去WEBUI中去查看。</p><h4 id="Driver的功能"><a href="#Driver的功能" class="headerlink" title="Driver的功能"></a>Driver的功能</h4><p>在standalone模式中Driver的功能</p><p>1、发送task</p><p>2、监控task执行，回收结果</p><p>3、申请资源  </p><h3 id="yarn模式两种提交任务方式"><a href="#yarn模式两种提交任务方式" class="headerlink" title="yarn模式两种提交任务方式"></a>yarn模式两种提交任务方式</h3><h4 id="yarn-client提交任务方式"><a href="#yarn-client提交任务方式" class="headerlink" title="yarn-client提交任务方式"></a>yarn-client提交任务方式</h4><p><strong>提交命令：</strong></p><blockquote><p>./spark-submit –master yarn  –class org.apache.saprk.examples.SparkPi …/examlpes/jars/spark-examples_2.11-2.2.0.jar 100</p><p>./spark-submit –master yarn-client –class org.apache.saprk.examples.SparkPi …/examlpes/jars/spark-examples_2.11-2.2.0.jar 100</p><p>./spark-submit –master yarn –depoly-mode client –class org.apache.saprk.examples.SparkPi …/examlpes/jars/spark-examples_2.11-2.2.0.jar 100</p></blockquote><p><strong>执行流程：</strong></p><ol><li>客户端提交application，Driver会在客户端启动</li><li>客户端向ResourceManager申请启动ApplicationMaster</li><li>ResourceManager收到请求之后，随机在一台NodeManager中启动ApplicationMaster</li><li>ApplicationMaster启动之后，向ResourceManager申请资源，用于启动Executor</li><li>ResourceManager收到请求之后，找到资源返回给ApplicationMaster一批NodeManager节点</li><li>ApplicationMaster连接NodeManager启动Executor</li><li>Executor启动之后会反向注册给Driver</li><li>Driver发送task到Executor执行，监控task的执行并回收结果   </li></ol><p><strong>执行原理图解：</strong></p><p><img src="/2020/10/11/Spark%E7%AC%94%E8%AE%B0-Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/Yarn-Client.png"></p><p><strong>总结：</strong></p><p>Yarn-client模式提交任务，Driver在客户端启动，当提交多分application，每个application的Driver都会在客户端启动，也会有网卡流量激增问题，这种模式适用于程序测试，**<code>不适用与生产环境</code>**，在客户端可以看到任务的执行和结果。</p><h4 id="Yarn-cluster提交任务方式"><a href="#Yarn-cluster提交任务方式" class="headerlink" title="Yarn-cluster提交任务方式"></a>Yarn-cluster提交任务方式</h4><p><strong>提交命令</strong></p><blockquote><p>./spark-submit –master yarn –depoly-mode cluster –class org.apache.saprk.examples.SparkPi …/examlpes/jars/spark-examples_2.11-2.2.0.jar 100</p><p>./spark-submit –master yarn-cluster –class org.apache.saprk.examples.SparkPi …/examlpes/jars/spark-examples_2.11-2.2.0.jar 100</p></blockquote><p><strong>执行原理图解</strong></p><p><img src="/2020/10/11/Spark%E7%AC%94%E8%AE%B0-Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/Yarn-cluster.png" alt="img"></p><p><strong>执行流程</strong></p><p>1、客户端提交Application,首先客户端向ResourceManager申请启动ApplicationMaster(AM)</p><p>2、ResourceManager收到请求之后，随机在一台NodeManager中启动ApplicationMaster,这里ApplicationMaster就相当于是Driver</p><p>3、ApplicationMaster启动之后，向ResourceManager申请资源，用于启动Executor</p><p>4、ResourceManager收到请求之后，找到资源返回给ApplicationMaster</p><p>5、ApplicationMaster连接NodeManager启动Executor</p><p>6、Executor启动之后会反向注册给ApplicationMaster(Driver)</p><p>7、ApplicationMaster(Driver)发送task到Executor执行</p><p><strong>总结</strong></p><p>Yarn-cluster模式提交任务，使用于生产环境，AM(Driver)****随机****在一台NM节点上启动，当提交多个application时，每个application的Driver会分散到集群中的NM中启动，相当于将Yarn-client模式的客户端网卡流量激增问题分散到集群中。在客户端看不到task执行和结果，要去WEBUI中查看。</p><h4 id="Application功能"><a href="#Application功能" class="headerlink" title="Application功能"></a>Application功能</h4><p>在Yarn模式下Application（Driver）的功能</p><ol><li>申请资源</li><li>启动executor</li><li>任务调度（包含上面Driver的功能）</li></ol><h4 id="术语解释"><a href="#术语解释" class="headerlink" title="术语解释"></a>术语解释</h4><ol><li>Master(Standalone)：资源管理的主节点（进程）</li><li>Cluster Manager：在集群上获取资源的外部服务（例如standalone、Mesos、Yarn）</li><li>Worker Node（standalone）：资源管理的从节点（进程），或者说管理本机资源的进程</li><li>Application：基于Spark的用户程序，包含了Driver程序和运行在集群上的executor程序</li><li>Driver Program：用来连接工作进程（Worker）的程序</li><li>Executor：是在一个worker进程所管理的节点上为某个application启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上。<strong>每个应用都有各自独立的executors</strong>。</li><li>Task：被送到某个executor上的工作单元</li><li>Job：包含恨到任务（Task）的并行计算，可以看作和action对应</li><li>Stage：一个job会被拆分很多组任务，每组任务被称为Stage（就像MapReduce分为map task和reduce task一样）</li></ol><h3 id="Spark任务调度和资源调度"><a href="#Spark任务调度和资源调度" class="headerlink" title="Spark任务调度和资源调度"></a><strong>Spark任务调度和资源调度</strong></h3><h4 id="spark资源调度和任务调度流程"><a href="#spark资源调度和任务调度流程" class="headerlink" title="spark资源调度和任务调度流程"></a>spark资源调度和任务调度流程</h4><p><img src="/2020/10/11/Spark%E7%AC%94%E8%AE%B0-Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/Spark%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%92%8C%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B.png" alt="img"></p><p>启动集群后，Worker节点会向Master节点汇报资源情况，Master掌握了集群资源情况。</p><p>当Spark提交一个Application后，根据RDD之间的依赖关系将Application形成一个DAG有向无环图。</p><p>任务提交后，Spark会在Driver端创建两个对象：DAGScheduler和TaskScheduler，DAGScheduler是任务调度的高层调度器，是一个对象。DAGScheduler的主要作用就是将DAG根据RDD之间的宽窄依赖关系划分为一个个的Stage，然后将这些Stage以TaskSet的形式提交给TaskScheduler（TaskScheduler是任务调度的低层调度器，这里TaskSet其实就是一个集合，里面封装的就是一个个的task任务,也就是stage中的并行度task任务），TaskSchedule会遍历TaskSet集合，拿到每个task后会将task发送到计算节点Executor中去执行（其实就是发送到Executor中的线程池ThreadPool去执行）。</p><blockquote><p>task在Executor线程池中的运行情况会向TaskScheduler反馈，当task执行失败时，则由TaskScheduler负责重试，将task重新发送给Executor去执行，默认重试3次。如果重试3次依然失败，那么这个task所在的stage就失败了。stage失败了则由DAGScheduler来负责重试，重新发送TaskSet到TaskSchdeuler，Stage默认重试4次。如果重试4次以后依然失败，那么这个job就失败了。job失败了，Application就失败了。（**<em>*</em>*Job=多个stage，Stage=多个同种task******）</p></blockquote><p>TaskScheduler不仅能重试失败的task,还会重试straggling（落后，缓慢）task（也就是执行速度比其他task慢太多的task）。如果有运行缓慢的task那么TaskScheduler会启动一个新的task来与这个运行缓慢的task执行相同的处理逻辑。两个task哪个先执行完，就以哪个task的执行结果为准。这就是Spark的推测执行机制。在Spark中推测执行默认是关闭的。推测执行可以通过spark.speculation属性来配置。</p><p><strong>注意：</strong></p><p>对于ETL(数据清洗)类型要入数据库的业务要关闭推测执行机制，这样就不会有重复的数据入库。</p><p>如果遇到数据倾斜的情况，开启推测执行则有可能导致一直会有task重新启动处理相同的逻辑，任务可能一直处于处理不完的状态。</p><p>推测执行机制默认是关闭的。</p><h4 id="图解Spark资源调度和任务调度的流程"><a href="#图解Spark资源调度和任务调度的流程" class="headerlink" title="图解Spark资源调度和任务调度的流程"></a><strong>图解Spark资源调度和任务调度的流程</strong></h4><p><img src="/2020/10/11/Spark%E7%AC%94%E8%AE%B0-Spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/%E5%9B%BE%E8%A7%A3Spark%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%92%8C%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B.png" alt="img"></p><h4 id="粗粒度资源申请和细粒度资源申请"><a href="#粗粒度资源申请和细粒度资源申请" class="headerlink" title="粗粒度资源申请和细粒度资源申请"></a>粗粒度资源申请和细粒度资源申请</h4><p><strong>粗粒度资源申请(Spark)</strong></p><p>获取到所有资源后才开始任务，执行完所有任务后释放资源；任务执行速度快，但是资源就无法充分利用。</p><p>在Application执行之前，将所有的资源申请完毕，当资源申请成功后，才会进行任务的调度，当所有的task执行完成后， 才会释放这部分资源。</p><p><strong>优点</strong>：在Application执行之前，所有的资源都申请完毕，每一个task直接使用资源就可以了，不需要task在执行前自己去申请资源，task启动就快了，task执行快了，stage执行就快了，job就快了，application执行就快了。</p><p><strong>缺点：</strong>直到最后一个task执行完成才会释放资源，集群的资源无法充分利用。</p><p>**细粒度资源申请（MapReduce) **</p><p>执行时每一个task去提前获取资源，执行完就释放资源；资源的利用率高，但是这样执行的效率就慢</p><p>Application执行之前不需要先去申请资源，而是直接执行，让job中的每一个task在执行前自己去申请资源，task执行完成就释放资源。</p><p><strong>优点：</strong>集群的资源可以充分利用。</p><p><strong>缺点：</strong>task自己去申请资源，task启动变慢，Application的运行就相应的变慢了。</p><p><strong>Reference</strong></p><p>1、<a href="https://blog.csdn.net/jiezou12138/article/details/88949947">spark 的任务提交方式</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> RDD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows安装Hadoop</title>
      <link href="2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/"/>
      <url>2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/</url>
      
        <content type="html"><![CDATA[<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>主机：Windows 10<br>Hadoop：2.7.7<br>Java：1.8.0_241</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><p>1、确认Java已经安装，并已经添加到环境变量中，测试方法：cmd中输入<code>java -version</code>，显示如下：</p><img src="/2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/JavaVersion.png" style="zoom:80%;"><p>2、下载 <a href="http://hadoop.apache.org/releases.html">hadoop binary</a>，解压后修改以下配置文件：</p><p>1）hadoop-2.7.1/etc/hadoop/hadoop-env.cmd，把<code>JAVA_HOME</code>的值修改为本机上jdk的路径，注意路径上不能有空格，jdk装在<code>Program File</code>文件夹下的同学可以这么写：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若JDK安装路径有空格</span></span><br><span class="line"><span class="comment"># set JAVA_HOME=C:\PROGRA~1\Java\jdk-9.0.1</span></span><br><span class="line"><span class="built_in">set</span> JAVA_HOME=D:\Programs\Java\jdk</span><br></pre></td></tr></table></figure><p>此时在hadoop根目录下的bin文件夹下，你可以输入<code>hadoop version</code>查看你的hadoop版本了。</p><img src="/2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/HadoopVersion.png" style="zoom:80%;"><p>2）hadoop-2.7.5/etc/hadoop/core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3）hadoop-2.7.5/etc/hadoop/mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>4）hadoop-2.7.1/etc/hadoop/hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>/d:/software/hadoop-2.7.5/data/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>/d:/software/hadoop-2.7.5/data/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意把<code>/d:/software/hadoop-2.7.5</code>替换成你的hadoop根目录</p><p>5）hadoop-2.7.5/etc/hadoop/yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3、 启动hadoop</p><p>1）在<code>bin</code>目录下输入<code>hadoop namenode -format</code>，应该能看到这样的结果：</p><img src="/2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/HadoopFormat.png" style="zoom:80%;"><p>2）在<code>sbin</code>目录下输入<code>start-all</code>，会有多个cmd窗口被创建，此时输入<code>jps</code>，应当看到如下结果：</p><img src="/2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/StartAll.png" style="zoom:80%;"><p>有时会遇到DataNode创建失败的情况，删除根目录下data/datanode文件夹再<code>start-all</code>解决问题。</p><p>3）在浏览器中输入<code>localhost:50070</code>应当能看到如下网页：</p><img src="/2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/port50070.png" style="zoom:80%;"><p>4）输入<code>localhost:8088</code>页面如下：</p><img src="/2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/port8088.png" style="zoom:80%;"><p>4、停止hadoop</p><p>在<code>bin</code>目录下输入<code>stop-all</code></p><h2 id="Wordcount运行"><a href="#Wordcount运行" class="headerlink" title="Wordcount运行"></a>Wordcount运行</h2><p>Hadoop基于HDFS(Hadoop Distributed File System)这一文件系统管理文件，使用指令<code>hadoop fs -op &lt;args&gt;</code>可以对HDFS进行操作。</p><p>1）我们先在HDFS中创建一个输入文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hdfs dfsadmin -safemode leave</span></span><br><span class="line">hadoop fs -mkdir /input</span><br></pre></td></tr></table></figure><p>当你发现直接-mkdir失败时，加上第一条里面的注释掉的内容就OK了。你可以通过<code>hadoop fs -ls /</code>查看HDFS根目录下的文件，应当已经包含了<code>/input</code>文件夹。你可以用<code>hdfs fs -rmr /input</code>删除这一文件夹。</p><p>2）在本地创建两个文本文件，上传到<code>/input</code>文件夹：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put *.txt /input</span><br></pre></td></tr></table></figure><p>这样我们输入就准备好了，确认一下：</p><img src="/2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/HadoopFS-ls.png" style="zoom:80%;"><p>3）运行示例jar包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /input /output</span><br></pre></td></tr></table></figure><p>一开始报错<code>createSymbolicError</code>，解决方案：运行-&gt;gpedit.msc-&gt;计算机配置-&gt;Windows设置-&gt;安全设置-&gt;本地策略-&gt;用户权限分配-&gt;创建符号链接，添加你的用户名。然后重启电脑，再运行就ok了。附上成功的截图：</p><img src="/2020/10/10/Windows%E5%AE%89%E8%A3%85Hadoop/port8088.png" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls &#x2F;output</span><br></pre></td></tr></table></figure><p>输出文件在output/part-r-00000，通过命令<code>hadoop fs -cat /output/part-r-00000</code>查看内容。</p><p><strong>Referencen:</strong></p><p>1、<a href="https://www.jianshu.com/p/faf038923093">Windows安装Hadoop</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github使用技巧</title>
      <link href="2020/10/09/github%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
      <url>2020/10/09/github%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h2 id="本地项目上传至github"><a href="#本地项目上传至github" class="headerlink" title="本地项目上传至github"></a>本地项目上传至github</h2><p>在安装好git的前提下：</p><h3 id="create-a-new-repository-on-the-command-line"><a href="#create-a-new-repository-on-the-command-line" class="headerlink" title="create a new repository on the command line"></a>create a new repository on the command line</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;# text&quot; &gt;&gt; README.md</span><br><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;first commit&quot;</span><br><span class="line">git branch -M main</span><br><span class="line">git remote add origin https:&#x2F;&#x2F;github.com&#x2F;marchboy&#x2F;text.git</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><h3 id="push-an-existing-repository-from-the-command-line"><a href="#push-an-existing-repository-from-the-command-line" class="headerlink" title="push an existing repository from the command line"></a>push an existing repository from the command line</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin https:&#x2F;&#x2F;github.com&#x2F;marchboy&#x2F;text.git</span><br><span class="line">git branch -M main</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure><p>利用 GitHub 强大的搜索功能，增加几个搜索参数即可轻松找到「目标人物」。</p><h2 id="搜作者"><a href="#搜作者" class="headerlink" title="搜作者"></a>搜作者</h2><table><thead><tr><th>搜索条件</th><th>备注</th></tr></thead><tbody><tr><td>location:</td><td>location:china，匹配用户填写的地址</td></tr><tr><td>language:</td><td>language:python , 匹配开发语言</td></tr><tr><td>followers:</td><td>follower: &gt;= 100 , 匹配拥有超过100名关注者的开发者</td></tr><tr><td>in:fullname:</td><td>John in:fullname ， 匹配用户实际名为John 的开发者</td></tr></tbody></table><p>需要在 GitHub 上找到优秀的项目和工具，同样，通过关键字或者设置搜索条件帮助你事半功倍找到好资源。我的使用习惯是先用某些关键词搜索，得到的搜索结果优先展示一些现成的软件和工具。</p><h2 id="搜项目"><a href="#搜项目" class="headerlink" title="搜项目"></a>搜项目</h2><table><thead><tr><th>搜索条件</th><th>备注</th></tr></thead><tbody><tr><td>Awesome + 关键字</td><td>Awesome windows，帮助找到优秀的windows软件</td></tr><tr><td>stars:</td><td>stars:&gt;=500，匹配收藏数量超过500的项目</td></tr><tr><td>language:</td><td>language:python , 匹配开发语言</td></tr><tr><td>forks:</td><td>forks:&gt;=500, 匹配分支数量超过500的项目</td></tr></tbody></table><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>stars:&gt;=500 language:javascript</p><p>**Reference: **</p><p><a href="https://sspai.com/post/46061">https://sspai.com/post/46061</a></p><p><a href="https://help.github.com/articles/about-searching-on-github/">https://help.github.com/articles/about-searching-on-github/</a></p>]]></content>
      
      
      <categories>
          
          <category> Github </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/10/01/hello-world/"/>
      <url>2020/10/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux环境下Hive的安装</title>
      <link href="2020/09/22/Linux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85/"/>
      <url>2020/09/22/Linux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="一、安装Hive"><a href="#一、安装Hive" class="headerlink" title="一、安装Hive"></a>一、安装Hive</h2><h3 id="1-1-下载并解压"><a href="#1-1-下载并解压" class="headerlink" title="1.1 下载并解压"></a>1.1 下载并解压</h3><p>下载所需版本的 Hive，这里我下载版本为 <code>cdh5.15.2</code>。下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载后进行解压</span></span><br><span class="line">tar -zxvf hive-1.1.0-cdh5.15.2.tar.gz</span><br></pre></td></tr></table></figure><h3 id="1-2-配置环境变量"><a href="#1-2-配置环境变量" class="headerlink" title="1.2 配置环境变量"></a>1.2 配置环境变量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/profile</span></span><br></pre></td></tr></table></figure><p>添加环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/app/hive-1.1.0-cdh5.15.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><p>使得配置的环境变量立即生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure><h3 id="1-3-修改配置"><a href="#1-3-修改配置" class="headerlink" title="1.3 修改配置"></a>1.3 修改配置</h3><p><strong>1. hive-env.sh</strong></p><p>进入安装目录下的 <code>conf/</code> 目录，拷贝 Hive 的环境配置模板 <code>flume-env.sh.template</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><p>修改 <code>hive-env.sh</code>，指定 Hadoop 的安装路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br></pre></td></tr></table></figure><p><strong>2. hive-site.xml</strong></p><p>新建 hive-site.xml 文件，内容如下，主要是配置存放元数据的 MySQL 的地址、驱动、用户名和密码等信息：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop001:3306/hadoop_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-4-拷贝数据库驱动"><a href="#1-4-拷贝数据库驱动" class="headerlink" title="1.4 拷贝数据库驱动"></a>1.4 拷贝数据库驱动</h3><p>将 MySQL 驱动包拷贝到 Hive 安装目录的 <code>lib</code> 目录下, MySQL 驱动的下载地址为：<a href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a> , 在本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录下我也上传了一份，有需要的可以自行下载。</p><p><img src="/2020/09/22/Linux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85/1.png"></p><h3 id="1-5-初始化元数据库"><a href="#1-5-初始化元数据库" class="headerlink" title="1.5 初始化元数据库"></a>1.5 初始化元数据库</h3><ul><li><p>当使用的 hive 是 1.x 版本时，可以不进行初始化操作，Hive 会在第一次启动的时候会自动进行初始化，但不会生成所有的元数据信息表，只会初始化必要的一部分，在之后的使用中用到其余表时会自动创建；</p></li><li><p>当使用的 hive 是 2.x 版本时，必须手动初始化元数据库。初始化命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># schematool 命令在安装目录的 bin 目录下，由于上面已经配置过环境变量，在任意位置执行即可</span><br><span class="line">schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure><p>这里我使用的是 CDH 的 <code>hive-1.1.0-cdh5.15.2.tar.gz</code>，对应 <code>Hive 1.1.0</code> 版本，可以跳过这一步。</p></li></ul><h3 id="1-6-启动"><a href="#1-6-启动" class="headerlink" title="1.6 启动"></a>1.6 启动</h3><p>由于已经将 Hive 的 bin 目录配置到环境变量，直接使用以下命令启动，成功进入交互式命令行后执行 <code>show databases</code> 命令，无异常则代表搭建成功。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hive</span><br></pre></td></tr></table></figure><p><img src="/2020/09/22/Linux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85/2.png"></p><p>在 Mysql 中也能看到 Hive 创建的库和存放元数据信息的表</p><p><img src="/2020/09/22/Linux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85/3.png"></p><h2 id="二、HiveServer2-beeline"><a href="#二、HiveServer2-beeline" class="headerlink" title="二、HiveServer2/beeline"></a>二、HiveServer2/beeline</h2><p>Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，因此产生了 HiveServer2。HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务。</p><p>HiveServer2 拥有自己的 CLI 工具——Beeline。Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于目前 HiveServer2 是 Hive 开发维护的重点，所以官方更加推荐使用 Beeline 而不是 Hive CLI。以下主要讲解 Beeline 的配置方式。</p><h3 id="2-1-修改Hadoop配置"><a href="#2-1-修改Hadoop配置" class="headerlink" title="2.1 修改Hadoop配置"></a>2.1 修改Hadoop配置</h3><p>修改 hadoop 集群的 core-site.xml 配置文件，增加如下配置，指定 hadoop 的 root 用户可以代理本机上所有的用户。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>之所以要配置这一步，是因为 hadoop 2.0 以后引入了安全伪装机制，使得 hadoop 不允许上层系统（如 hive）直接将实际用户传递到 hadoop 层，而应该将实际用户传递给一个超级代理，由该代理在 hadoop 上执行操作，以避免任意客户端随意操作 hadoop。如果不配置这一步，在之后的连接中可能会抛出 <code>AuthorizationException</code> 异常。</p><blockquote><p>关于 Hadoop 的用户代理机制，可以参考：<a href="https://blog.csdn.net/u012948976/article/details/49904675#%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E8%A7%A3%E8%AF%BB">hadoop 的用户代理机制</a> 或 <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html">Superusers Acting On Behalf Of Other Users</a></p></blockquote><h3 id="2-2-启动hiveserver2"><a href="#2-2-启动hiveserver2" class="headerlink" title="2.2 启动hiveserver2"></a>2.2 启动hiveserver2</h3><p>由于上面已经配置过环境变量，这里直接启动即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> nohup hiveserver2 &amp;</span></span><br></pre></td></tr></table></figure><h3 id="2-3-使用beeline"><a href="#2-3-使用beeline" class="headerlink" title="2.3 使用beeline"></a>2.3 使用beeline</h3><p>可以使用以下命令进入 beeline 交互式命令行，出现 <code>Connected</code> 则代表连接成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> beeline -u jdbc:hive2://hadoop001:10000 -n root</span></span><br></pre></td></tr></table></figure><p><img src="/2020/09/22/Linux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85/4.png"></p><p><strong>Reference</strong></p><p>1、<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Linux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2.md">Linux环境下Hive的安装</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>以视频类APP为例：看如何搭建数据指标体系</title>
      <link href="2020/09/19/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/"/>
      <url>2020/09/19/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="为什么需要数据指标"><a href="#为什么需要数据指标" class="headerlink" title="为什么需要数据指标"></a>为什么需要数据指标</h1><p>现阶段互联网发展已经进入精细化运营时代，精细化运营则要求产品能拥有完整、准确且有效的数据。因此为自己的产品搭建一套数据指标体系，对于促进产品和业务增长是至关重要的。</p><p>指标能够量化的衡量业务的好坏，评价业务当前情况，为业务的发展提供有效的指引，同时能使团队成员建立共同的目标并为之努力。</p><h1 id="数据指标搭建模型"><a href="#数据指标搭建模型" class="headerlink" title="数据指标搭建模型"></a>数据指标搭建模型</h1><h2 id="根据业务模块选择与业务匹配的数据指标"><a href="#根据业务模块选择与业务匹配的数据指标" class="headerlink" title="根据业务模块选择与业务匹配的数据指标"></a>根据业务模块选择与业务匹配的数据指标</h2><p>首先需要判断业务类型，根据业务来选择对应的指标模块。</p><table><thead><tr><th>业务模块</th><th>业务关注的核心</th><th>业务关心的指标</th></tr></thead><tbody><tr><td>工具模块</td><td>效率</td><td>使用量、核心功能达成率、使用频次</td></tr><tr><td>内容浏览模块</td><td>内容质&amp;量</td><td>浏览数、浏览时常</td></tr><tr><td>交易模块</td><td>转化率</td><td>详情页转化率、交易总金额、客单价、复购率</td></tr><tr><td>社区模块</td><td>活跃度</td><td>发布量、互动量、关系密度（用户与用户之间的关系）</td></tr></tbody></table><h2 id="AARRR模型"><a href="#AARRR模型" class="headerlink" title="AARRR模型"></a>AARRR模型</h2><p>麦克卢尔将创业公司最需要关注的指标分为五大类：获取用户（Acquisition）、提高活跃（Activation）、提高留存率（Retention）、获取营收（Revenue）和自传播（Referral），简称AARRR。每个环节都有这个环节应该关注的指标，这些环节并不一定遵循严格的先后顺序。</p><table><thead><tr><th>环节</th><th>业务关注的核心</th><th>业务关注的指标</th></tr></thead><tbody><tr><td>获取用户</td><td>用户如果知道产品，如何获取新用户</td><td>获得流量、用户新增情况、流量转化率、渠道对比情况、用户获取成本</td></tr><tr><td>提高活跃度</td><td>用户是否使用产品的核心功能</td><td>某个核心功能的使用量</td></tr><tr><td>提高留存率</td><td>用户使用后是否会再次使用</td><td>距上次使用时间、用户参与度、日活、月活、流失率、留存率</td></tr><tr><td>获取营收</td><td>用户为产品带来的收益</td><td>客户终身价值、免费到付的转化率、广告收入、交易金额</td></tr><tr><td>自传播</td><td>用户是否愿意分享该产品</td><td>分享数、邀请数、病毒式传播、病毒式传播周期</td></tr></tbody></table><h2 id="埃里克·莱斯的增长引擎说"><a href="#埃里克·莱斯的增长引擎说" class="headerlink" title="埃里克·莱斯的增长引擎说"></a>埃里克·莱斯的增长引擎说</h2><p>在《精益求精》一书中，埃里克·莱斯提出了驱动创业增长的三大引擎，它们都有各自对应的关键绩效指标（KPI）。</p><table><thead><tr><th>类型</th><th>业务关注的核心</th><th>业务关注的指标</th></tr></thead><tbody><tr><td>黏着式增长引擎</td><td>衡量用户粘性</td><td>留存率、流失率、使用频率、距上次登录时间</td></tr><tr><td>病毒式增长引擎</td><td>关注用户传播</td><td>病毒传播系数（每用户带来的新用户数）、病毒传播周期（用户完成一次邀请所用的时间）</td></tr><tr><td>付费式增长引擎</td><td>判断公司能否赚钱，公司的商业模式</td><td>LTV（客户终身价值）、CAC（客户获取成本）</td></tr></tbody></table><h2 id="PULSE模型"><a href="#PULSE模型" class="headerlink" title="PULSE模型"></a>PULSE模型</h2><p>PULSE模型是衡量用户体验的重要指标，也经常被用来度量产品的整体表现。如果一款产品响应时间为10秒，肯恩大部分用户会放弃这款产品；如果一款产品7天活跃用户数仅剩下1%，那将很难实现用户增长；如果一款产品有很好的流量却没没有办法变现，那公司将无法盈利。</p><p>此模型的指标包括：</p><ul><li>Page view：页面浏览量；</li><li>Uptime：响应时间；</li><li>Latency：延迟；</li><li>Seven days active users: 7天活跃用户数；</li><li>Earning：收益。</li></ul><h2 id="HEART模型"><a href="#HEART模型" class="headerlink" title="HEART模型"></a>HEART模型</h2><p>HEART模型是由Google推出用来衡量用户体验设计成果的模型，主要用来评估用户体验，来自于论文《Measuring the User Experience on a LargeScale：User-Centered Metrics for WebApplications》，即《大规模测量用户体验：以用户为中心的网页应用度量体系》。</p><table><thead><tr><th>名称</th><th>指标</th></tr></thead><tbody><tr><td>愉悦度（Happiness）</td><td>可用性、易用性、推荐意愿觉感受度等，指标数据；可通过调研问卷、产品评价打分得到</td></tr><tr><td>参与度（Engagement）</td><td>参与时长、频率、深度和参与会话数、访问频率、访问深度、页面停留时间、产生UGC的数量，日活跃度、周活跃度、月活跃度</td></tr><tr><td>接受度（Adoption）</td><td>指用户对一个功能或者一组功能的接受情况或者新用户接受一个产品的情况；可统计核心功能点击率和停留时间</td></tr><tr><td>留存率（Retention）</td><td>次日留存率、周留存率、月留存率</td></tr><tr><td>任务完成（Task Success）</td><td>任务完成率、完成时间、错误率</td></tr></tbody></table><h1 id="以视频类APP为例搭建数据指标体系"><a href="#以视频类APP为例搭建数据指标体系" class="headerlink" title="以视频类APP为例搭建数据指标体系"></a>以视频类APP为例搭建数据指标体系</h1><h2 id="用户指标选择及定义"><a href="#用户指标选择及定义" class="headerlink" title="用户指标选择及定义"></a>用户指标选择及定义</h2><p><img src="/2020/09/19/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/%E7%94%A8%E6%88%B7%E6%8C%87%E6%A0%87%E9%80%89%E6%8B%A9%E5%8F%8A%E5%AE%9A%E4%B9%89.png"></p><h2 id="用户互动指标选择及定义"><a href="#用户互动指标选择及定义" class="headerlink" title="用户互动指标选择及定义"></a>用户互动指标选择及定义</h2><p><img src="/2020/09/19/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/%E7%94%A8%E6%88%B7%E4%BA%92%E5%8A%A8%E6%8C%87%E6%A0%87%E9%80%89%E6%8B%A9%E5%8F%8A%E5%AE%9A%E4%B9%89.png"></p><h2 id="内容指标选择及定义"><a href="#内容指标选择及定义" class="headerlink" title="内容指标选择及定义"></a>内容指标选择及定义</h2><p><img src="/2020/09/19/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/%E5%86%85%E5%AE%B9%E6%8C%87%E6%A0%87%E9%80%89%E6%8B%A9%E5%8F%8A%E5%AE%9A%E4%B9%89.png"></p><h2 id="商业指标选择及定义"><a href="#商业指标选择及定义" class="headerlink" title="商业指标选择及定义"></a>商业指标选择及定义</h2><p><img src="/2020/09/19/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/%E5%95%86%E4%B8%9A%E6%8C%87%E6%A0%87%E9%80%89%E6%8B%A9%E5%8F%8A%E5%AE%9A%E4%B9%89.png"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>根据产品中包含的不同功能，还有针对相应功能的指标。比如对于推送功能，有推送到达率、打开率等指标衡量；针对FEED流的刷新功能，有刷新次数、刷新渗透率等指标。</p><p>数据指标的建立是为数据分析和业务服务的，在实际操作过程中，需要根据相应的数据分析需求和业务需求定义相应的数据指标。</p><p>团队内部需要定义达成共识的指标体系，比如活跃用户数这一指标需要定义清楚什么样的用户是活跃（启动APP/完成登录…..)的，用户以什么方式标识（设备ID/用户ID…….）。</p><p><strong>Reference</strong></p><p>1、<a href="http://www.woshipm.com/operate/3424576.html">以视频类APP为例：看如何搭建数据指标体系</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 数据分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch分布式训练</title>
      <link href="2020/09/15/pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
      <url>2020/09/15/pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<p><strong>Reference：</strong></p><p>1、<a href="https://zhuanlan.zhihu.com/p/76638962">Pytorch 分布式训练</a></p><p>2、<a href="https://github.com/ultralytics/yolov3/issues/552">AttributeError: module ‘torch.distributed’ has no attribute ‘init_process_group’</a></p><p>3、<a href="https://zhuanlan.zhihu.com/p/86441879">pytorch多gpu并行训练</a></p><p>4、<a href="https://fyubang.com/2019/07/23/distributed-training3/">【分布式训练】单机多卡的正确打开方式（三）：PyTorch</a></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阿里移动事业部面试总结</title>
      <link href="2020/09/11/ali-interview/"/>
      <url>2020/09/11/ali-interview/</url>
      
        <content type="html"><![CDATA[<h2 id="谈谈python2和python3的区别"><a href="#谈谈python2和python3的区别" class="headerlink" title="谈谈python2和python3的区别"></a>谈谈python2和python3的区别</h2><h3 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h3><p>py3中默认字符编码是<code>UTF-8</code>，因此使用Python3不需要文件顶部写‘# coding=utf-8’；</p><p>py2中默认字符编码是<code> ASCII</code>，如果文件中出现了中文，需要在顶部加入coding声明**# coding=utf-8**</p><p><strong>注意：# coding=utf-8的=号两边不要空格。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">py2: </span><br><span class="line">  　　- <span class="built_in">ascii</span> </span><br><span class="line"> 　　 文件头可以修改：<span class="comment">#-*- encoding:utf-8 -*-</span></span><br><span class="line">py3:</span><br><span class="line"> 　　 - utf-<span class="number">8</span></span><br><span class="line">  　　文件头可以修改：<span class="comment">#-*- encoding:utf-8 -*-</span></span><br></pre></td></tr></table></figure><h3 id="字符串类型"><a href="#字符串类型" class="headerlink" title="字符串类型"></a>字符串类型</h3><p>Python3：</p><p>str 对象和 bytes 对象可以使用 .encode() (<strong>str -&gt; bytes</strong>) 或 .decode() (<strong>bytes -&gt; str</strong>)方法相互转化。</p><p><strong>从str到bytes，是编码,  比特流 = str(串, encoding=’utf-8’)</strong></p><p><strong>从bytes到str，是解码，串 = bytes(比特流, encoding=’utf-8’)</strong></p><blockquote><p><strong>为什么英文显示是b’hello’呢，因为英文里，utf-8和ASCII是兼容的，所以只是显示为b’hello’，其实也是数字</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## str --&gt; bytes</span></span><br><span class="line">tt = <span class="string">&#x27;奥德赛&#x27;</span></span><br><span class="line"><span class="built_in">type</span>(tt)</span><br><span class="line"></span><br><span class="line">ss = <span class="built_in">bytes</span>(tt, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="comment"># to bytes</span></span><br><span class="line"><span class="built_in">type</span>(ss)</span><br><span class="line"></span><br><span class="line">rr = ss.decode(<span class="string">&quot;utf-8&quot;</span>)          <span class="comment"># to string</span></span><br><span class="line"><span class="built_in">type</span>(rr)</span><br><span class="line"></span><br><span class="line">uu = <span class="built_in">str</span>(ss, encoding=<span class="string">&#x27;utf-8&#x27;</span>)   <span class="comment"># to string</span></span><br><span class="line"><span class="built_in">type</span>(uu)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">&lt;class &#x27;bytes&#x27;&gt;</span></span><br><span class="line"><span class="string">&lt;class &#x27;str&#x27;&gt;</span></span><br><span class="line"><span class="string">&lt;class &#x27;str&#x27;&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><em><strong>可以使用encode()函数对字符串进行编码，转换成二进制字节数据，也可用decode()函数将字节解码成字符串</strong></em>；用decode()函数解码，英文可不要用指定编码格式，中文需要指定解码方式；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;中文编码&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="string">&#x27;中文&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(a)</span><br><span class="line"><span class="built_in">str</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.encode(encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"><span class="string">b&#x27;\xe4\xb8\xad\xe6\x96\x87&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;中文解码&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = b.decode(encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c</span><br><span class="line"><span class="string">&#x27;中文&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;可以省略解码格式&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = b.decode()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line"><span class="string">&#x27;中文&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Python2：</p><p>Python2中字符串有两种类型：unicode和str，前者表示文本字符串，后者表示字节序列，两者没有明显的界限。<br>如上所述，Python3中也有两种类型，str表示字符串，byte表示字节序列，任何需要写入文本或者网络传输的数据都只接收字节序列。</p><p>字符串：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">py2: </span><br><span class="line">    unicode         v = <span class="string">u&quot;root&quot;</span>    <span class="comment">#本质上用unicode存储（万国码）</span></span><br><span class="line">    (<span class="built_in">str</span>/<span class="built_in">bytes</span>)     v = <span class="string">&quot;root&quot;</span>     <span class="comment">#本质用字节存储</span></span><br><span class="line">       </span><br><span class="line">py3:         </span><br><span class="line">    <span class="built_in">str</span>             v = <span class="string">&quot;root&quot;</span>     <span class="comment">#本质上用unicode存储（万国码）</span></span><br><span class="line">    <span class="built_in">bytes</span>           v = <span class="string">b&quot;root&quot;</span>    <span class="comment">#本质上用字节存储</span></span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = <span class="string">&#x27;存贮&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line"><span class="string">&#x27;\xe5\xad\x98\xe8\xb4\xae&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(t)</span><br><span class="line">&lt;<span class="built_in">type</span> <span class="string">&#x27;str&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.encode(encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">UnicodeDecodeError: <span class="string">&#x27;ascii&#x27;</span> codec can<span class="string">&#x27;t decode byte 0xe5 in position 0: ordinal not in range(128)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; </span></span><br><span class="line"><span class="string">&gt;&gt;&gt; s = t.decode(encoding=&#x27;</span>utf-<span class="number">8</span><span class="string">&#x27;)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; s</span></span><br><span class="line"><span class="string">u&#x27;</span>\u5b58\u8d2e<span class="string">&#x27;</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; print(s)</span></span><br><span class="line"><span class="string">存贮</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; type(s)</span></span><br><span class="line"><span class="string">&lt;type &#x27;</span>unicode<span class="string">&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>补充知识（参考2）</p><p>**原码：数值X的原码记为[x]原，如果机器字长为n(即采用n个二进制位表示数据)。则最高位是符号位。0表示正号，1表示负号，其余的n-1位表示数值的绝对值。数值零的原码表示有两种形式：[+0]原=0000 0000  ,-[0]原=1000 0000. **</p><p>例子：若机器字长n等于8，则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[+1]原&#x3D;0000 00001      [-1]原&#x3D;1000 00001 </span><br><span class="line">[+127]原&#x3D;0111 1111     [-127]原&#x3D;1111 1111</span><br><span class="line">[+45]原&#x3D;0010 1101      [-45]原&#x3D;1010 1101  </span><br></pre></td></tr></table></figure><p>可见，原码，在计算数值上出问题了，当然，你也可以实验下，原码在计算正数和正数的时候，它是一点问题都没有的，但是出现负数的时候就出现问题了。所以才会有我下面将的问题：反码。</p><p><strong>反码：数值X的反码记作[x]反，如果机器字长为n，则最高位是符号位，0表示正号，1表示负号，正数的反码与原码相同，负数的反码则是其绝对值按位求反。数值0的反码表示有两种形式：[+0]反=0000 0000  ,-[0]反=1111 1111.</strong></p><p>例子：若机器字长n等于8，则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[+1]反&#x3D;0000 00001      [-1]反&#x3D;1111 1110 </span><br><span class="line">[+127]反&#x3D;0111 1111     [-127]反&#x3D;1000 0000</span><br><span class="line">[+45]反&#x3D;0010 1101      [-45]反&#x3D;1101 0010  </span><br></pre></td></tr></table></figure><p>在看反码计算的问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1+（-1）&#x3D;0     |   (0000 0001)反+（1111 1110）反&#x3D;（1111 1111）反&#x3D;（1000 0000）原&#x3D;【-0】 可以看到，虽然是-0，但是问题还不是很大。</span><br><span class="line">1+（-2）&#x3D;-1    |   (0000 0001）反+（1111 1101）反&#x3D;（1111 1110）反&#x3D;（1000 0001）原&#x3D;【-1】 可以看到，没有问题。</span><br><span class="line">(-1)+（2）&#x3D;1   |   (1111 1110）反+（0000 0010）反&#x3D;（0000 0000）反&#x3D;（0000 0000）原&#x3D;【0】 可以看到，问题发生了，因为溢出，导致结果变为0了。</span><br><span class="line">所以，看以看到，用反码表示，问题依然没有解决，所以，出现了下面的补码。</span><br></pre></td></tr></table></figure><p><strong>补码：数值X的补码记作[x]补，如果机器字长为n，则最高位是符号位，0表示正号，1表示负号，正数的补码与原码反码都相同，负数的补码则等于其反码的末尾加1。数值0的补码表示有唯一的编码：[+0]补=0000 0000  ,-[0]补=0000 0000.</strong></p><p>例子：若机器字长n等于8，则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[+1]补&#x3D;0000 00001      [-1]补&#x3D;1111 1111 </span><br><span class="line">[+127]补&#x3D;0111 1111     [-127]补&#x3D;1000 0001</span><br><span class="line">[+45]补&#x3D;0010 1101      [-45]补&#x3D;1101 0011 </span><br></pre></td></tr></table></figure><p>在看补码计算的问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1+（-1）&#x3D;0        |      （0000 0001）补+（1111 1111）补&#x3D;（0000 0000）补&#x3D;（0000 0000）原&#x3D;【0】 可以看到。没有问题</span><br><span class="line">1+（-2）&#x3D;-1       |      （0000 0001）补+（1111 1110）补&#x3D;（1111 1111）补&#x3D;（1000 0001）原&#x3D;【-1】 可以看到，没有问题</span><br><span class="line">-1+（2）&#x3D;1        |      （1111 1111）补+（0000 0010）补&#x3D;（0000 0001）补 &#x3D;（0000 0001）原&#x3D;【1】 可以看到，没有问题</span><br></pre></td></tr></table></figure><p>通过上面的计算，我们发现，用补码的方式，就不存在在原码和反码中存在的计算问题了。其实，这也是计算机表达带符号整数用补码的原因。如果，你觉得我举得例子太少，缺少代表行，你可以自己试试。不过，放心补码一定是不会存在原码和反码的问题的。</p><h3 id="缩进格式"><a href="#缩进格式" class="headerlink" title="缩进格式"></a>缩进格式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Python2的缩进机制中，1个Tab和8个Space等价的，同时允许缩进中Tab和Space在代码中共存，但这种机制在部分IDE使用中有问题。</span><br><span class="line"></span><br><span class="line">Python3中Tab和Space不再能共存，同时存在时会报错：</span><br><span class="line">TabError: inconsistent use of tabs and spaces in indentation.</span><br></pre></td></tr></table></figure><h3 id="print-exec的用法"><a href="#print-exec的用法" class="headerlink" title="print, exec的用法"></a>print, exec的用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在python2中，<span class="built_in">print</span>作为语句使用，而在python3中作为函数使用，<span class="built_in">exec</span>同理：</span><br><span class="line"></span><br><span class="line">print(“hello”,“world”)</span><br><span class="line"></span><br><span class="line">在python2中则输出为一个元组；</span><br><span class="line">在pyton3中输出为两个字符串，默认中间用空格隔开。</span><br></pre></td></tr></table></figure><h3 id="True和False用法"><a href="#True和False用法" class="headerlink" title="True和False用法"></a>True和False用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Python2中True和False都是全局变量，数值分别表示为1和0，因为是变量，所以存在可以被赋值的情况。</span><br><span class="line"></span><br><span class="line">Python3中True和False是关键字，固定指向两个固定对象，不再允许被重新赋值。</span><br></pre></td></tr></table></figure><h3 id="小数和除法的用法"><a href="#小数和除法的用法" class="headerlink" title="小数和除法的用法"></a>小数和除法的用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Python2中&#x2F;作用就是整除，只输出整数，&#x2F;&#x2F;是小数除法；</span><br><span class="line">而在Python3中用&#x2F;&#x2F;作为整除，&#x2F;是小数除法。</span><br><span class="line"></span><br><span class="line">实例：3&#x2F;2在Python2中输出为1，在Python3中是1.5</span><br></pre></td></tr></table></figure><h3 id="小数和除法的用法-1"><a href="#小数和除法的用法-1" class="headerlink" title="小数和除法的用法"></a>小数和除法的用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python2中/作用就是整除，只输出整数，//是小数除法；</span></span><br><span class="line"><span class="comment"># 在默认情况下，这两种运算符有很大的重叠地方，比如，当两个数都是整数的时候，两者的运算结果是没有区别的。如果想在python中让这两种有一个明确的分工。即&quot;/&quot;可以用于float除法，&quot;//&quot;用于整除法，我们可以在程序开始的时候做以下</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"></span><br><span class="line"><span class="comment">#而在Python3中用//作为整除，/是小数除法。</span></span><br><span class="line"><span class="comment">#实例：3/2在Python2中输出为1，在Python3中是1.5</span></span><br></pre></td></tr></table></figure><h3 id="比较运算符的区别"><a href="#比较运算符的区别" class="headerlink" title="比较运算符的区别"></a>比较运算符的区别</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Python2中任意两对象都可以比较。</span><br><span class="line"></span><br><span class="line">Python3中只有同种数据类型对象可以比较。</span><br></pre></td></tr></table></figure><h3 id="nonlocal"><a href="#nonlocal" class="headerlink" title="nonlocal"></a>nonlocal</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Python2中可以在函数中使用global声明变量为全局变量，但是给一个变量声明为非局部变量是无法实现的。在Python3，新增了关键字nonlocal，使得定义非局部变量成为了可能。</span><br></pre></td></tr></table></figure><h3 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Python2中很多返回列表对象的内置函数和方法。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在Python3中都改成了返回类似于迭代器的对象，迭代器的惰性求值特性使得操作大数据更加有效率。</span><br></pre></td></tr></table></figure><h3 id="for循环变量值区别"><a href="#for循环变量值区别" class="headerlink" title="for循环变量值区别"></a>for循环变量值区别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python2中，for循环会修改外部相同名称变量的值：</span></span><br><span class="line"></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line">print(<span class="string">&#x27;comprehension: &#x27;</span>, [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)])</span><br><span class="line">print(<span class="string">&#x27;after: i =&#x27;</span>, i)  <span class="comment">#i=4</span></span><br><span class="line">类似于Python3中：</span><br><span class="line">i=<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span>  <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    print(i)</span><br><span class="line">    print(i)          <span class="comment">##两次结果都是4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Python3，for循环不会修改外部相同名称变量的值：</span></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line">print(<span class="string">&#x27;comprehension: &#x27;</span>, [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)])</span><br><span class="line">print(<span class="string">&#x27;after: i =&#x27;</span>, i) <span class="comment">#i=1</span></span><br><span class="line"></span><br><span class="line">[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]生成的是一个链表</span><br></pre></td></tr></table></figure><h3 id="round函数返回值区别"><a href="#round函数返回值区别" class="headerlink" title="round函数返回值区别"></a>round函数返回值区别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Python3，<span class="built_in">round</span>函数返回<span class="built_in">int</span>类型值</span><br><span class="line">  <span class="built_in">isinstance</span>(<span class="built_in">round</span>(<span class="number">15.5</span>),<span class="built_in">int</span>)   <span class="comment">#True</span></span><br><span class="line"></span><br><span class="line">Python2，<span class="built_in">round</span>函数返回<span class="built_in">float</span>类型值</span><br><span class="line">  <span class="built_in">isinstance</span>(<span class="built_in">round</span>(<span class="number">15.5</span>),<span class="built_in">float</span>) <span class="comment">#True</span></span><br></pre></td></tr></table></figure><h3 id="通过input-解析用户输入"><a href="#通过input-解析用户输入" class="headerlink" title="通过input()解析用户输入"></a>通过input()解析用户输入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Python3中<span class="built_in">input</span>得到的为<span class="built_in">str</span>；</span><br><span class="line">Python2的<span class="built_in">input</span>的到的为<span class="built_in">int</span>型，Python2的raw_input得到的为<span class="built_in">str</span>类型；</span><br><span class="line"></span><br><span class="line">统一一下：Python3中用<span class="built_in">input</span>，Python2中用row_input，都输入为<span class="built_in">str</span>。</span><br></pre></td></tr></table></figure><h3 id="迭代器range和xrange"><a href="#迭代器range和xrange" class="headerlink" title="迭代器range和xrange"></a>迭代器range和xrange</h3><p>py3中的range == py2中的 xrange， 返回类似迭代器的东西，节省内存空间</p><h3 id="语句变函数"><a href="#语句变函数" class="headerlink" title="语句变函数"></a>语句变函数</h3><p>py3中为print(), exec() 是一个方法，必须加上括号； </p><p>py2中为print, exec，是一个语句</p><h3 id="数据传输"><a href="#数据传输" class="headerlink" title="数据传输"></a>数据传输</h3><p>py3中socket传过来的数据是byte类型 / hashlib包update也需要传bytes类型的数据；</p><p>py2中则可以直接传入str, e.g</p><h3 id="两数相除"><a href="#两数相除" class="headerlink" title="两数相除"></a>两数相除</h3><p>py2中，1/2（两个整数相除）结果是0；</p><p>py3中是0.5了</p><p>python 2.2+ 以上都可以使用 from _<em>future_</em> import division 实现改特性, 同时注意 // 取代了之前的 / 运算</p><h3 id="print函数"><a href="#print函数" class="headerlink" title="print函数"></a>print函数</h3><p>Python3中print为一个函数，必须用括号括起来；</p><p>Python2中print为class。</p><h2 id="对于一个dict，按其value值进行排序"><a href="#对于一个dict，按其value值进行排序" class="headerlink" title="对于一个dict，按其value值进行排序"></a>对于一个dict，按其value值进行排序</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 按value排序，sorted()函数不会改变原来的对象，而是会返回一个新的已经排序好的对象；而sort则是在原位重新排列列表，会覆盖原有的列表。</span></span><br><span class="line"><span class="built_in">sorted</span>(d.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>/<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 按value排序，并保存到一个新的列表中</span></span><br><span class="line">[(k, v) <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">sorted</span>(d.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>/<span class="literal">False</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment">## 按value排序，并保存到一个新的dict中</span></span><br><span class="line">[&#123;k: v&#125; <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">sorted</span>(d.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse=<span class="literal">True</span>/<span class="literal">False</span>)]</span><br></pre></td></tr></table></figure><p>PS：不要与map混淆，map(function, iterable, …)是将function应用于iterable的每一个元素，结果以列表的形式返回。</p><h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><h3 id="窗口函数有以下功能："><a href="#窗口函数有以下功能：" class="headerlink" title="窗口函数有以下功能："></a>窗口函数有以下功能：</h3><p>1）同时具有分组和排序的功能</p><p>2）不减少原表的行数</p><p>3）语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;窗口函数&gt; over (partition by &lt;用于分组的列名&gt;</span><br><span class="line">                order by &lt;用于排序的列名&gt;)</span><br></pre></td></tr></table></figure><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>若一张表里面记录了一些用户的ID和其出行的城市信息，写一条SQL输出他出行最高频次的那条记录，表如下。</p><table><thead><tr><th align="left">id</th><th align="left">city</th></tr></thead><tbody><tr><td align="left">10000</td><td align="left">广州</td></tr><tr><td align="left">10000</td><td align="left">广州</td></tr><tr><td align="left">10000</td><td align="left">深圳</td></tr><tr><td align="left">10001</td><td align="left">广州</td></tr><tr><td align="left">10001</td><td align="left">深圳</td></tr><tr><td align="left">10001</td><td align="left">深圳</td></tr></tbody></table><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>1、先对该表出行记录汇总，计算出每个id对应出行city的次数；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT id, city, count(*) AS ctime FROM travel GROUP BY id, city </span><br></pre></td></tr></table></figure><p>2、对步骤1中的出行次数进行排序；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT t1.id, t1.city, ctime, ROW_NUMBER() over ( PARTITION BY t1.id ORDER BY ctime DESC ) AS rank FROM</span><br><span class="line"></span><br><span class="line">  (SELECT id, city, count(*) AS ctime FROM travel GROUP BY id, city ) AS t1 </span><br></pre></td></tr></table></figure><p>3、取出top 1。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">SELECT t2.id, t2.city FROM (</span><br><span class="line"></span><br><span class="line">  SELECT t1.id, t1.city, ctime, ROW_NUMBER() over ( PARTITION BY t1.id ORDER BY ctime DESC ) AS rank FROM</span><br><span class="line"></span><br><span class="line">  (SELECT id, city, count(*) AS ctime FROM travel GROUP BY id, city ) AS t1 </span><br><span class="line"></span><br><span class="line">) AS t2</span><br><span class="line"></span><br><span class="line">WHERE t2.rank &#x3D; 1</span><br><span class="line"></span><br><span class="line">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span><br><span class="line"></span><br><span class="line">WITH t2 AS (   </span><br><span class="line">WITH t1 AS (</span><br><span class="line">    </span><br><span class="line">SELECT id, city, count(*) AS ctime FROM travel GROUP BY id, city </span><br><span class="line">) </span><br><span class="line"></span><br><span class="line">    SELECT t1.id,t1.city, ctime, ROW_NUMBER() over ( PARTITION BY t1.id ORDER BY ctime DESC ) AS rank FROM t1</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line">SELECT t2.id,t2.city FROM t2 WHEREt2.rank &#x3D; 1</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><table><thead><tr><th>id</th><th>city</th></tr></thead><tbody><tr><td>10000</td><td>广州</td></tr><tr><td>10001</td><td>深圳</td></tr></tbody></table><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="关于排序"><a href="#关于排序" class="headerlink" title="关于排序"></a>关于排序</h4><p>Rank()：可查询出并列第一，是跳跃查询；两个第一名下来就是第三名；</p><p>Row_number()：是没有重复值的排序（即使两条记录相等也是不重复的），可以利用它来实现分页；</p><p>dense_rank()：连续排序，两个第二名仍然跟着第三名。</p><h4 id="关于group-by和over-partition-by的区别"><a href="#关于group-by和over-partition-by的区别" class="headerlink" title="关于group by和over partition by的区别"></a>关于group by和over partition by的区别</h4><p>group by分组汇总后改变表的行数，一行只有一个类别；</p><p>而over partition by分组汇总不会减少原表中的行数。</p><h2 id="数据仓库的层"><a href="#数据仓库的层" class="headerlink" title="数据仓库的层"></a>数据仓库的层</h2><p>参考：<a href="https://marchboy.github.io/2021/03/16/data-warehouse/">数据仓库的基础知识</a></p><h2 id="谈谈贝叶斯概率"><a href="#谈谈贝叶斯概率" class="headerlink" title="谈谈贝叶斯概率"></a>谈谈贝叶斯概率</h2><h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>贝叶斯定理由英国数学家贝叶斯 (Thomas Bayes 1702-1761) 发展，用来描述两个条件概率之间的关系，比如 $P(A|B)$ 和$P(B|A)$。主要用于文本分类。</p><p>（1）条件概率公式</p><p>设A，B是两个事件，且$P(B)&gt;0$，则在事件B发生的条件下，事件A发生的条件概率为：<br>$$<br>P(A|B) = \frac{P(AB)}{P(B)}<br>$$<br>（2）由条件概率公式得出乘法公式：$P(AB)=P(A|B)P(B)=P(B|A)P(A)$</p><p>也可变形为：<br>$$<br>P(A|B) = \frac{P(B|A) · P(A)} {P(B)}<br>$$<br>也可以这样来看贝叶斯公式：<br>$$<br>P(A|B) = P(A) · \frac{P(B|A)} {P(B)}<br>$$</p><ul><li><p>第一部分是先验概率$P(A)$，后一部分是“调整因子”。</p></li><li><p>P(A) 称为”先验概率”，即在B事件发生之前，我们对A事件概率的一个判断。</p></li><li><p>$P(A|B)$称为”后验概率”， 即在B事件发生之后，我们对A事件概率的重新评估。</p></li><li><p>$\frac{P(B|A)} {P(B)}$称为”可能性函数”，这是一个调整因子，使得预估概率更接近真实概率。</p></li></ul><p>（3）全概率公式：</p><p>全概率公式的意义在于，当直接计算P(A)较为困难，而$P(B_i)$ 和$P(A|B_i)(i=1,2,\cdots)$的计算较为简单时，可以利用全概率公式计算$P(A)$。将事件A分解成几个小事件，<code>通过求小事件的概率</code>，然后相加从而求得事件A的概率。而将事件A进行分割的时候，不是直接对A进行分割，而是先找到样本空间Ω的一个个划分$B_1,B_2,\cdots,B_n$，这样事件A就被事件$AB_1,AB_2,\cdots,AB_n$分解成了n 部分。</p><p>即$A=AB_1+AB_2+…+AB_n$, 每一个$B_i$发生都可能导致A发生相应的概率是$P(A|B_i)$ ，由加法公式得：<br>$$<br>\begin{align}<br>P(A) &amp;= P(AB_1)+P(AB_2)+\cdots+P(AB_n)\<br>   &amp;= P(A|B_1)·P(B_1)+P(A|B_2)·P(B_2)+\cdots+P(A|B_n)·P(B_n)<br>\end{align}<br>$$</p><h3 id="垃圾邮件问题"><a href="#垃圾邮件问题" class="headerlink" title="垃圾邮件问题"></a>垃圾邮件问题</h3><p>垃圾邮件曾经是一个令人头痛的问题，长期困扰着邮件运营商和用户。据统计，用户收到的电子邮件中80%以上是垃圾邮件。</p><p>传统的垃圾邮件过滤方法，主要有”关键词法”和”校验码法”等。</p><p>关键词法的过滤依据是特定的词语，（如垃圾邮件的关键词：“发票”，“贷款”，“利率”，“中奖”，“办证”，“抽奖”，“号码”，“钱”，“款”，“幸运”……等等。）但这种方法效果很不理想，而且容易规避。一是正常邮件中也可能有这些关键词，非常容易误判。二是将关键词进行变形，如“代！开-发/票”，就很容易规避关键词过滤，如果将关键词的各种变形都找出来过滤，那是无穷无尽的，而且更容易误判正常邮件。</p><p>校验码法则是计算邮件文本的校验码，再与已知的垃圾邮件进行对比。它们的识别效果都不理想，而且很容易规避。</p><p>直到2002年，Paul Graham提出使用“贝叶斯方法”过滤垃圾邮件，经过几年的工程化应用，才算解决了这个问题。而这种方法的效果，好的不可思议。此外，这种过滤方法还具有自我学习能力，会根据新收到的邮件，不断调整。收到的垃圾邮件越多，它的准确率就越高。</p><ul><li>贝叶斯过滤器的使用过程</li></ul><p>现在我们收到一封新邮件，我们假定它是 正常邮件 和 垃圾邮件的概率各是50%，即：</p><p>$$<br>P(正常) = P(垃圾) =50%<br>$$<br>然后，对这封新邮件的内容进行解析，发现其中含有“发票”这个词，那么这封邮件属于垃圾邮件的概率提高到多少？其实就是计算一个条件概率，在有“发票”词语的条件下，邮件是垃圾邮件的概率：P(垃圾|发票)。直接计算肯定是无法计算了，这时要用到贝叶斯定理：<br>$$<br>P(垃圾|发票) = \frac{P(发票|垃圾) · P(垃圾)} {P(发票)}<br>$$<br>根据全概率公式：<br>$$<br>P(发票)=P(发票|垃圾)⋅P(垃圾)+P(发票|正常)⋅P(正常)<br>$$</p><p>所以：<br>$$<br>P(垃圾|发票) = \frac{P(发票|垃圾) · P(垃圾)} {P(发票)=P(发票|垃圾)⋅P(垃圾)+P(发票|正常)⋅P(正常)}<br>$$<br>其中，P(发票|垃圾) 表示所有垃圾邮件中出现“发票”的概率，我们假设100封垃圾邮件中有5封包含“发票”这个词，那么这个概率是5%。P(发票|正常) 表示所有正常邮件中出现“发票”的概率，我们假设1000封正常邮件中有1封包含“发票”这个词，那么这个概率是0.1%。于是：</p><p>$$<br>P(垃圾|发票)= \frac{5%×50%} {5%×50% +0.1%×50%}=98%<br>$$</p><p>因此，这封新邮件是垃圾邮件的概率是98%。从贝叶斯思维的角度，这个“发票”推断能力很强，直接将垃圾邮件50%的概率提升到98%了。那么，我们是否就此能给出结论：这是封垃圾邮件？</p><p>根据以上的“发票”关键词，我们不能给出结论说这是一封垃圾邮件。这里还有2个核心问题没有解决：</p><ul><li><p>（1）P(发票|垃圾) 和 P(发票|正常)是我们假定的，怎样实际计算它们？</p></li><li><p>（2）正常邮件也是可能含有“发票”这个词，误判了怎么办？</p></li></ul><h3 id="建立历史资料库"><a href="#建立历史资料库" class="headerlink" title="建立历史资料库"></a>建立历史资料库</h3><p>贝叶斯过滤器是一种统计学过滤器，建立在已有的统计结果之上。所以，我们必须预先提供两组已经识别好的邮件，一组是正常邮件，另一组是垃圾邮件。</p><p>我们用这两组邮件，对过滤器进行”训练”。这两组邮件的规模越大，训练效果就越好。Paul Graham使用的邮件规模，是正常邮件和垃圾邮件各4000封。</p><p>“训练”过程很简单。首先，解析所有邮件，提取每一个词。然后，计算每个词语在正常邮件和垃圾邮件中的出现频率。</p><p>比如，我们假定”贷款”这个词，在4000封垃圾邮件中，有200封包含这个词，那么它的出现频率就是5%；而在4000封正常邮件中，只有2封包含这个词，那么出现频率就是0.05%。（【注释】如果某个词只出现在垃圾邮件中，Paul Graham就假定，它在正常邮件的出现频率是1%，反之亦然。这样做是为了避免概率为0。随着邮件数量的增加，计算结果会自动调整。）</p><p>有了这个初步的统计结果，<strong>概率值计算问题</strong>就解决了。过滤器就可以投入使用了。</p><h3 id="联合概率的计算"><a href="#联合概率的计算" class="headerlink" title="联合概率的计算"></a>联合概率的计算</h3><p>对于<strong>误判问题</strong>，可以采用“多特征判断”的思路，就像猫和老虎，如何单看颜色，花纹都不好判断，那就颜色、花纹、大小、体重等一起来判断。</p><p>同理，对于“发票”不好来判断，那就联合其他词语一起来判断，如果这封邮件中除了“发票”，还有“常年”，“代开”，“各种”，“行业”，“绝对正规”，“税点低”等词语，那么就通过这些词语联合认定这封邮件是垃圾邮件。</p><p>在基本方法计算的基础上，选取前n个（例如n=3，实际应用中是15个词/字以上）概率最高的词，假设为：“发票”，“常年”，“代开”。然后计算其联合条件概率。即在这3个词同时出现的条件下，这是一封垃圾邮件的概率。</p><p>即：P(垃圾|发票;常年;代开)，这时仍要用到贝叶斯定理：<br>$$<br>P(垃圾|发票；常年；代开) = \frac {P(发票；常年；代开|垃圾)·P(垃圾)}{P(发票；常年；代开)}<br>$$<br>这里假设：所有词语彼此之间是不相关的（严格说这个假设不成立；实际上各词语之间不可能完全没有相关性，但可以忽略）。</p><p>所以：<br>$$<br>P(垃圾|发票；常年；代开) = \frac{P(发票|垃圾)·P(常年|垃圾)·P(代开|垃圾)·P(垃圾)}{P(发票；常年；代开)}<br>$$<br>由于上式中右边的分母不太好求。我们可以求这封邮件是正常邮件的概率：<br>$$<br>P(正常|发票；常年；代开)=\frac{P(发票|正常)·P(常年|正常)·P(代开|正常)·P(正常)}{P(发票；常年；代开)}<br>$$<br>上面两个式子相除，得到：<br>$$<br>\frac{P(垃圾|发票；常年；代开)}{P(正常|发票；常年；代开)}=\frac{P(发票|垃圾)·P(常年|垃圾)·P(代开|垃圾)·P(垃圾)}{P(发票|正常)·P(常年|正常)·P(代开|正常)·P(正常)}<br>$$<br>即在这3个词同时出现的情况下，属于垃圾邮件的概率与属于正常邮件的概率的比值。上边式子中的每一项，都可以用前面介绍的统计学方法得到。 假设：<br>$$<br>\begin{align}<br>P(常年|垃圾)&amp;=5% \<br>P(常年|正常)&amp;=5% \<br>P(代开|垃圾)&amp;=5% \<br>P(发票|垃圾)&amp;=5% \<br>P(代开|正常)&amp;=0.1% \<br>P(发票|正常)&amp;=0.1% \<br>\end{align}<br>$$<br>那么上式比值为2500，即多个词（或字）联合认定，这封邮件是垃圾邮件的概率是正常邮件概率的2500倍，可以确定是垃圾邮件了。</p><h2 id="讲讲AC自动机算法，与KMP的区别"><a href="#讲讲AC自动机算法，与KMP的区别" class="headerlink" title="讲讲AC自动机算法，与KMP的区别"></a>讲讲AC自动机算法，与KMP的区别</h2><h2 id="讲讲stacking，如果stacking若干模型后没有提升，可能的原因有哪些"><a href="#讲讲stacking，如果stacking若干模型后没有提升，可能的原因有哪些" class="headerlink" title="讲讲stacking，如果stacking若干模型后没有提升，可能的原因有哪些"></a>讲讲stacking，如果stacking若干模型后没有提升，可能的原因有哪些</h2><p><strong>Reference:</strong></p><p>1、<a href="https://blog.csdn.net/lyb3b3b/article/details/74993327">Python3中的bytes和str类型</a></p><p>2、<a href="https://blog.csdn.net/LonelyRoamer/article/details/7670869">数据在计算机中的存储形式和运算</a></p><p>3、<a href="https://zhuanlan.zhihu.com/p/75790486">贝叶斯</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux Tools</title>
      <link href="2020/09/08/linux-tools/"/>
      <url>2020/09/08/linux-tools/</url>
      
        <content type="html"><![CDATA[<h2 id="bash-profile和-bashrc的区别："><a href="#bash-profile和-bashrc的区别：" class="headerlink" title=".bash_profile和.bashrc的区别："></a>.bash_profile和.bashrc的区别：</h2><blockquote><p>/etc/profile: 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从/etc/profile.d目录的配置文件中搜集shell的设置.</p><p>/etc/bashrc:  为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.</p><p>~/.bash_profile: 每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.</p><p>~/.bashrc: 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该该文件被读取.</p><p>~/.bash_logout: 当每次退出系统(退出bash shell)时,执行该文件.</p></blockquote><p>需要注意的是：在/etc/profile里设置系统环境变量时，路径末尾不能以”/“结尾，否则将导致整个PATH变量出错；另外，/etc/profile中设定的变量（全局）的可以作用于任何用户，而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量，他们是”父子”关系。</p><h2 id="vim配置"><a href="#vim配置" class="headerlink" title="vim配置"></a>vim配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;自动缩进</span></span><br><span class="line"><span class="string">set autoindent</span></span><br><span class="line"><span class="string">&quot;</span>类似C语言风格的缩进</span><br><span class="line"><span class="built_in">set</span> cindent</span><br><span class="line"><span class="string">&quot;智能缩进:每一行都和前一行有相同的缩进量,</span></span><br><span class="line"><span class="string">&quot;</span>同时这种缩进形式能正确的识别出花括号,当遇到右花括号（&#125;）,</span><br><span class="line"><span class="string">&quot;则取消缩进形式。此外还增加了识别C语言关键字的功能。</span></span><br><span class="line"><span class="string">&quot;</span>如果一行是以<span class="comment">#开头的(比如宏)，那么这种格式将会被特殊对待而不采用缩进格式</span></span><br><span class="line"><span class="built_in">set</span> smartindent</span><br><span class="line"><span class="string">&quot;For Python Programmers</span></span><br><span class="line"><span class="string">&quot;</span>autocmd FileType python <span class="built_in">set</span> textwidth=79 <span class="string">&quot; PEP-8 Friendly</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;把输入的tab自动转换成空格，Python用户必用~</span></span><br><span class="line"><span class="string">set expandtab</span></span><br><span class="line"><span class="string">&quot;</span>一个tab键占据4个空格</span><br><span class="line"><span class="built_in">set</span> tabstop=4</span><br><span class="line"><span class="string">&quot;一开始,插入的就是4个空格,此时一旦你再按下一次tab,这次的四个空格就会和上次的四个空格组合起来变成一个制表符</span></span><br><span class="line"><span class="string">set softtabstop=4</span></span><br><span class="line"><span class="string">&quot;</span>每一级自动缩进的空格数</span><br><span class="line"><span class="built_in">set</span> shiftwidth=4</span><br><span class="line"><span class="string">&quot;根据文件类型设定缩进,覆盖掉默认的~</span></span><br><span class="line"><span class="string">autocmd FileType python setlocal tabstop=4 shiftwidth=4 softtabstop=4 textwidth=150</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure><p>F9直接保存并执行代码，在用户目录下.vimrc文件中追加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nnoremap &lt;buffer&gt; &lt;F9&gt; :w&lt;CR&gt;: <span class="built_in">exec</span> <span class="string">&#x27;!python3&#x27;</span> shellescape(@%, 1)&lt;cr&gt;</span><br></pre></td></tr></table></figure><h2 id="Linux-软硬链接"><a href="#Linux-软硬链接" class="headerlink" title="Linux 软硬链接"></a>Linux 软硬链接</h2><h3 id="Linux链接概念"><a href="#Linux链接概念" class="headerlink" title="Linux链接概念"></a>Linux链接概念</h3><p>Linux链接分两种，一种被称为硬链接（Hard Link），另一种被称为符号链接（Symbolic Link）。默认情况下，ln命令产生硬链接。</p><h4 id="硬连接"><a href="#硬连接" class="headerlink" title="硬连接"></a>硬连接</h4><p>硬连接指通过索引节点来进行连接。在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号(Inode Index)。在Linux中，多个文件名指向同一索引节点是存在的。一般这种连接就是硬连接。硬连接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬连接到重要文件，以防止“误删”的功能。其原因如上所述，因为对应该目录的索引节点有一个以上的连接。只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个连接被删除后，文件的数据块及目录的连接才会被释放。也就是说，文件真正删除的条件是与之相关的所有硬连接文件均被删除。</p><h4 id="软连接"><a href="#软连接" class="headerlink" title="软连接"></a>软连接</h4><p>另外一种连接称之为符号连接（Symbolic Link），也叫软连接。软链接文件有类似于Windows的快捷方式。它实际上是一个特殊的文件。在符号连接中，文件实际上是一个文本文件，其中包含的有另一文件的位置信息。</p><h3 id="通过实验加深理解"><a href="#通过实验加深理解" class="headerlink" title="通过实验加深理解"></a>通过实验加深理解</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[oracle@Linux]$ touch f1          <span class="comment"># 创建一个测试文件f1</span></span><br><span class="line">[oracle@Linux]$ ln f1 f2            <span class="comment"># 创建f1的一个硬连接文件f2</span></span><br><span class="line">[oracle@Linux]$ ln -s f1 f3        <span class="comment"># 创建f1的一个符号连接文件f3</span></span><br><span class="line">[oracle@Linux]$ ls -li                 <span class="comment"># -i参数显示文件的inode节点信息</span></span><br><span class="line">total 0</span><br><span class="line">9797648 -rw-r--r--  2 oracle oinstall 0 Apr 21 08:11 f1</span><br><span class="line">9797648 -rw-r--r--  2 oracle oinstall 0 Apr 21 08:11 f2</span><br><span class="line">9797649 lrwxrwxrwx  1 oracle oinstall 2 Apr 21 08:11 f3 -&gt; f1</span><br></pre></td></tr></table></figure><p>从上面的结果中可以看出，硬连接文件f2与原文件f1的inode节点相同，均为9797648，然而符号连接文件的inode节点不同。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[oracle@Linux]$ <span class="built_in">echo</span> <span class="string">&quot;I am f1 file&quot;</span> &gt;&gt;f1</span><br><span class="line">[oracle@Linux]$ cat f1</span><br><span class="line">I am f1 file</span><br><span class="line">[oracle@Linux]$ cat f2</span><br><span class="line">I am f1 file</span><br><span class="line">[oracle@Linux]$ cat f3</span><br><span class="line">I am f1 file</span><br><span class="line">[oracle@Linux]$ rm -f f1</span><br><span class="line">[oracle@Linux]$ cat f2</span><br><span class="line">I am f1 file</span><br><span class="line">[oracle@Linux]$ cat f3</span><br><span class="line">cat: f3: No such file or directory</span><br></pre></td></tr></table></figure><p>通过上面的测试可以看出：当删除原始文件f1后，硬连接f2不受影响，但是符号连接f1文件无效</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>依此您可以做一些相关的测试，可以得到以下全部结论：<br>1).删除符号连接f3,对f1,f2无影响；<br>2).删除硬连接f2，对f1,f3也无影响；<br>3).删除原文件f1，对硬连接f2没有影响，导致符号连接f3失效；<br>4).同时删除原文件f1,硬连接f2，整个文件会真正的被删除。</p><p><strong>Reference:</strong></p><p>1、<a href="https://github.com/Vimjas/vim-python-pep8-indent">vim-python-pep8-indent</a></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RecommendSystem Notes</title>
      <link href="2020/09/08/recommend-system/"/>
      <url>2020/09/08/recommend-system/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是推荐系统"><a href="#什么是推荐系统" class="headerlink" title="什么是推荐系统"></a>什么是推荐系统</h2><p>随着信息技术和互联网的发展，人们逐渐从信息匮乏的时代走到了信息过载的时代。无论是信息消费者还是信息生产者都遇到了很大的挑战：</p><p>其一，作为消费者，如何从大量信息中找到自己感兴趣的信息；</p><p>其二，作为生产者，如何让自己的信息脱颖而出，受到广大用户的关注。</p><p>推荐系统的基本任务是联系用户和物品，解决信息过载的问题，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。</p><p>推荐算法的本质是通过一定的方式将用户和物品联系起来，而不同的推荐系统利用了不同的方式，如好友、用户的历史兴趣记录以及用户的注册信息等。</p><h3 id="推荐系统的几种方式"><a href="#推荐系统的几种方式" class="headerlink" title="推荐系统的几种方式"></a>推荐系统的几种方式</h3><p>社会化推荐（Social Recommendation）</p><p>基于内容的推荐（Content-Based filtering）</p><p>基于协同过滤的推荐（Collaborative filtering）</p><h3 id="推荐系统评测"><a href="#推荐系统评测" class="headerlink" title="推荐系统评测"></a>推荐系统评测</h3><p>准确度</p><p>覆盖度</p><p>新颖度</p><p>惊喜度、信任度、透明度</p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AB-Test</title>
      <link href="2020/09/07/AB-Test/"/>
      <url>2020/09/07/AB-Test/</url>
      
        <content type="html"><![CDATA[<p><strong>Reference：</strong></p><p>1、<a href="https://www.zhihu.com/question/20458233">推荐系统中的A/B测试，谁能详细说下思路？</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>凸集和凸函数</title>
      <link href="2020/09/07/%E5%87%B8%E9%9B%86%E5%92%8C%E5%87%B8%E5%87%BD%E6%95%B0/"/>
      <url>2020/09/07/%E5%87%B8%E9%9B%86%E5%92%8C%E5%87%B8%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h3><p>若集合C内任意两点之间的线段均在集合C内，则称集合C为凸集。即对与凸集，它具备性质：<br>$$<br>若\forall x_1, x_2 \in C, \forall \theta \in [0,1]，则x = \theta · x_1 + (1 - \theta) · x_2 \in C<br>$$</p><h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p><strong>Reference：</strong></p><p>1、<a href="https://blog.csdn.net/mingzhuo_126/article/details/82722455">使用Typora添加数学公式</a></p><p>2、<a href="https://www.cnblogs.com/muyisir/p/11440164.html">Typora 使用 Markdown 嵌入 LaTeX 数学公式符号语法</a></p><p>3、<a href="https://blog.csdn.net/u013914471/article/details/82973812">Markdown常用符号及排版</a></p><p>4、<a href="https://runninggump.github.io/2018/12/05/%E6%88%90%E5%8A%9F%E8%A7%A3%E5%86%B3%E5%9C%A8hexo%E4%B8%AD%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E9%97%AE%E9%A2%98/">成功解决在hexo中无法显示数学公式的问题</a></p><p>5、<a href="https://zhuanlan.zhihu.com/p/51127402">一文详解凸函数和凸优化，干货满满</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据清洗</title>
      <link href="2020/09/04/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
      <url>2020/09/04/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/</url>
      
        <content type="html"><![CDATA[<h2 id="分析数据"><a href="#分析数据" class="headerlink" title="分析数据"></a>分析数据</h2><p>即探索性数据分析，对数据进行摸底，了解数据分布情况。一般可以通过绘制直方图、散点图、箱线图或者QQ图来观察数据的基本分布情况</p><h2 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h2><h2 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h2><h2 id="去重处理"><a href="#去重处理" class="headerlink" title="去重处理"></a>去重处理</h2><p><strong>Reference：</strong></p><p>1、<a href="https://www.cnblogs.com/charlotte77/p/5606926.html">机器学习基础与实践（一）—-数据清洗</a></p><p>2、<a href="https://zhuanlan.zhihu.com/p/112501514">数据分布特征的描述</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GeoHash Notes</title>
      <link href="2020/09/03/geohash-notes/"/>
      <url>2020/09/03/geohash-notes/</url>
      
        <content type="html"><![CDATA[<hr><p>GeoHash只是空间索引的一种方式，特别适合点数据，而对线、面数据采用R树索引更有优势（可参考：**<a href="http://www.cnblogs.com/LBSer/p/3392491.html">深入浅出空间索引：为什么需要空间索引</a>**）。</p><p><strong>Reference:</strong></p><p>1、<a href="https://www.cnblogs.com/LBSer/p/3310455.html"><a href="https://www.cnblogs.com/LBSer/p/3310455.html">GeoHash核心原理解析</a></a></p><p>2、<a href="https://halfrost.com/go_spatial_search/">高效的多维空间点索引算法</a></p><p><a href="https://github.com/GongDexing/Geohash">https://github.com/GongDexing/Geohash</a></p><p><a href="https://guanqr.com/tech/website/hexo-theme-next-comments/">https://guanqr.com/tech/website/hexo-theme-next-comments/</a></p>]]></content>
      
      
      <categories>
          
          <category> GeoHash </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GeoHash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive Notes</title>
      <link href="2020/09/01/hive-notes/"/>
      <url>2020/09/01/hive-notes/</url>
      
        <content type="html"><![CDATA[<h2 id="hive-的一些配置"><a href="#hive-的一些配置" class="headerlink" title="hive 的一些配置"></a>hive 的一些配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//显示所在数据库名</span><br><span class="line"><span class="built_in">set</span> hive.cli.print.current.db=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">//显示字段名</span><br><span class="line"><span class="built_in">set</span> hive.cli.print.header=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">//不显示表名</span><br><span class="line"><span class="built_in">set</span> hive.resultset.use.unique.column.names=<span class="literal">false</span>;</span><br></pre></td></tr></table></figure><p>或者在hive/conf/hive-site.xml文件中添加配置项：</p><a id="more"></a><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--显示数据库名--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--不显示表名--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.resultset.use.unique.column.names<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--显示字段名--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="hive执行聚合任务卡在Kill-Command不动假死"><a href="#hive执行聚合任务卡在Kill-Command不动假死" class="headerlink" title="hive执行聚合任务卡在Kill Command不动假死"></a>hive执行聚合任务卡在Kill Command不动假死</h2><h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><p>在hive中执行了“select count(*) from ll_voucher_issue”后，一直无反应，具体为：</p><p><img src="/2020/09/01/hive-notes/20200831_hive.png" alt="hive"></p><p>但若是执行其他非MR运算如select * from table limit 10，则不会卡住。</p><p>PS:</p><p>1、hadoop伪分布式搭建好后，使用hadoop dfsadmin -report可以查看，是否所有的节点都已经成功启动。</p><p>2、使用jps查看进程，应该会有datanode, nodemanger, namenode, secondrynamenode, resourcemanger这些进程，如果缺少那一个的话，那证明hadoop环境没有成功启动。</p><h3 id="问题原因："><a href="#问题原因：" class="headerlink" title="问题原因："></a>问题原因：</h3><p>猜测是Hive没有连接上mapreduce，通过检查hive-env.sh发现。</p><h3 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h3><p>1、配置hive/conf/hive-env.sh（未尝试，先记录下来）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/apps/hadoop</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/apps/hive/conf</span><br></pre></td></tr></table></figure><p>2、修改hadoop/etc/hadoop/mapred-site.xml（解决）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:8001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>原mapred_site.xml文件配置为：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>Reference:</strong></p><p>1、<a href="https://blog.csdn.net/llhp123/article/details/103754487">hive执行任务MR时卡死/假死</a></p><p>2、<a href="https://blog.csdn.net/wmlove_hqy/article/details/78819023">hive执行job时候假死，kill comman卡住解决办法</a></p><p>3、<a href="https://blog.csdn.net/qq_36743482/article/details/78383964">hive建表语句详解</a></p><p>4、<a href="https://github.com/heibaiying/BigData-Notes">BigData-Notes</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL Notes</title>
      <link href="2020/08/30/sql-notes/"/>
      <url>2020/08/30/sql-notes/</url>
      
        <content type="html"><![CDATA[<h2 id="窗口函数【over-partition-by】"><a href="#窗口函数【over-partition-by】" class="headerlink" title="窗口函数【over partition by】"></a>窗口函数【over partition by】</h2><h3 id="窗口函数功能"><a href="#窗口函数功能" class="headerlink" title="窗口函数功能"></a>窗口函数功能</h3><p>1）同时具有分组和排序的功能</p><p>2）不减少原表的行数</p><p>3）语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;窗口函数&gt; over (partition by &lt;用于分组的列名&gt;</span><br><span class="line">                order by &lt;用于排序的列名&gt;)</span><br></pre></td></tr></table></figure><h3 id="聚合函数group-by与开窗函数over-partition-by的区别"><a href="#聚合函数group-by与开窗函数over-partition-by的区别" class="headerlink" title="聚合函数group by与开窗函数over partition by的区别"></a>聚合函数group by与开窗函数over partition by的区别</h3><p>group by分组汇总后改变表的行数，一行只有一个类别；</p><p>而over partition by分组汇总后不会减少原表中的行数。</p><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>若一张表里面记录了一些用户的ID和其出行的城市信息，写一条SQL输出他出行最高频次的那条记录，表如下。</p><table><thead><tr><th align="left">id</th><th align="left">city</th></tr></thead><tbody><tr><td align="left">10000</td><td align="left">广州</td></tr><tr><td align="left">10000</td><td align="left">广州</td></tr><tr><td align="left">10000</td><td align="left">深圳</td></tr><tr><td align="left">10001</td><td align="left">广州</td></tr><tr><td align="left">10001</td><td align="left">深圳</td></tr><tr><td align="left">10001</td><td align="left">深圳</td></tr></tbody></table><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>1、先对该表出行记录汇总，计算出每个id对应出行city的次数；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT id, city, count(*) AS ctime FROM travel GROUP BY id, city </span><br></pre></td></tr></table></figure><p>2、对步骤1中的出行次数进行排序；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT t1.id, t1.city, ctime, ROW_NUMBER() over ( PARTITION BY t1.id ORDER BY ctime DESC ) AS rank FROM</span><br><span class="line"></span><br><span class="line">  (SELECT id, city, count(*) AS ctime FROM travel GROUP BY id, city ) AS t1 </span><br></pre></td></tr></table></figure><p>3、取出top 1。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT t2.id, t2.city FROM (</span><br><span class="line"></span><br><span class="line">  SELECT t1.id, t1.city, ctime, ROW_NUMBER() over ( PARTITION BY t1.id ORDER BY ctime DESC ) AS rank FROM</span><br><span class="line"></span><br><span class="line">  (SELECT id, city, count(*) AS ctime FROM travel GROUP BY id, city ) AS t1 </span><br><span class="line"></span><br><span class="line">) AS t2</span><br><span class="line"></span><br><span class="line">WHERE t2.rank &#x3D; 1</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><table><thead><tr><th>id</th><th>city</th></tr></thead><tbody><tr><td>10000</td><td>广州</td></tr><tr><td>10001</td><td>深圳</td></tr></tbody></table><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="关于排序"><a href="#关于排序" class="headerlink" title="关于排序"></a>关于排序</h4><p>Rank()：可查询出并列第一，是跳跃查询；两个第一名下来就是第三名；</p><p>Row_number()：是没有重复值的排序（即使两条记录相等也是不重复的），可以利用它来实现分页；</p><p>dense_rank()：连续排序，两个第二名仍然跟着第三名。</p><h4 id="关于group-by和over-partition-by的区别"><a href="#关于group-by和over-partition-by的区别" class="headerlink" title="关于group by和over partition by的区别"></a>关于group by和over partition by的区别</h4><p>group by分组汇总后改变表的行数，一行只有一个类别；</p><p>而over partition by分组汇总不会减少原表中的行数。</p><h2 id="公用表表达式【WITH-AS】"><a href="#公用表表达式【WITH-AS】" class="headerlink" title="公用表表达式【WITH AS】"></a>公用表表达式【WITH AS】</h2><p>对于子查询，可以使用with as句法【公式表达式（CTE，Common Table Expression ）】简化提高可读性，同时其效率快很多。</p><h2 id="Redis日志级别"><a href="#Redis日志级别" class="headerlink" title="Redis日志级别"></a><a href="https://www.cnblogs.com/yizhan-509/p/11675611.html">Redis日志级别</a></h2><p>Redis默认的设置为verbose，开发测试阶段可以用debug，生产模式一般选用notice</p><p>1.debug：会打印出很多信息，适用于开发和测试阶段</p><p>2.verbose（冗长的）：包含很多不太有用的信息，但比debug要清爽一些</p><p>3.notice：适用于生产模式</p><p>4.warning : 警告信息</p><p><strong>Redis：默认配置文件 redis.conf</strong></p><p>Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。普遍用于目前主流的分布式架构系统中，关于redis的详细介绍，见另一篇文章：</p><p><a href="http://link.zhihu.com/?target=https://blog.csdn.net/xuri24/article/details/84990683">redis的安装与介绍blog.csdn.net</a></p><p>redis的<a href="https://www.cnblogs.com/fan-1994716/p/11922874.html">多机数据库实现</a>，主要分为以下三种：</p><p>1.Redis哨兵（Sentinel）</p><p>2.Redis复制（主从）</p><p>3.Redis集群</p><h2 id="binlog模式分三种（row，statement，mixed）"><a href="#binlog模式分三种（row，statement，mixed）" class="headerlink" title="binlog模式分三种（row，statement，mixed）"></a>binlog模式分三种（row，statement，mixed）</h2><h3 id="1-Row"><a href="#1-Row" class="headerlink" title="1.Row"></a>1.Row</h3><p>日志中会记录成每一行数据被修改的形式，然后在slave端再对相同的数据进行修改，只记录要修改的数据，只有value，不会有sql多表关联的情况。<br>优点：在row模式下，bin-log中可以不记录执行的sql语句的上下文相关的信息，仅仅只需要记录那一条记录被修改了，修改成什么样了，所以row的日志内容会非常清楚的记录下每一行数据修改的细节，非常容易理解。而且不会出现某些特定情况下的存储过程和function，以及trigger的调用和出发无法被正确复制问题。<br>缺点：在row模式下，所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容。</p><h3 id="2-Statement"><a href="#2-Statement" class="headerlink" title="2.Statement"></a>2.Statement</h3><p>每一条会修改数据的sql都会记录到master的binlog中，slave在复制的时候sql进程会解析成和原来master端执行多相同的sql再执行。<br><strong>优点</strong>：在statement模式下首先就是解决了row模式的缺点，不需要记录每一行数据的变化减少了binlog日志量，节省了I/O以及存储资源，提高性能。因为他只需要记录在master上所执行的语句的细节以及执行语句的上下文信息。<br><strong>缺点</strong>：在statement模式下，由于他是记录的执行语句，所以，为了让这些语句在slave端也能正确执行，那么他还必须记录每条语句在执行的时候的一些相关信息，也就是上下文信息，以保证所有语句在slave端被执行的时候能够得到和在master端执行时候相同的结果。另外就是，由于mysql现在发展比较快，很多的新功能不断的加入，使mysql的复制遇到了不小的挑战，自然复制的时候涉及到越复杂的内容，bug也就越容易出现。在statement中，目前已经发现不少情况会造成<strong>Mysql的复制出现问题，主要是修改数据的时候使用了某些特定的函数或者功能的时候会出现</strong>，比如：sleep()函数在有些版本中就<strong>不能被正确复制</strong>，在存储过程中使用了last_insert_id()函数，<strong>可能会使slave和master上得到不一致的id</strong>等等。由于row是基于每一行来记录的变化，所以不会出现，类似的问题。</p><h3 id="3-Mixed"><a href="#3-Mixed" class="headerlink" title="3.Mixed"></a>3.Mixed</h3><p>从官方文档中看到，之前的 MySQL 一直都只有基于 statement 的复制模式，直到 5.1.5 版本的 MySQL 才开始支持 row 复制。从 5.0 开始，MySQL 的复制已经解决了大量老版本中出现的无法正确复制的问题。但是由于存储过程的出现，给 MySQL Replication 又带来了更大的新挑战。另外，看到官方文档说，从 5.1.8 版本开始，MySQL 提供了除 Statement 和 Row 之外的第三种复制模式：Mixed，实际上就是前两种模式的结合。<strong>在 Mixed 模式下，MySQL 会根据执行的每一条具体的 SQL 语句来区分对待记录的日志形式，也就是在 statement 和 row 之间选择一种</strong>。新版本中的 statment 还是和以前一样，仅仅记录执行的语句。而新版本的 MySQL 中对 row 模式也被做了优化，并不是所有的修改都会以 row 模式来记录，比如遇到表结构变更的时候就会以 statement 模式来记录，如果 SQL 语句确实就是 update 或者 delete 等修改数据的语句，那么还是会记录所有行的变更。</p><h2 id="Mysql主从复制的实现原理"><a href="#Mysql主从复制的实现原理" class="headerlink" title="Mysql主从复制的实现原理"></a>Mysql主从复制的实现原理</h2><p><img src="/2020/08/30/sql-notes/slave-master.jpg"></p><p><strong>MySQL之间数据复制的基础是二进制日志文件（binary log file）。一台MySQL数据库一旦启用二进制日志后，其作为master，它的数据库中所有操作都会以“事件”的方式记录在二进制日志中，其他数据库作为slave通过一个I/O线程与主服务器保持通信，并监控master的二进制日志文件的变化，如果发现master二进制日志文件发生变化，则会把变化复制到自己的中继日志中，然后slave的一个SQL线程会把相关的“事件”执行到自己的数据库中，以此实现从数据库和主数据库的一致性，也就实现了主从复制。</strong></p><p>实现MySQL主从复制需要进行的配置：</p><p><strong>主服务器：</strong><br>开启二进制日志<br>    配置唯一的server-id<br>    获得master二进制日志文件名及位置<br>    创建一个用于slave和master通信的用户账号<br><strong>从服务器：</strong><br>    配置唯一的server-id<br>    使用master分配的用户账号读取master二进制日志<br>    启用slave服务</p><p><strong>具体实现过程如下：</strong></p><h3 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h3><p>1、主从数据库版本最好一致</p><p>2、主从数据库内数据保持一致</p><p>主数据库：182.92.172.80 /linux</p><p>从数据库：123.57.44.85 /linux</p><h3 id="二、主数据库master修改"><a href="#二、主数据库master修改" class="headerlink" title="二、主数据库master修改"></a>二、主数据库master修改</h3><p><strong>1.修改mysql配置</strong></p><p>找到主数据库的配置文件my.cnf(或者my.ini)，我的在/etc/mysql/my.cnf,在[mysqld]部分插入如下两行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">log-bin=mysql-bin #开启二进制日志</span><br><span class="line">server-id=1 #设置server-id</span><br></pre></td></tr></table></figure><p><strong>2.重启mysql，创建用于同步的用户账号</strong></p><p>打开mysql会话<code>shell&gt;mysql -hlocalhost -uname -ppassword</code></p><p>创建用户并授权：</p><p>用户：rel1</p><p>密码：slavepass</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE USER &#39;repl&#39;@&#39;123.57.44.85&#39; IDENTIFIED BY &#39;slavepass&#39;;#创建用户</span><br><span class="line">mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;repl&#39;@&#39;123.57.44.85&#39;;#分配权限</span><br><span class="line">mysql&gt; flush privileges;   #刷新权限</span><br></pre></td></tr></table></figure><p><strong>3.查看master状态，记录二进制文件名(mysql-bin.000003)和位置(73)：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql &gt; SHOW MASTER STATUS;</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br><span class="line">| mysql-bin.000003 | 73       | test         | manual,mysql     |</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br></pre></td></tr></table></figure><h3 id="三、从服务器slave修改"><a href="#三、从服务器slave修改" class="headerlink" title="三、从服务器slave修改"></a>三、从服务器slave修改</h3><p>1.修改mysql<strong>配置</strong></p><p>同样找到my.cnf配置文件，添加server-id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">server-id=2 #设置server-id，必须唯一</span><br></pre></td></tr></table></figure><p>2.重启mysql，打开mysql会话，执行同步SQL语句(需要主服务器主机名，登陆凭据，二进制文件的名称和位置)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CHANGE MASTER TO</span><br><span class="line">    -&gt;     MASTER_HOST&#x3D;&#39;182.92.172.80&#39;,</span><br><span class="line">    -&gt;     MASTER_USER&#x3D;&#39;rep1&#39;,</span><br><span class="line">    -&gt;     MASTER_PASSWORD&#x3D;&#39;slavepass&#39;,</span><br><span class="line">    -&gt;     MASTER_LOG_FILE&#x3D;&#39;mysql-bin.000003&#39;,</span><br><span class="line">    -&gt;     MASTER_LOG_POS&#x3D;73;</span><br></pre></td></tr></table></figure><p>3.启动slave同步进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; start slave;</span><br></pre></td></tr></table></figure><p>4.查看slave状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show slave status\G;</span></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 182.92.172.80</span><br><span class="line">                  Master_User: rep1</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000013</span><br><span class="line">          Read_Master_Log_Pos: 11662</span><br><span class="line">               Relay_Log_File: mysqld-relay-bin.000022</span><br><span class="line">                Relay_Log_Pos: 11765</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000013</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">              Replicate_Do_DB: </span><br><span class="line">          Replicate_Ignore_DB: </span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>当Slave_IO_Running和Slave_SQL_Running都为YES的时候就表示主从同步设置成功了。接下来就可以进行一些验证了，比如在主master数据库的test数据库的一张表中插入一条数据，在slave的test库的相同数据表中查看是否有新增的数据即可验证主从复制功能是否有效，还可以关闭slave（mysql&gt;stop slave;）,然后再修改master，看slave是否也相应修改（停止slave后，master的修改不会同步到slave），就可以完成主从复制功能的验证了。</p><p>还可以用到的其他相关参数：</p><p>master开启二进制日志后默认记录所有库所有表的操作，可以通过配置来指定只记录指定的数据库甚至指定的表的操作，具体在mysql配置文件的[mysqld]可添加修改如下选项：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">不同步哪些数据库  </span><br><span class="line"></span><br><span class="line">binlog-ignore-db = mysql  </span><br><span class="line">binlog-ignore-db = test  </span><br><span class="line">binlog-ignore-db = information_schema  </span><br><span class="line"></span><br><span class="line">只同步哪些数据库，除此之外，其他不同步  </span><br><span class="line"></span><br><span class="line">binlog-do-db = game  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如之前查看master状态时就可以看到只记录了test库，忽略了manual和mysql库。</p><p><strong>Reference：</strong></p><p>1、<a href="https://blog.csdn.net/smcfy/article/details/42713651">insert … on duplicate key update column=IF(条件,值1,值2 ) 简直神一样的操作</a></p><p>2、<a href="https://www.cnblogs.com/lcngu/p/5335170.html">OVER(PARTITION BY)函数介绍</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo Notes</title>
      <link href="2020/08/29/hexo-notes/"/>
      <url>2020/08/29/hexo-notes/</url>
      
        <content type="html"><![CDATA[<hr><p><a href="https://hexo.io/">Hexo</a>是一个快速、简洁而又高效的博客框架。Hexo使用Markdown解析文章，常与Github、gitlab结合组成静态网页托管服务。</p><h2 id="基本环境准备"><a href="#基本环境准备" class="headerlink" title="基本环境准备"></a>基本环境准备</h2><p>1、首先分别安装<a href="https://git-scm.com/">Git</a>和<a href="https://nodejs.org/zh-cn/">Node.js</a>，然后在命令行中输入相应的命令验证是否成功。</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git version</span><br><span class="line">$ git version 2.25.0.windows.1</span><br><span class="line"></span><br><span class="line">$ node -v</span><br><span class="line">$ v12.18.3</span><br><span class="line"></span><br><span class="line">$ npm -v</span><br><span class="line">$ 6.14.6</span><br></pre></td></tr></table></figure><h2 id="开始搭建Hexo系统"><a href="#开始搭建Hexo系统" class="headerlink" title="开始搭建Hexo系统"></a>开始搭建Hexo系统</h2><p>1、安装Hexo</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-cli -g</span><br></pre></td></tr></table></figure><p>2、开始搭建博客</p><p>新建一个文件夹，如E:\myBlog，从终端进入myBlog文件夹后，输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init hexoBlog  //项目目录</span><br><span class="line">$ <span class="built_in">cd</span> HexoBlog         //进入项目目录</span><br><span class="line">$ npm install         // 安装</span><br><span class="line">$ hexo server         //启动服务</span><br></pre></td></tr></table></figure><p>然后在浏览器中输入lcalhost:4000，即可查看hexo默认landscape主题界面。</p><h2 id="选择主题"><a href="#选择主题" class="headerlink" title="选择主题"></a>选择主题</h2><p>1、Hexo提供了丰富的<a href="https://hexo.io/themes/">主题</a>，这里以切换为<a href="https://github.com/theme-next/hexo-theme-next">Next主题</a>为例。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> hexoBlog</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><p>2、下载完成后，在<strong>站点配置文件</strong>~/HexoBlog/_config.yml文件中，找到theme字段，将landscape修改为 next。然后在终端输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean   //清除缓存</span><br><span class="line">$ hexo g       //重新生成代码</span><br><span class="line">$ hexo s       //部署到本地</span><br></pre></td></tr></table></figure><p>3、选择scheme（参考<a href="https://theme-next.iissnan.com/getting-started.html">Next</a>文档）</p><p>Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是：</p><ul><li>Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白</li><li>Mist - Muse 的紧凑版本，整洁有序的单栏外观</li><li>Pisces - 双栏 Scheme，小家碧玉似的清新</li></ul><p>Scheme 的切换通过更改 <strong>主题配置文件</strong>，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 <code>#</code> 去除即可。</p><p>选择 Pisces Scheme</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">scheme: Pisces</span><br></pre></td></tr></table></figure><h2 id="发布至Github，使得外网可以访问文章"><a href="#发布至Github，使得外网可以访问文章" class="headerlink" title="发布至Github，使得外网可以访问文章"></a>发布至Github，使得外网可以访问文章</h2><p>1、注册GitHub后，新建一个Repository，项目名必须遵守格式：账户名.github.io，并在Initialize this repository with栏目中勾选Add a README file，保存。</p><p>2、在项目右侧的Setting进去，下拉至GitHub Pages选择主题保存后，会发现下新建的项目已经发布至外网。</p><p>3、将Hexo与Github Pages联系起来</p><p>3.1、首先要先配置SSH Key</p><p>打开命令行输入<code>cd ~.ssh</code>，如果没有报错，就说明是以前生成过的，直接使用<code>cat ~/.ssh/id_ras.pub</code>即可查看本机SSH Key。</p><p>如果报错，则执行命令全局配置本地账户：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&#x27;your name&#x27;</span></span><br><span class="line">git config --global user.email <span class="string">&#x27;your email&#x27;</span></span><br><span class="line"></span><br><span class="line">//然后生成密钥</span><br><span class="line">ssh-keygen -t rsa -C <span class="string">&#x27;your email&#x27;</span></span><br><span class="line"></span><br><span class="line">//确认并添加主机到本机SSH可信列表</span><br><span class="line">ssh -T git@github.com</span><br><span class="line">//若返回 Hi, XXXX! you<span class="string">&#x27;ve succcessfully authenticate,but ithub does not provide shell access.</span></span><br></pre></td></tr></table></figure><p>3.2、登录github，将本机SSH Key添加到SSH and GPG keys中。</p><p>3.3、打开站点配置文件，在文件末尾添加配置如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: </span><br><span class="line">    github: https://github.com/[your github name]/[your github name].github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>3.4、安装hexo-deployer-git插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>3.5、发布</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g -d</span><br></pre></td></tr></table></figure><p>官方文档：<a href="https://hexo.io/zh-cn/docs/github-pages">部署Heo到GitHub Pages</a></p><p>3.6、开始写作</p><p>新建文章</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new <span class="string">&#x27;文章标题&#x27;</span></span><br></pre></td></tr></table></figure><p>然后在站点目录下(/source/_post)，可以看到新建的文章标题</p><p>执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>即可在本地查阅到文章，最后，部署到GitHub上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure><p>其中hexo clean适用于清除缓存文件（db.json）和已生成的静态文件（public），在某些情况下，尤其是更换主题后，发现对站点的更新不生效，可能需要执行该命令。</p><p>3.7、Git出错</p><p><img src="/2020/08/29/hexo-notes/git_fatal.png"></p><p>问题原因：</p><p>远程仓库的文件过大，需要设置本地仓库大小</p><p>解决办法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、首先输入如下命令：</span></span><br><span class="line">git config http.sslVerify &quot;false&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 若出现下列错误：</span></span><br><span class="line">fatal: not in a git directory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">再继续执行 git config --global http.sslVerify <span class="string">&quot;false&quot;</span> 问题解决</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、文件大小的上限设置：</span></span><br><span class="line">git config --global http.postBuffer 524288000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、如果还是git代码还是下载失败，则需要继续修改git缓存的大小</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 参考链接：https://www.jianshu.com/p/fb2b6a19436a</span></span><br></pre></td></tr></table></figure><h2 id="添加阅读统计代码"><a href="#添加阅读统计代码" class="headerlink" title="添加阅读统计代码"></a>添加阅读统计代码</h2><p>1、打开next主题配置文件\themes\next\_config.yml，搜索找到<strong>busuanzi_count</strong>，把enable设置为true。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show Views / Visitors of the website / page with busuanzi.</span></span><br><span class="line"><span class="comment"># Get more information on http://ibruce.info/2015/04/04/busuanzi</span></span><br><span class="line">busuanzi_count:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  total_visitors: <span class="literal">true</span>   <span class="comment">#统计访客数</span></span><br><span class="line">  total_visitors_icon: user</span><br><span class="line">  total_views: <span class="literal">true</span>    <span class="comment">#统计访问数</span></span><br><span class="line">  total_views_icon: eye</span><br><span class="line">  <span class="comment">#post_views: true   #统计文章阅读数,如果有重复阅读次数，请注释这段代码</span></span><br><span class="line">  <span class="comment">#post_views_icon: eye</span></span><br></pre></td></tr></table></figure><p>2、同样是在next主题配置文件\themes\next\_config.yml下，搜索<strong>footer</strong>，在它底下添加counter，设值为true。</p><p><strong>PS: 最新版的next主题无需修改。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#统计</span></span><br><span class="line">counter: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>3、来到themes\next\layout_partials，找到<strong>footer.swig</strong>文件，打开编辑，在底下添加代码。</p><p><strong>PS: 最新版的next主题无需修改。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> theme.footer.counter %&#125;</span><br><span class="line">    &lt;script async src=<span class="string">&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;</span>&gt;&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>4、重启Hexo服务即可。</p><h2 id="hexo中插入数学公式"><a href="#hexo中插入数学公式" class="headerlink" title="hexo中插入数学公式"></a>hexo中插入数学公式</h2><blockquote><p>原生hexo并不支持数学公式，需要安装插件 <a href="https://www.mathjax.org/">mathJax</a>。<a href="https://www.mathjax.org/">mathJax</a> 是一款运行于浏览器中的开源数学符号渲染引擎，使用 <a href="https://www.mathjax.org/">mathJax</a> 可以方便的在浏览器中嵌入数学公式。<a href="https://www.mathjax.org/">mathJax</a> 使用网络字体产生高质量的排版，因此可适应各种分辨率，它的显示是基于文本的而非图片，因此显示效果更好。这些公式可以被搜索引擎使用，因此公式里的符号一样可以被搜索引擎检索到。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> npm install hexo-math --save</span></span><br></pre></td></tr></table></figure><p>在站点配置文件 <em>_config.yml</em> 中添加：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">math:</span></span><br><span class="line">  <span class="attr">engine:</span> <span class="string">&#x27;mathjax&#x27;</span> <span class="comment"># or &#x27;katex&#x27;</span></span><br><span class="line">  <span class="attr">mathjax:</span></span><br><span class="line">    <span class="comment"># src: custom_mathjax_source</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="comment"># MathJax config</span></span><br></pre></td></tr></table></figure><p>在 next 主题配置文件中 <em>themes/next-theme/_config.yml</em> 中将 mathJax 设为 true:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MathJax Support</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">cdn:</span> <span class="string">//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span></span><br></pre></td></tr></table></figure><h3 id="分数与开方"><a href="#分数与开方" class="headerlink" title="分数与开方"></a>分数与开方</h3><p>可以使用\frac 或者 \over 实现分数的显示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$\frac xy$</span><br><span class="line">$ x+3 \over y+5 $</span><br></pre></td></tr></table></figure><p>分别显示为：$\frac xy$和$x+3 \over y+5$。</p><p>开方使用\sqrt:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ \sqrt&#123;x^5&#125; $</span><br><span class="line">$ \sqrt[3]&#123;\frac xy&#125; $</span><br></pre></td></tr></table></figure><p>分别显示为：$\sqrt{x^5}$和 $\sqrt[3]{\frac xy}$。</p><h3 id="求和与积分"><a href="#求和与积分" class="headerlink" title="求和与积分"></a>求和与积分</h3><p>求和使用\sum,可加上下标，积分使用\int可加上下限，双重积分用\iint:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ \sum_&#123;i&#x3D;0&#125;^n $</span><br><span class="line">$ \int_1^\infty $</span><br><span class="line">$ \iint_1^\infty $</span><br></pre></td></tr></table></figure><p>分别显示:$\sum_{i=0}^n$和$\int_1^\infty$以及$\iint_1^\infty$。</p><h3 id="极限"><a href="#极限" class="headerlink" title="极限"></a>极限</h3><p>极限使用\lim:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ \lim_&#123;x \to 0&#125; $</span><br></pre></td></tr></table></figure><p>显示为：$\lim_{x \to 0}$</p><h3 id="表格与矩阵"><a href="#表格与矩阵" class="headerlink" title="表格与矩阵"></a>表格与矩阵</h3><p>表格样式lcr表示居中，|加入一条竖线，\hline表示行间横线，列之间用&amp;分隔，行之间用\分隔</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;array&#125;&#123;c|lcr&#125;</span><br><span class="line">n &amp; \text&#123;Left&#125; &amp; \text&#123;Center&#125; &amp; \text&#123;Right&#125; \\</span><br><span class="line">\hline</span><br><span class="line">1 &amp; 1.97 &amp; 5 &amp; 12 \\</span><br><span class="line">2 &amp; -11 &amp; 19 &amp; -80 \\</span><br><span class="line">3 &amp; 70 &amp; 209 &amp; 1+i \\</span><br><span class="line">\end&#123;array&#125;$$</span><br></pre></td></tr></table></figure><p>显示效果为：</p><p>$$\begin{array}{c|lcr}n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\hline 1 &amp; 1.97 &amp; 5 &amp; 12 \2 &amp; -11 &amp; 19 &amp; -80 \3 &amp; 70 &amp; 209 &amp; 1+i \ \end{array}$$</p><p>表格的插入也可以使用以下方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">名称|说明</span><br><span class="line">---|---|---</span><br><span class="line">temperature|  室内温度</span><br><span class="line">set temperature|  设定温度</span><br><span class="line">height|  室内高度</span><br></pre></td></tr></table></figure><p>显示效果：</p><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody><tr><td>temperature</td><td>室内温度</td></tr><tr><td>set temperature</td><td>设定温度</td></tr><tr><td>height</td><td>室内高度</td></tr></tbody></table><p>矩阵显示和表格很相似　　</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$$\left[</span><br><span class="line">\begin&#123;matrix&#125;</span><br><span class="line">V_A \\\\</span><br><span class="line">V_B \\\\</span><br><span class="line">V_C \\\\</span><br><span class="line">\end&#123;matrix&#125;</span><br><span class="line">\right] &#x3D;</span><br><span class="line">\left[</span><br><span class="line">\begin&#123;matrix&#125;</span><br><span class="line">1 &amp; 0 &amp; L \\\\</span><br><span class="line">-cosψ &amp; sinψ &amp; L \\\\</span><br><span class="line">-cosψ &amp; -sinψ &amp; L</span><br><span class="line">\end&#123;matrix&#125;</span><br><span class="line">\right]</span><br><span class="line">\left[</span><br><span class="line">\begin&#123;matrix&#125;</span><br><span class="line">V_x \\\\</span><br><span class="line">V_y \\\\</span><br><span class="line">W \\\\</span><br><span class="line">\end&#123;matrix&#125;</span><br><span class="line">\right] $$</span><br></pre></td></tr></table></figure><p>$$\left[<br>\begin{matrix}<br>V_A \\<br>V_B \\<br>V_C \\<br>\end{matrix}<br>\right] =<br>\left[<br>\begin{matrix}<br>1 &amp; 0 &amp; L \\<br>-cosψ &amp; sinψ &amp; L \\<br>-cosψ &amp; -sinψ &amp; L<br>\end{matrix}<br>\right]<br>\left[<br>\begin{matrix}<br>V_x \\<br>V_y \\<br>W \\<br>\end{matrix}<br>\right]$$</p><p>需要注意的是，如果将markdown文件发布至hexo，需要要文件开头打开mathjax的开关，否则公式还是不会显示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">title: Hexo Notes</span><br><span class="line">date: 2020-08-29 16:00:00</span><br><span class="line">categories:</span><br><span class="line">- hexo</span><br><span class="line">tags:</span><br><span class="line">- hexo</span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><h2 id="为Hexo添加tags、categories属性"><a href="#为Hexo添加tags、categories属性" class="headerlink" title="为Hexo添加tags、categories属性"></a>为Hexo添加tags、categories属性</h2><h3 id="创建“分类”选项"><a href="#创建“分类”选项" class="headerlink" title="创建“分类”选项"></a>创建“分类”选项</h3><h4 id="生成“分类”页并添加tpye属性"><a href="#生成“分类”页并添加tpye属性" class="headerlink" title="生成“分类”页并添加tpye属性"></a>生成“分类”页并添加tpye属性</h4><p>打开命令行，进入博客所在文件夹。执行命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new page categories</span><br></pre></td></tr></table></figure><p>成功后会提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INFO  Created: ~&#x2F;Documents&#x2F;blog&#x2F;source&#x2F;categories&#x2F;index.md</span><br></pre></td></tr></table></figure><p>根据上面的路径，找到<code>index.md</code>这个文件，打开后默认内容是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章分类</span><br><span class="line">date: 2017-05-27 13:47:40</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>添加<code>type: &quot;categories&quot;</code>到内容中，添加后是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章分类</span><br><span class="line">date: 2017-05-27 13:47:40</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>保存并关闭文件。</p><h4 id="给文章添加“categories”属性"><a href="#给文章添加“categories”属性" class="headerlink" title="给文章添加“categories”属性"></a>给文章添加“categories”属性</h4><p>打开需要添加分类的文章，为其添加categories属性。下方的<code>categories: web前端</code>表示添加这篇文章到“web前端”这个分类。注意：hexo一篇文章只能属于一个分类，也就是说如果在“- web前端”下方添加“-xxx”，hexo不会产生两个分类，而是把分类嵌套（即该文章属于 “- web前端”下的 “-xxx ”分类）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: jQuery对表单的操作及更多应用</span><br><span class="line">date: 2017-05-26 12:12:57</span><br><span class="line">categories: </span><br><span class="line">- web前端</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>至此，成功给文章添加分类，点击首页的“分类”可以看到该分类下的所有文章。当然，只有添加了<code>categories: xxx</code>的文章才会被收录到首页的“分类”中。</p><h3 id="创建“标签”选项"><a href="#创建“标签”选项" class="headerlink" title="创建“标签”选项"></a>创建“标签”选项</h3><h4 id="生成“标签”页并添加tpye属性"><a href="#生成“标签”页并添加tpye属性" class="headerlink" title="生成“标签”页并添加tpye属性"></a>生成“标签”页并添加tpye属性</h4><p>打开命令行，进入博客所在文件夹。执行命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new page tags</span><br></pre></td></tr></table></figure><p>成功后会提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INFO  Created: ~&#x2F;Documents&#x2F;blog&#x2F;source&#x2F;tags&#x2F;index.md</span><br></pre></td></tr></table></figure><p>根据上面的路径，找到<code>index.md</code>这个文件，打开后默认内容是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 标签</span><br><span class="line">date: 2017-05-27 14:22:08</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>添加<code>type: &quot;tags&quot;</code>到内容中，添加后是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章分类</span><br><span class="line">date: 2017-05-27 13:47:40</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>保存并关闭文件。</p><h4 id="给文章添加“tags”属性"><a href="#给文章添加“tags”属性" class="headerlink" title="给文章添加“tags”属性"></a>给文章添加“tags”属性</h4><p>打开需要添加标签的文章，为其添加tags属性。下方的<code>tags:</code>下方的<code>- jQuery</code> <code>- 表格</code><br><code>- 表单验证</code>就是这篇文章的标签了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: jQuery对表单的操作及更多应用</span><br><span class="line">date: 2017-05-26 12:12:57</span><br><span class="line">categories: </span><br><span class="line">- web前端</span><br><span class="line">tags:</span><br><span class="line">- jQuery</span><br><span class="line">- 表格</span><br><span class="line">- 表单验证</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>至此，成功给文章添加分类，点击首页的“标签”可以看到该标签下的所有文章。当然，只有添加了<code>tags: xxx</code>的文章才会被收录到首页的“标签”中。</p><p>细心的朋友可能已经发现，这两个的设置几乎一模一样！是的，没错，思路都是一样的。所以我们可以打开scaffolds/post.md文件，在tages:上面加入categories:,保存后，之后执行<code>hexo new 文章名</code>命令生成的文件，页面里就有<code>categories:</code>项了。</p><p>scaffolds目录下，是新建页面的模板，执行新建命令时，是根据这里的模板页来完成的，所以可以在这里根据你自己的需求添加一些默认值。</p><h2 id="添加字数统计和阅读时长"><a href="#添加字数统计和阅读时长" class="headerlink" title="添加字数统计和阅读时长"></a>添加字数统计和阅读时长</h2><p>1、首先安装一个插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-symbols-count-time --save</span><br></pre></td></tr></table></figure><p>2、接着在<strong>站点配置文件</strong>~/HexoBlog/_config.yml文件中，添加以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  symbols: <span class="literal">true</span></span><br><span class="line">  tiem: <span class="literal">true</span></span><br><span class="line">  total_symbols: <span class="literal">true</span></span><br><span class="line">  total_time: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>3、最后在next主题的配置文件中开启symbols_count_time字段（可能与busuanzi统计文章阅读数重复）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/hexo-symbols-count-time</span></span><br><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: <span class="literal">true</span></span><br><span class="line">  item_text_post: <span class="literal">true</span></span><br><span class="line">  item_text_total: <span class="literal">true</span></span><br><span class="line">  awl: 4</span><br><span class="line">  wpm: 300</span><br></pre></td></tr></table></figure><p>重启一下Hexo即可。</p><h2 id="添加评论系统"><a href="#添加评论系统" class="headerlink" title="添加评论系统"></a>添加评论系统</h2><p>本文使用了Gitalk评论系统，Gitalk是一个基于Github Issue的评论插件，使用Github账号登录，界面干净整洁，最喜欢的一点是支持 <code>MarkDown</code> 语法，在个人博客里添加了之后就可以很简便的进行评论和回复了。</p><h3 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h3><ul><li><p>使用 Github 登录</p></li><li><p>支持多语言 <code>[en, zh-CN, zh-TW, es-ES, fr]</code></p></li><li><p>支持个人或组织</p></li><li><p>无干扰模式（设置 <code>distractionFreeMode</code> 为 <code>true</code> 开启）</p></li><li><p>快捷键提交评论 （<code>cmd</code> | <code>ctrl + enter</code>）</p></li><li><p>支持 <code>MarkDown</code> 语法</p></li></ul><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="创建Github仓库"><a href="#创建Github仓库" class="headerlink" title="创建Github仓库"></a>创建Github仓库</h4><ul><li>需要在自己的 <code>Github</code> 账号下创建一个仓库来存放评论，创建的仓库只要 <code>public</code> 就行，其余没要求。</li></ul><h4 id="创建-Github-Application"><a href="#创建-Github-Application" class="headerlink" title="创建 Github Application"></a>创建 Github Application</h4><ul><li><p>需要创建一个 <code>Github Application</code> 用来授权登录，如果没有 <a href="https://github.com/settings/applications/new">点击这里申请</a>，<code>Authorization callback URL</code> 填写你主页地址，比如我的就是 <code>https://[yourname].github.io/</code>，其他都随意填。</p></li><li><p>授权路径：Github首页–IconLogo–Setting–Developer setting–OAuth Apps–New OAuth Apps。Authorization callback URL和Homepage URL写得一样。</p></li></ul><h4 id="为-Next-主题添加-Gitalk-支持"><a href="#为-Next-主题添加-Gitalk-支持" class="headerlink" title="为 Next 主题添加 Gitalk 支持"></a>为 Next 主题添加 Gitalk 支持</h4><ul><li><p>首先创建 <code>Gitalk</code> 的 <code>swig</code> 文件，放在 <code>themes/next/layout/_third-party/comments</code> 文件夹下，命名为 <code>gitalk.swig</code> ，内容如下：</p></li><li><p><strong>PS: 最新版的next主题无需修改。</strong></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125;</span><br><span class="line">  <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">href</span>=<span class="string">&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;gitalk-container&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span></span><br><span class="line"><span class="javascript">    <span class="keyword">var</span> gitalk = <span class="keyword">new</span> Gitalk(&#123;</span></span><br><span class="line"><span class="handlebars"><span class="xml">      clientID: &#x27;</span><span class="template-variable">&#123;&#123; <span class="name">theme.gitalk.clientID</span> &#125;&#125;</span><span class="xml">&#x27;,</span></span></span><br><span class="line"><span class="handlebars"><span class="xml">      clientSecret: &#x27;</span><span class="template-variable">&#123;&#123; <span class="name">theme.gitalk.clientSecret</span> &#125;&#125;</span><span class="xml">&#x27;,</span></span></span><br><span class="line"><span class="handlebars"><span class="xml">      repo: &#x27;</span><span class="template-variable">&#123;&#123; <span class="name">theme.gitalk.repo</span> &#125;&#125;</span><span class="xml">&#x27;,</span></span></span><br><span class="line"><span class="handlebars"><span class="xml">      owner: &#x27;</span><span class="template-variable">&#123;&#123; <span class="name">theme.gitalk.owner</span> &#125;&#125;</span><span class="xml">&#x27;,</span></span></span><br><span class="line"><span class="handlebars"><span class="xml">      admin: [&#x27;</span><span class="template-variable">&#123;&#123; <span class="name">theme.gitalk.admin</span> &#125;&#125;</span><span class="xml">&#x27;],</span></span></span><br><span class="line">      id: location.pathname,</span><br><span class="line"><span class="handlebars"><span class="xml">      distractionFreeMode: &#x27;</span><span class="template-variable">&#123;&#123; <span class="name">theme.gitalk.distractionFreeMode</span> &#125;&#125;</span><span class="xml">&#x27;</span></span></span><br><span class="line">    &#125;)</span><br><span class="line"><span class="javascript">    gitalk.render(<span class="string">&#x27;gitalk-container&#x27;</span>)</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></li><li><p>在主题文件 <code>themes/next/layout/_third-party/comments/index.swig</code> 中引入刚刚添加的文件。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% include &#x27;gitalk.swig&#x27; %&#125;</span><br></pre></td></tr></table></figure></li><li><p>在 <code>themes/next/layout/_partials/comments.swig</code> 文件末找到最后的 <code>&#123;% endif %&#125;</code> 语句，替换为如下代码：</p></li><li><p><strong>PS: 最新版的next主题无需修改。</strong></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% elseif theme.gitalk.enable %&#125;</span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;gitalk-container&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></li><li><p>在 <code>themes/next/_config.yml</code> 文件中添加 <code>Gitalk</code> 的配置。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gitalk</span></span><br><span class="line"><span class="comment"># For more information: https://gitalk.github.io, https://github.com/gitalk/gitalk</span></span><br><span class="line"><span class="attr">gitalk:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">github_id:</span> [<span class="string">yourname</span>] <span class="comment"># GitHub repo owner</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">hexo_comments</span> <span class="comment"># Repository name to store issues</span></span><br><span class="line">  <span class="attr">client_id:</span> <span class="string">xxxxxxxxxxxx</span> <span class="comment"># GitHub Application Client ID</span></span><br><span class="line">  <span class="attr">client_secret:</span> <span class="string">xxxxxxxxxxxxxxxx</span> <span class="comment"># GitHub Application Client Secret</span></span><br><span class="line">  <span class="attr">admin_user:</span> [<span class="string">yourname</span>] <span class="comment"># GitHub repo owner and collaborators, only these guys can initialize gitHub issues</span></span><br><span class="line">  <span class="attr">distraction_free_mode:</span> <span class="literal">false</span> <span class="comment"># Facebook-like distraction free mode</span></span><br><span class="line">  <span class="comment"># Gitalk&#x27;s display language depends on user&#x27;s browser or system environment</span></span><br><span class="line">  <span class="comment"># If you want everyone visiting your site to see a uniform language, you can set a force language value</span></span><br><span class="line">  <span class="comment"># Available values: en | es-ES | fr | ru | zh-CN | zh-TW</span></span><br><span class="line">  <span class="attr">language:</span> <span class="string">en</span></span><br></pre></td></tr></table></figure></li><li><p>最后执行 <code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code> 重新发布博客即可。</p></li></ul><p><strong>附加内容：</strong></p><p>由于国内网络原因，gitlab会有时出现Error: Network Error的错误导致登录状态异常，或者是不能登录。因此本文在Github Pages页面上添加了Valine的评论系统，主要过程如下：</p><p>1、在<a href="https://console.leancloud.app/login.html#/signin">LeanCloud</a>上注册一个账号【华东节点和国际站账号不能通用，要记得自己注册的时候选择的是哪个节点，我使用的是国际站节点】，然后注册随意填写就行，保存后会生成appid和appkey。</p><p>2、在next主题配置文件下，找到Valine配置选项，我的配置为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Valine</span></span><br><span class="line"><span class="comment"># For more information: https://valine.js.org, https://github.com/xCss/Valine</span></span><br><span class="line">valine:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  appid: xxxxxxxxxxxx <span class="comment"># Your leancloud application appid</span></span><br><span class="line">  appkey: xxxxxxxxxxxxx <span class="comment"># Your leancloud application appkey</span></span><br><span class="line">  notify: <span class="literal">false</span> <span class="comment"># Mail notifier</span></span><br><span class="line">  verify: <span class="literal">false</span> <span class="comment"># Verification code</span></span><br><span class="line">  placeholder: Just go go <span class="comment"># Comment box placeholder</span></span><br><span class="line">  avatar: mm <span class="comment"># Gravatar style</span></span><br><span class="line">  guest_info: nick,mail,link <span class="comment"># Custom comment header</span></span><br><span class="line">  pageSize: 10 <span class="comment"># Pagination size</span></span><br><span class="line">  language: <span class="comment"># Language, available values: en, zh-cn</span></span><br><span class="line">  visitor: <span class="literal">true</span> <span class="comment"># Article reading statistic</span></span><br></pre></td></tr></table></figure><p>将上述步骤一的appid和appkey复制替换到配置文件里面即可。</p><h2 id="hexo的next主题个性化教程，打造炫酷网站"><a href="#hexo的next主题个性化教程，打造炫酷网站" class="headerlink" title="hexo的next主题个性化教程，打造炫酷网站"></a>hexo的next主题个性化教程，打造炫酷网站</h2><p>参考：<a href="https://www.jianshu.com/p/f054333ac9e6">hexo的next主题个性化教程:打造炫酷网站</a></p><h2 id="电脑重装系统后如何恢复Hexo博客"><a href="#电脑重装系统后如何恢复Hexo博客" class="headerlink" title="电脑重装系统后如何恢复Hexo博客"></a>电脑重装系统后如何恢复Hexo博客</h2><p>参考：<a href="https://garveyzhong.gitee.io/2020/05/27/hexo/%E7%94%B5%E8%84%91%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%90%8E%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8DHexo%E5%8D%9A%E5%AE%A2/">电脑重装系统后如何恢复Hexo博客</a></p><h2 id="hexo使用markdown图片无法显示问题"><a href="#hexo使用markdown图片无法显示问题" class="headerlink" title="hexo使用markdown图片无法显示问题"></a>hexo使用markdown图片无法显示问题</h2><p><code>hexo</code>默认无法自动处理文章插入本地图片，需要通过扩展插件支持。</p><h3 id="图片路径问题"><a href="#图片路径问题" class="headerlink" title="图片路径问题"></a>图片路径问题</h3><p>配置<code>_config.yml</code>里面的<code>post_asset_folder:false</code>这个选项设置为<code>true</code>。</p><p>安装<a href="https://github.com/7ym0n/hexo-asset-image.git">hexo-asset-image</a>，运行<code>hexo n &quot;xxxx&quot;</code>来生成md博文时，<code>/source/_posts</code>文件夹内除了xxxx.md文件还有一个同名的文件夹，把图片放入该文件夹。</p><p>使用<code>![xxx](xxx/xxx.png)</code>直接插入图片即可。</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>由于hexo3版本后对很多插件支持有问题，<a href="https://github.com/CodeFalling/hexo-asset-image.git">hexo-asset-image</a>插件在处理<code>date.permalink</code>链接时出现路径错误，把年月去掉了，导致最后生成的路径为<code>%d/xxx/xxx</code>需要对其做兼容处理。通过判断当前版本是否等于<code>3</code>的版本做不同的路径分割。</p><p>在代码中加入：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> version = <span class="built_in">String</span>(hexo.version).split(<span class="string">&#x27;.&#x27;</span>);</span><br></pre></td></tr></table></figure><p>修改<code>date.permalink</code>处理：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> link = date.permalink;  </span><br><span class="line"><span class="keyword">if</span>(version.length &gt; <span class="number">0</span> &amp;&amp; <span class="built_in">Number</span>(version[<span class="number">0</span>]) == <span class="number">3</span>) </span><br><span class="line">    <span class="keyword">var</span> beginPos = getPosition(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">1</span>) + <span class="number">1</span>; </span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">    <span class="keyword">var</span> beginPos = getPosition(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">3</span>) + <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>重新生成静态文件即可正确显示。</p><p>可直接安装已经修改过得插件<code>npm install https://github.com/7ym0n/hexo-asset-image --save</code>。</p><h2 id="添加网易云音乐插件"><a href="#添加网易云音乐插件" class="headerlink" title="添加网易云音乐插件"></a>添加网易云音乐插件</h2><p>在网易云音乐网页中，打开<code>生成外链播放器</code>，将HTML代码复制到主题站点目录<code>themes\next\layout\_macro\sidebar.swig</code>找到<code>sidebar-inner</code>，将网易云插件代码代码粘贴到此<code>div</code>标签后即可。</p><p>演示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;aside class=<span class="string">&quot;sidebar&quot;</span>&gt;</span><br><span class="line"> &lt;div class=<span class="string">&quot;sidebar-inner&quot;</span>&gt; //从下面开始复制，粘贴到这里</span><br><span class="line">  &lt;!--网易云插件--&gt;</span><br><span class="line">  &lt;iframe frameborder=<span class="string">&quot;no&quot;</span> border=<span class="string">&quot;0&quot;</span> marginwidth=<span class="string">&quot;0&quot;</span> marginheight=<span class="string">&quot;0&quot;</span> width=330 height=86 </span><br><span class="line">      src=<span class="string">&quot;//music.163.com/outchain/player?type=2&amp;id=463352828&amp;auto=0&amp;height=66&quot;</span>&gt;</span><br><span class="line">  &lt;/iframe&gt;</span><br></pre></td></tr></table></figure><p>在博客文件夹打开<code>GitBash</code>执行<code>hexo clean</code>、<code>hexo g</code>、<code>hexo s</code>，即可看到效果。</p><h2 id="加入站点内容搜索功能"><a href="#加入站点内容搜索功能" class="headerlink" title="加入站点内容搜索功能"></a>加入站点内容搜索功能</h2><p>本站点使用的是Local Search。加入站点内容搜索功能步骤如下：</p><p>安装hexo-generator-search</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure><p>注意：安装时应在站点根目录下，即Hexo目录下<br>添加search字段，在站点<code>Hexo/_config.yml</code>中<code>Extensions</code>下面添加search字段，如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br></pre></td></tr></table></figure><p>打开主题配置文件<code>next/_config.yml</code>找到<code>Local search</code>，将enable设置为true。</p><h2 id="添加打赏功能"><a href="#添加打赏功能" class="headerlink" title="添加打赏功能"></a>添加打赏功能</h2><p>准备支付宝和微信二维码</p><ul><li>微信是在<code>收付款-&gt;二维码收款-&gt;保存收款码</code></li><li>支付宝是在<code>收钱-&gt;保存图片</code></li></ul><p><strong>在<code>next/_config.yml</code>中配置图片</strong></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reward (Donate)</span></span><br><span class="line"><span class="attr">reward_settings:</span></span><br><span class="line">  <span class="comment"># If true, reward would be displayed in every article by default.</span></span><br><span class="line">  <span class="comment"># You can show or hide reward in a specific article throuth `reward: true | false` in Front-matter.</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">animation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">comment:</span> <span class="string">坚持原创技术分享，您的支持将鼓励我继续创作！</span></span><br><span class="line"></span><br><span class="line"><span class="attr">reward:</span></span><br><span class="line">  <span class="attr">wechatpay:</span> <span class="string">/source/images/wechat.png</span></span><br><span class="line">  <span class="attr">alipay:</span> <span class="string">/source/images/alipay.jpg</span></span><br><span class="line">  <span class="comment">#bitcoin: /images/bitcoin.png</span></span><br></pre></td></tr></table></figure><blockquote><p>wechat.jpg、alipay.png图片放入<code>hexo/source/uploads</code>中。</p></blockquote><h2 id="隐藏网页底部powered-By-Hexo-强力驱动"><a href="#隐藏网页底部powered-By-Hexo-强力驱动" class="headerlink" title="隐藏网页底部powered By Hexo / 强力驱动"></a>隐藏网页底部powered By Hexo / 强力驱动</h2><p>在next的<code>_config.yml</code>，搜索<code>powered</code>，设置如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">powered:</span><br><span class="line">  <span class="comment"># Hexo link (Powered by Hexo).</span></span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Version info of Hexo after Hexo link (vX.X.X).</span></span><br><span class="line">  version: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">theme:</span><br><span class="line">  <span class="comment"># Theme &amp; scheme info link (Theme - NexT.scheme).</span></span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Version info of NexT after scheme info (vX.X.X).</span></span><br><span class="line">  version: <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>最新版本的主题中只需要设置<code>powered: false</code>即可。</p><h2 id="在文章底部增加版权信息"><a href="#在文章底部增加版权信息" class="headerlink" title="在文章底部增加版权信息"></a>在文章底部增加版权信息</h2><p>在<code>next/_config.yml</code>找到<code>creative_commons</code>，设置如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creative Commons 4.0 International License.</span></span><br><span class="line"><span class="comment"># See: https://creativecommons.org/share-your-work/licensing-types-examples</span></span><br><span class="line"><span class="comment"># Available values of license: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zero</span></span><br><span class="line"><span class="comment"># You can set a language value if you prefer a translated version of CC license, e.g. deed.zh</span></span><br><span class="line"><span class="comment"># CC licenses are available in 39 languages, you can find the specific and correct abbreviation you need on https://creativecommons.org</span></span><br><span class="line">creative_commons:</span><br><span class="line">  license: by-nc-sa</span><br><span class="line">  sidebar: <span class="literal">false</span></span><br><span class="line">  post: <span class="literal">true</span></span><br><span class="line">  language:</span><br></pre></td></tr></table></figure><p>（<strong>注意</strong>：如果解析出来之后，你的原始链接有问题：如：<code>http://yoursite.com/2018/07/29/Windows下使用hexo搭建博客/</code>，那么在根目录下<code>_config.yml</code>中写成类似这样：）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## If your site is put in a subdirectory, set url as &#x27;http://yoursite.com/child&#x27; and root as &#x27;/child/&#x27;</span></span><br><span class="line">url: https://marchboy.github.io/</span><br><span class="line">root: /</span><br><span class="line">permalink: :year/:month/:day/:title/</span><br><span class="line">permalink_defaults:</span><br></pre></td></tr></table></figure><p>就行了。</p><h2 id="阅读更多"><a href="#阅读更多" class="headerlink" title="阅读更多"></a>阅读更多</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-excerpt --save</span><br></pre></td></tr></table></figure><p>在站点配置文件中配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">excerpt:</span><br><span class="line">  depth: 5</span><br><span class="line">  excerpt_excludes: []</span><br><span class="line">  more_excludes: []</span><br><span class="line">  hideWholePostExcerpts: true</span><br></pre></td></tr></table></figure><h2 id="国内网络访问失败"><a href="#国内网络访问失败" class="headerlink" title="国内网络访问失败"></a>国内网络访问失败</h2><p>在网络和共享中心–属性–IPv4–DNS服务器地址。将DNS服务器地址改为：</p><table><thead><tr><th>首选DNS服务器</th><th>备选DNS服务器</th></tr></thead><tbody><tr><td>223.5.5.5</td><td>223.6.6.6</td></tr></tbody></table><p>然后再刷新github.io网页即可。</p><p><strong>Reference：</strong></p><p>1、<a href="https://blog.csdn.net/baidu_34310405/article/details/102665373">Hexo Next主题添加访客统计、访问次数统计、文章阅读次数统计</a></p><p>2、<a href="%5Bhttps://linlif.github.io/2017/05/27/Hexo%E4%BD%BF%E7%94%A8%E6%94%BB%E7%95%A5-%E6%B7%BB%E5%8A%A0%E5%88%86%E7%B1%BB%E5%8F%8A%E6%A0%87%E7%AD%BE/%5D(https://linlif.github.io/2017/05/27/Hexo%E4%BD%BF%E7%94%A8%E6%94%BB%E7%95%A5-%E6%B7%BB%E5%8A%A0%E5%88%86%E7%B1%BB%E5%8F%8A%E6%A0%87%E7%AD%BE/)">Hexo使用攻略-添加分类及标签</a></p><p>3、<a href="https://segmentfault.com/a/1190000017986794"><a href="https://segmentfault.com/a/1190000017986794">超详细Hexo+Github Page搭建技术博客教程</a></a></p><p>4、<a href="https://liujunzhou.top/2018/8/10/gitalk-error/#%E4%BD%BF%E7%94%A8Github%E8%B4%A6%E6%88%B7%E7%99%BB%E9%99%86%E8%AF%84%E8%AE%BA%E5%8C%BA%E6%97%B6401%E6%8A%A5%E9%94%99">【已解决】Hexo NexT使用Gitalk未找到相关的Issues进行评论Error:Validation Failed</a></p><p>5、<a href="https://www.jianshu.com/p/b5f509f25872">为 Hexo 的 Next 主题添加 Gitalk 评论</a></p><p>6、<a href="https://www.jianshu.com/p/3db6a61d3782">hexo使用markdown图片无法显示问题</a></p><p>7、<a href="https://www.zdaiot.com/Linux/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/windows%E4%B8%8B%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/">windows下使用hexo搭建博客</a></p><p>8、<a href="http://stevenshi.me/2017/06/26/hexo-insert-formula/">hexo中插入数学公式</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
