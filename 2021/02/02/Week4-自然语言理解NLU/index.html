<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marchboy.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="关于CrossWOZ数据集第一个大规模的中文跨域“人机交互”任务导向的数据集。 CrossWOZ包含 6K 个对话，102K 个句子，涉及 5 个领域（景点、酒店、餐馆、地铁、出租）。 将对话分成五种类型：单领域 S，多领域 M，多领域加交通 M+T，跨领域 CM，跨领域加交通 CM+T。交通代表了地铁和出租领域，M 和 CM 的区别是有没有跨领域的约束。">
<meta property="og:type" content="article">
<meta property="og:title" content="week4-自然语言理解NLU">
<meta property="og:url" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/index.html">
<meta property="og:site_name" content="ComputerScience">
<meta property="og:description" content="关于CrossWOZ数据集第一个大规模的中文跨域“人机交互”任务导向的数据集。 CrossWOZ包含 6K 个对话，102K 个句子，涉及 5 个领域（景点、酒店、餐馆、地铁、出租）。 将对话分成五种类型：单领域 S，多领域 M，多领域加交通 M+T，跨领域 CM，跨领域加交通 CM+T。交通代表了地铁和出租领域，M 和 CM 的区别是有没有跨领域的约束。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Dingtalk_20210202211237.jpg">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/attention-encoderdecoder.gif">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Attention.png">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Attentiontypes.png">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/transformer.png">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/sequenceLabeling.png">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/slot.png">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Encode-decoder.png">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Slot-gated.png">
<meta property="og:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Bert.png">
<meta property="article:published_time" content="2021-02-01T16:00:00.000Z">
<meta property="article:modified_time" content="2021-04-17T17:49:02.807Z">
<meta property="article:author" content="进军要努力呀">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="任务型对话">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Dingtalk_20210202211237.jpg">

<link rel="canonical" href="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>week4-自然语言理解NLU | ComputerScience</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-74793810-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-74793810-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?a5f7448ac0b523badc121b2d870f1b0c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ComputerScience</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">温故而知新</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="进军要努力呀">
      <meta itemprop="description" content="可怕的是自己内心的堕落">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ComputerScience">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          week4-自然语言理解NLU
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-02-02 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-02T00:00:00+08:00">2021-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-04-18 01:49:02" itemprop="dateModified" datetime="2021-04-18T01:49:02+08:00">2021-04-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D/" itemprop="url" rel="index"><span itemprop="name">任务型对话</span></a>
                </span>
            </span>

          
            <span id="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/" class="post-meta-item leancloud_visitors" data-flag-title="week4-自然语言理解NLU" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="关于CrossWOZ数据集"><a href="#关于CrossWOZ数据集" class="headerlink" title="关于CrossWOZ数据集"></a>关于CrossWOZ数据集</h1><p>第一个大规模的中文跨域“人机交互”任务导向的数据集。</p>
<p>CrossWOZ包含 6K 个对话，102K 个句子，涉及 5 个领域（景点、酒店、餐馆、地铁、出租）。</p>
<p>将对话分成五种类型：单领域 S，多领域 M，多领域加交通 M+T，跨领域 CM，跨领域加交通 CM+T。交通代表了地铁和出租领域，M 和 CM 的区别是有没有跨领域的约束。</p>
<p>此外，语料库包含丰富的对话状态注释，以及用户和系统端的对话行为。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;dialog_action&quot;:[</span><br><span class="line">  &quot;Greet&quot;,</span><br><span class="line">  &quot;Domain&quot;,</span><br><span class="line">  &quot;Entity&quot;,</span><br><span class="line">  &quot;Slot&quot;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>更多关于CrossWOZ，参考：</p>
<p>1、<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.11893.pdf">CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Datase</a></p>
<p>2、<a target="_blank" rel="noopener" href="https://www.aminer.cn/research_report/5f178da221d8d82f52e5a305">这篇顶会，助你徒手搭建任务导向对话系统</a></p>
<h1 id="Intent-Classification"><a href="#Intent-Classification" class="headerlink" title="Intent Classification"></a>Intent Classification</h1><p>在常见的一个意图的分类情形中，实际还需要处理其他更为复杂的意图识别情形，有：</p>
<p><strong>情形一  NLU中意图和槽位的多样性：</strong></p>
<p>在对话系统的NLU中，意图识别（Intent Detection）和槽位填充（Slot Filling）是两个重要的子任务。其中，意图识别可以看做是NLP中的一个分类任务，而槽位填充可以看做是一个序列标注任务。</p>
<p>在早期的系统中，通常的做法是将两者拆分成两个独立的子任务。但这种做法跟人类的语言理解方式是不一致的，事实上我们在实践中发现，两者很多时候是具有较强相关性的，比如下边的例子：</p>
<blockquote>
<p>1.我要听[北京天安门, song] — Intent：播放歌曲<br>2.帮我叫个车，到[北京天安门, location] — Inent：打车<br>3.播放[忘情水, song] — Intent：播放歌曲<br>4.播放[复仇者联盟, movie] — Intent：播放视频</p>
</blockquote>
<p>例子1和2中，可以看到同样是“北京天安门”，由于意图的不同，该实体具备完全不同的槽位类型。</p>
<p>例子3和4中，由于槽位类型的不同，导致了最终意图的不同，这往往意味着，在对话系统中的后继流程中将展现出完全不同的行为——-打开网易音乐播放歌曲 or 打开爱奇艺播放电影。</p>
<p><strong>情形二  Multi Intents：</strong></p>
<p>在对话NLU中，可能还会出现一些多意图的情形： 如景点距离我多远，能帮我导航出来吗？</p>
<p>这里就涉及到用户的几个意图：找景点、找距离。</p>
<p>那在做多分类的时候，一般是在网络输出层（Output Layer）上，需要接上一层Softmax activation function；接着输出每一类的概率，针对每一类的概率之后，需要识别每一个分类来判断它是属于0还是1，所以还需要接上一层Sigmoid，并且Loss也不能使用交叉熵了。有：</p>
<p>Softmax激活函数</p>
<script type="math/tex; mode=display">
Softmax\_activation\_function = \frac{e^{z_i}}{\sum_{j=1}^Ke^{z_j}}</script><p>交叉熵</p>
<script type="math/tex; mode=display">
Cross Entropy = -\sum_{c=1}^M{y_{i,c}}log(p_{i,c})</script><p>Sigmoid激活函数</p>
<script type="math/tex; mode=display">
Sigmoid = \frac{1}{e^x + 1}</script><p>信息熵</p>
<script type="math/tex; mode=display">
H(X)=−\sum_{i=1}^n p(xi)log(p(xi))</script><p><strong>情形三  多轮返回（Multi Turn）</strong></p>
<p>包含上下文的信息，需要引入history，例如：</p>
<p>今天天气不好啊，我下午能去做什么呢—-&gt; 去打球、在家打游戏，出去唱K</p>
<p>在此引出BERT框架，大致为：</p>
<p>预料【输入】 —&gt; 分词（Tokenization）—&gt;</p>
<p>CLS、               Tok1、…、Tok N —&gt; <strong>BERT</strong> —&gt;</p>
<p>C（vector）、 T1、   …、 T N     —-&gt;  Dense Layer +Activation  —&gt; Probalities【输出】</p>
<h1 id="关于Bert"><a href="#关于Bert" class="headerlink" title="关于Bert"></a>关于Bert</h1><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/46cb208d45c3">彻底理解 Google BERT 模型</a></p>
<p>以下摘自论文【BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding】：</p>
<blockquote>
<p>BERT is designed to pre-train depp bidirectional represntations from <strong>unlabeled text</strong> by jointlly conditioning on both <strong>left and right context</strong> in all layers.</p>
<p>As a result, the pre-trained BERT model can be finetuned with just one <strong>additional output</strong> layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p>
<p>The masked language model <strong>randomly masks some of the tokens</strong> from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.</p>
<p>During fine-tuning, all parameters are fine-tuned. </p>
<p>[CLS] is a special symbol added in front of every input example, </p>
<p>[CLS] 一个全局的向量，放在第一个句子的首位，经过 BERT 得到的的表征向量 <strong>C</strong> 可以用于后续的分类任务。 [CLS] we must add this token to the start of each sentence, so BERT knows we’re doing classification。</p>
<p>[SEP] is a special separator token (e.g. separating questions/answers).</p>
<p>[SEP] 句子结尾的标记，用于分开两个输入句子，例如输入句子 A 和 B，要在句子 A，B 后面增加 [SEP] 标志。</p>
<p>[UNK] 标志指的是未知字符。</p>
<p>[UNK] BERT understands tokens that were in the training set. Everything else can be encoded using the <code>[UNK]</code> (unknown) token。</p>
<p>[MASK] 标志用于遮盖句子中的一些单词，将单词用 [MASK] 遮盖之后，再利用 BERT 输出的 [MASK] 向量预测单词是什么。</p>
</blockquote>
<p>摘自【<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=Tbodro8Fpmwr">sentiment-analysis-with-bert</a>】</p>
<blockquote>
<p>BERT (introduced in <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">this paper</a>) stands for Bidirectional Encoder Representations from Transformers.</p>
<p>BERT（本文介绍）代表来自Transformers的双向编码器表示。</p>
<p>Bidirectional - to understand the text you’re looking you’ll have to look back (at the previous words) and forward (at the next words)</p>
<p>双向-要理解您要查找的文本，您必须向后（在前一个单词处）和向前（在后一个单词处）。</p>
<p>The Transformer reads entire sequences of tokens at once. In a sense, the model is non-directional, while LSTMs read sequentially (left-to-right or right-to-left). The attention mechanism allows for learning contextual relations between words.</p>
<p>Transformer一次读取整个令牌序列。从某种意义上说，该模型是无方向性的，而LSTM则按顺序读取（从左到右或从右到左）。注意机制允许学习单词之间的上下文关系。</p>
<p>BERT was trained by masking 15% of the tokens with the goal to guess them. An additional objective was to predict the next sentence.</p>
<p>BERT通过掩盖15％的tokens来进行训练，目的是猜测它们。另一个目标是预测下一个句子。</p>
<h4 id="Masked-Language-Modeling-Masked-LM"><a href="#Masked-Language-Modeling-Masked-LM" class="headerlink" title="Masked Language Modeling (Masked LM)"></a>Masked Language Modeling (Masked LM)</h4><p>The objective of this task is to guess the masked tokens. Let’s look at an example, and try to not make it harder than it has to be:</p>
<p>这个任务的目的是猜测被屏蔽的Tokens。让我们看一个例子，尽量不要让它变得比原来更难：</p>
<p>That’s <code>[mask]</code> she <code>[mask]</code> -&gt; That’s what she said</p>
<h4 id="Next-Sentence-Prediction-NSP"><a href="#Next-Sentence-Prediction-NSP" class="headerlink" title="Next Sentence Prediction (NSP)"></a>Next Sentence Prediction (NSP)</h4><p>Given a pair of two sentences, the task is to say whether or not the second follows the first (binary classification). Let’s continue with the example:</p>
<p><strong><em>Input</em> = <code>[CLS]</code> That’s <code>[mask]</code> she <code>[mask]</code>. [SEP] Hahaha, nice! [SEP]</strong></p>
<p><strong><em>Label</em> = <em>IsNext</em></strong></p>
<p><strong><em>Input</em> = <code>[CLS]</code> That’s <code>[mask]</code> she <code>[mask]</code>. [SEP] Dwight, you ignorant <code>[mask]</code>! [SEP]</strong></p>
<p><strong><em>Label</em> = <em>NotNext</em></strong></p>
<p>The training corpus was comprised of two entries: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.06724">Toronto Book Corpus</a> (800M words) and English Wikipedia (2,500M words).</p>
<p>While the original Transformer has an encoder (for reading the input) and a decoder (that makes the prediction), BERT uses only the decoder.原始的Transformer具有一个编码器（用于读取输入）和一个解码器（用于进行预测），而BERT仅使用解码器。</p>
<p>BERT is simply a pre-trained stack of Transformer Encoders. How many Encoders? We have two versions - with 12 (BERT base) and 24 (BERT Large).</p>
<h4 id="Is-This-Thing-Useful-in-Practice"><a href="#Is-This-Thing-Useful-in-Practice" class="headerlink" title="Is This Thing Useful in Practice?"></a>Is This Thing Useful in Practice?</h4><p>The BERT paper was released along with <a target="_blank" rel="noopener" href="https://github.com/google-research/bert">the source code</a> and pre-trained models.</p>
<p>The best part is that you can do Transfer Learning (thanks to the ideas from OpenAI Transformer) with BERT for many NLP tasks - Classification, Question Answering, Entity Recognition, etc. You can train with small amounts of data and achieve great performance!</p>
</blockquote>
<p><strong>发表BERT的动机（Motivation）：</strong></p>
<p>1、大量数据，没有标签</p>
<p>2、当前word embedding 模型能力不够强大，有较多的局限性</p>
<p>3、没有考虑上下文信息，word2vec没有根据上文来生成向量</p>
<p>4、问答中长文本的依赖，传统的RNN会导致梯度的消失（LSTM可以缓解，但也会发生梯度消失）</p>
<p>5、迁移学习能否进一步推广应用，做fine turn的工作</p>
<p>Basic idea</p>
<p>随机masks words in sentence and predict them</p>
<p>Transformer architecture</p>
<p>Next sentence prediction</p>
<p>严格来说，Bert是一种训练策略，不是新的架构设计。</p>
<p><strong>BERT-DATA</strong></p>
<p>WordPieces instead of words, （eg.playing-&gt; play+ ing）它的实现方式是是一种叫BPE（Byte-Pair Encoding）的双字节解码。</p>
<p>1、减少词汇的数量</p>
<p>2、增加每一个单词的样本数量</p>
<p>3、避免OOV</p>
<p>WordPieces带来的问题：</p>
<p>probability—-pro+ ##bali + ##lity </p>
<p>如果bali被masked，则很容易预测到probability这个单词，在中文中亦是如此。</p>
<p>解决方案：Whole Word Masking</p>
<p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Dingtalk_20210202211237.jpg" alt></p>
<blockquote>
<p>补充点：</p>
<p>1、关于whole word piece（WWP），它是对BERT的一个很有效的改进</p>
<p><a target="_blank" rel="noopener" href="http://fancyerii.github.io/2019/08/02/bert-pretrain-imp/">对BERT的pretraining改进的几篇文章</a></p>
<p><a target="_blank" rel="noopener" href="https://www.ctolib.com/ymcui-Chinese-BERT-wwm.html">中文全词覆盖（Whole Word Masking）BERT的预训练模型</a></p>
<p>2、BPE</p>
<p>现在基本性能好一些的NLP模型，例如OpenAI GPT，google的BERT，在数据预处理的时候都会有WordPiece的过程。WordPiece字面理解是把word拆成piece一片一片，其实就是这个意思。</p>
<p>WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。</p>
<p>BPE的过程可以理解为把一个单词再拆分，使得我们的此表会变得精简，并且寓意更加清晰。</p>
<p>比如”loved”,”loving”,”loves”这三个单词。其实本身的语义都是“爱”的意思，但是如果我们以单词为单位，那它们就算不一样的词，在英语中不同后缀的词非常的多，就会使得词表变的很大，训练速度变慢，训练的效果也不是太好。</p>
<p>BPE算法通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/huangyc/p/10223075.html">一文读懂BERT中的WordPiece</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/az9996/article/details/108858708">BERT 中的tokenizer和wordpiece和bpe（byte pair encoding）分词算法</a></p>
<p>3、Transformer-XL</p>
<p>Transformer-XL的依存关系比RNN长80％，比原始Transformer长450％，在短序列和长序列上均具有更好的性能，并且在评估期间比原始Transformer快1800倍以上。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jiangxinyang/p/11534492.html">NLP中的预训练语言模型（三）—— XL-Net和Transformer-XL</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70745925">论文笔记 —— Transformer-XL</a></p>
</blockquote>
<h1 id="关于Attention"><a href="#关于Attention" class="headerlink" title="关于Attention"></a>关于Attention</h1><p>Attention的思想如同它的名字一样，就是“注意力”，<strong>在预测结果时把注意力放在不同的特征上</strong>，核心逻辑就是<strong>从【关注全部】 到 【关注重点】</strong>。</p>
<p>在视觉图像、文本任务中， Attention机制是<strong>将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。</strong></p>
<p>优点：</p>
<p><strong>参数少：</strong>模型复杂度跟 <a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/cnn/">CNN</a>、<a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/rnn/">RNN</a> 相比，复杂度更小，参数也更少。所以对算力的要求也就更小。</p>
<p><strong>速度快：</strong>Attention 解决了 RNN 不能并行计算的问题。Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。</p>
<p><strong>效果好：</strong>在 Attention 机制引入之前，有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的。</p>
<p>Attention 是挑重点，就算文本比较长，也能从中间抓住重点，不丢失重要的信息。</p>
<p><strong>Attention原理</strong></p>
<p>attention 引入 Encoder-Decoder 框架下，完成机器翻译任务的大致流程。</p>
<p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/attention-encoderdecoder.gif" alt></p>
<p><strong>Attention基本原理的基本步骤:</strong></p>
<p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Attention.png" alt></p>
<p>第一步： query 和 key 进行相似度计算，得到权值</p>
<p>第二步：将权值进行归一化，得到直接可用的权重</p>
<p>第三步：将权重和 value 进行加权求和</p>
<p>Attenttion 类型：</p>
<p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Attentiontypes.png" alt></p>
<p><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/attention/">EasyAI-Attention 机制</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/43493999">NLP中的Attention原理和源码解析</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/guoyaohua/p/9429924.html">Attention Model（注意力模型）学习总结</a></p>
<h1 id="关于Transformer"><a href="#关于Transformer" class="headerlink" title="关于Transformer"></a>关于Transformer</h1><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/transformer.png" alt></p>
<p><strong>参考老师PPT32-45页</strong></p>
<p>参考：</p>
<p>1、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221">论文详解Transformer （Attention Is All You Need）</a></p>
<p>2、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/345680792">Transformer 一篇就够了（一）： Self-attenstion</a></p>
<p>3、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/347492368">Transformer 一篇就够了（二）： Transformer中的Self-attenstion</a></p>
<p>4、技术细节 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54530247">模型优化之Layer Normalization</a></p>
<p>5、<a target="_blank" rel="noopener" href="https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d">Multi-label Text Classification using BERT – The Mighty Transformer</a></p>
<h1 id="NLU-Slot-Extraction"><a href="#NLU-Slot-Extraction" class="headerlink" title="NLU: Slot Extraction"></a>NLU: Slot Extraction</h1><p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/sequenceLabeling.png" alt></p>
<p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/slot.png" alt></p>
<h1 id="NLU-Joint-Intent-Slot-extraction"><a href="#NLU-Joint-Intent-Slot-extraction" class="headerlink" title="NLU: Joint Intent-Slot extraction"></a>NLU: Joint Intent-Slot extraction</h1><h4 id="Attention-Based-Recurrent-Neural-Network-Models-for-Joint-Intent-Detection-and-Slot-Filling（2016）"><a href="#Attention-Based-Recurrent-Neural-Network-Models-for-Joint-Intent-Detection-and-Slot-Filling（2016）" class="headerlink" title="Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling（2016）"></a>Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling（2016）</h4><p>基于注意力的编码器-解码器神经网络模型最近在机器翻译和语音识别中显示出令人鼓舞的结果。在这项工作中，我们提出了一种基于注意力的神经网络模型，用于联合意图检测和slot filling，这对于许多语音理解和对话系统都是至关重要的步骤。与机器翻译和语音识别不同，对齐在slot filling中是显式的。我们探索将对齐信息整合到编码器-解码器框架中的不同策略。从编码器-解码器模型中的注意力机制中学习，我们进一步建议将注意力引入基于对齐的RNN模型。这种关注为意图分类和slot filling预测提供了更多信息。我们的独立任务模型在ATIS任务上实现了最优的意图检测错误率和slot fillingF1分数。与独立任务模型相比，我们的联合训练模型在意图检测上进一步获得了0.56％的绝对误差（相对值23.8％的相对误差），在slot filling上获得了0.23％的绝对增益。</p>
<p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Encode-decoder.png" alt></p>
<p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/shengyan5515/article/details/105691543">Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling论文笔记</a></p>
<h4 id="Slot-Gated-Modeling-for-Joint-Slot-Filling-and-Intent-Prediction（2018）"><a href="#Slot-Gated-Modeling-for-Joint-Slot-Filling-and-Intent-Prediction（2018）" class="headerlink" title="Slot-Gated Modeling for Joint Slot Filling and Intent Prediction（2018）"></a>Slot-Gated Modeling for Joint Slot Filling and Intent Prediction（2018）</h4><p>基于注意力的递归神经网络模型用于联合意图检测和插槽填充，具有最先进的性能，同时具有独立的注意力权重。考虑到插槽和意图之间存在很强的关系，本文提出了一种插槽门，其重点是学习意图和插槽注意向量之间的关系，以便通过全局优化获得更好的语义框架结果。实验表明，与基准ATIS和Snips数据集上的注意力模型相比，我们提出的模型显著提高了句子级语义框架的准确率，相对注意度模型分别提高了4.2％和1.9％。</p>
<p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Slot-gated.png" alt></p>
<p>笔记：<a target="_blank" rel="noopener" href="https://blog.csdn.net/shengyan5515/article/details/105776303">Slot-Gated Modeling for Joint Slot Filling and Intent Prediction论文笔记</a></p>
<h4 id="BERT-for-Joint-Intent-Classification-and-Slot-Filling（2019）"><a href="#BERT-for-Joint-Intent-Classification-and-Slot-Filling（2019）" class="headerlink" title="BERT for Joint Intent Classification and Slot Filling（2019）"></a>BERT for Joint Intent Classification and Slot Filling（2019）</h4><p>意图分类和slot filling是自然语言理解的两个基本任务。它们经常遭受小规模的人工标签训练数据的困扰，导致泛化能力差，尤其是对于稀有单词。最近，一种新的语言表示模型BERT（来自Transformers的双向编码器表示）有助于在大型未标记的语料库上进行预训练深层的双向表征，并在处理完各种自然语言处理任务后创建了最新的模型简单的微调。但是，在探索BERT以获得自然语言理解方面并没有付出很多努力。在这项工作中，我们提出了一个基于BERT的联合意图分类和slot filling模型。实验结果表明，与基于注意力的递归神经网络模型和slot-gated模型相比，我们提出的模型在多个公共基准数据集上的意图分类准确性，slot filling F1和句子级语义准确性都有了显着提高。</p>
<p><img src="/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/Bert.png" alt></p>
<p>笔记：<a target="_blank" rel="noopener" href="https://blog.csdn.net/shengyan5515/article/details/105678169">BERT for Joint Intent Classification and Slot Filling论文笔记</a></p>

    </div>

    
    
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;"> ------ 本文结束------</div>
    
</div>
        <div class="reward-container">
  <div>Donate comment here.</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="进军要努力呀 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="进军要努力呀 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>进军要努力呀
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://marchboy.github.io/2021/02/02/Week4-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3NLU/" title="week4-自然语言理解NLU">https://marchboy.github.io/2021/02/02/Week4-自然语言理解NLU/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
              <a href="/tags/%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D/" rel="tag"><i class="fa fa-tag"></i> 任务型对话</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/01/11/Week3-RASA%E6%BA%90%E7%A0%81%E5%92%8C%E5%AE%9A%E5%88%B6%E5%8C%96%E4%BD%A0%E7%9A%84%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA/" rel="prev" title="week3-RASA源码和定制化你的对话机器人">
      <i class="fa fa-chevron-left"></i> week3-RASA源码和定制化你的对话机器人
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/02/07/Week5-HuggingFace's-Transformers/" rel="next" title="week5-Further NLU and Dialogue State Tracking">
      week5-Further NLU and Dialogue State Tracking <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
      <div class="tabs tabs-comment">
        <ul class="nav-tabs">
            <li class="tab"><a href="#comment-gitalk">gitalk</a></li>
            <li class="tab"><a href="#comment-valine">valine</a></li>
        </ul>
        <div class="tab-content">
            <div class="tab-pane gitalk" id="comment-gitalk">
              <div class="comments" id="gitalk-container"></div>
            </div>
            <div class="tab-pane valine" id="comment-valine">
              <div class="comments" id="valine-comments"></div>
            </div>
        </div>
      </div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E4%BA%8ECrossWOZ%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.</span> <span class="nav-text">关于CrossWOZ数据集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Intent-Classification"><span class="nav-number">2.</span> <span class="nav-text">Intent Classification</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E4%BA%8EBert"><span class="nav-number">3.</span> <span class="nav-text">关于Bert</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Masked-Language-Modeling-Masked-LM"><span class="nav-number">3.0.0.1.</span> <span class="nav-text">Masked Language Modeling (Masked LM)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Next-Sentence-Prediction-NSP"><span class="nav-number">3.0.0.2.</span> <span class="nav-text">Next Sentence Prediction (NSP)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Is-This-Thing-Useful-in-Practice"><span class="nav-number">3.0.0.3.</span> <span class="nav-text">Is This Thing Useful in Practice?</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E4%BA%8EAttention"><span class="nav-number">4.</span> <span class="nav-text">关于Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E4%BA%8ETransformer"><span class="nav-number">5.</span> <span class="nav-text">关于Transformer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLU-Slot-Extraction"><span class="nav-number">6.</span> <span class="nav-text">NLU: Slot Extraction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLU-Joint-Intent-Slot-extraction"><span class="nav-number">7.</span> <span class="nav-text">NLU: Joint Intent-Slot extraction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Attention-Based-Recurrent-Neural-Network-Models-for-Joint-Intent-Detection-and-Slot-Filling%EF%BC%882016%EF%BC%89"><span class="nav-number">7.0.0.1.</span> <span class="nav-text">Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling（2016）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Slot-Gated-Modeling-for-Joint-Slot-Filling-and-Intent-Prediction%EF%BC%882018%EF%BC%89"><span class="nav-number">7.0.0.2.</span> <span class="nav-text">Slot-Gated Modeling for Joint Slot Filling and Intent Prediction（2018）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BERT-for-Joint-Intent-Classification-and-Slot-Filling%EF%BC%882019%EF%BC%89"><span class="nav-number">7.0.0.3.</span> <span class="nav-text">BERT for Joint Intent Classification and Slot Filling（2019）</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="进军要努力呀"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">进军要努力呀</p>
  <div class="site-description" itemprop="description">可怕的是自己内心的堕落</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">43</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marchboy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marchboy" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jjgdut@gmail.com" title="E-Mail → mailto:jjgdut@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://plus.google.com/yourname" title="Google → https:&#x2F;&#x2F;plus.google.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Google</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/yourname" title="YouTube → https:&#x2F;&#x2F;youtube.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/pinard" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;pinard" rel="noopener" target="_blank">刘建平Pinard</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://vel.life/" title="https:&#x2F;&#x2F;vel.life&#x2F;" rel="noopener" target="_blank">思维之海</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://wulc.me/" title="https:&#x2F;&#x2F;wulc.me&#x2F;" rel="noopener" target="_blank">吴良超的学习笔记</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/IT_job" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;IT_job" rel="noopener" target="_blank">IT_job的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.zdaiot.com/categories/" title="https:&#x2F;&#x2F;www.zdaiot.com&#x2F;categories&#x2F;" rel="noopener" target="_blank">zdaiot</a>
        </li>
    </ul>
  </div>


      </div>
	 <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=320 height=86 
	 src="//music.163.com/outchain/player?type=2&id=463352828&auto=0&height=66">
		    </iframe>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">进军要努力呀</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">182k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">2:45</span>
</div><!-- ����ҳ�ײ�������վ����ʱ�� -->
<span id="timeDate">��������...</span><span id="times">����ʱ����...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("09/01/2020 00:00:00");//�˴��޸���Ľ�վʱ�������վ����ʱ��
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "Run for "+dnum+" Days ";
        document.getElementById("times").innerHTML = hnum + " Hours " + mnum + " m " + snum + " s";
    }
setInterval("createtime()",250);
</script>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '9de70fca612487225ccc',
      clientSecret: 'c5242111211342b5c4846f7f261816d1940d3996',
      repo        : 'marchboy.github.io',
      owner       : 'marchboy',
      admin       : ['marchboy'],
      id          : '2bee412d0421c50cbb64c72a6bf00763',
        language: '',
      distractionFreeMode: false
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qAoqMWohGTiECb1RgaNqvgNB-MdYXbMMI',
      appKey     : 'ISI8QFSo7GcRyExPHSjMkz7R',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
